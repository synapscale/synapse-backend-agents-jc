# ðŸš€ SynapScale Backend - Cursor Rules v2.0
# Plataforma de AutomaÃ§Ã£o com IA - Regras completas e otimizadas para desenvolvimento

## ðŸ“‹ CONTEXTO DO PROJETO

VocÃª estÃ¡ trabalhando no **SynapScale Backend**, uma plataforma empresarial de automaÃ§Ã£o com IA de nÃ­vel production. Sistema robusto e escalÃ¡vel com:

- **Backend**: FastAPI 0.110+ + SQLAlchemy 2.0+ + PostgreSQL + Redis + Alembic
- **IA**: IntegraÃ§Ã£o multi-provedor (OpenAI, Anthropic, Google, Grok, DeepSeek, Llama, Tess)
- **Arquitetura**: API REST + WebSockets + Workflows + Marketplace + Analytics + Billing
- **ProduÃ§Ã£o**: Sistema estÃ¡vel com 242 endpoints, 70.7% de taxa de sucesso, cobertura de testes
- **Ferramentas**: Taskmaster (task management), Cursor MCP, sistema de tags, rate limiting
- **Infraestrutura**: Docker, Render, S3/GCS storage, Sentry monitoring

## âš ï¸ REGRA CRÃTICA - ARQUIVO PRINCIPAL DA APLICAÃ‡ÃƒO

### **ðŸš¨ ÃšNICA FONTE DE VERDADE: `src/synapse/main.py`**

**CONTEXTO**: Este projeto teve conflitos entre `app.py` e `main.py` causando duplicaÃ§Ã£o de cÃ³digo e confusÃ£o. Foi estabelecido `main.py` como arquivo principal Ãºnico.

**âŒ PROIBIDO:**
- Criar `src/synapse/app.py` ou recriar esse arquivo
- Criar outros arquivos que instanciem o FastAPI app (ex: `server.py`, `application.py`)
- Duplicar as funÃ§Ãµes de inicializaÃ§Ã£o do FastAPI que estÃ£o em `main.py`
- Modificar referÃªncias de deploy para apontar para qualquer arquivo que nÃ£o seja `main.py`

**âœ… OBRIGATÃ“RIO:**
- TODA inicializaÃ§Ã£o do FastAPI deve estar APENAS em `src/synapse/main.py`
- Deploy, scripts e documentaÃ§Ã£o devem referenciar APENAS `src.synapse.main:app`
- Qualquer alteraÃ§Ã£o na aplicaÃ§Ã£o principal deve ser feita SOMENTE em `main.py`

**ðŸŽ¯ VERIFICAÃ‡ÃƒO PRÃ‰-COMMIT:**
- [ ] `src/synapse/app.py` nÃ£o existe
- [ ] ReferÃªncias de deploy apontam para `src.synapse.main:app`
- [ ] NÃ£o hÃ¡ duplicaÃ§Ã£o de instÃ¢ncia FastAPI em outros arquivos

---

## ðŸ§ª REGRA CRÃTICA - GERENCIAMENTO DE SCRIPTS DE TESTE

### **ðŸš¨ REGRA ABSOLUTA: REUTILIZAR ANTES DE CRIAR**

**CONTEXTO**: O repositÃ³rio estÃ¡ sendo poluÃ­do com scripts de teste/validaÃ§Ã£o/anÃ¡lise duplicados e abandonados. Precisamos manter organizaÃ§Ã£o e reutilizar cÃ³digo existente.

**âŒ PROIBIDO:**
- Criar novos scripts de teste/validaÃ§Ã£o/anÃ¡lise sem verificar se jÃ¡ existe similar
- Deixar scripts na raiz do repositÃ³rio quando podem estar em `tests/`
- Abandonar scripts apÃ³s uso sem limpeza
- Duplicar lÃ³gica de teste em novos arquivos
- Criar scripts pontuais sem plano de limpeza

**âœ… OBRIGATÃ“RIO:**

### 1. **VERIFICAÃ‡ÃƒO ANTES DE CRIAR**
```bash
# âœ… SEMPRE verificar scripts existentes primeiro
ls -la *.py | grep -E "(test|check|validate|verify|analyze)"
find tests/ -name "*.py" | grep -i "similar_functionality"
```

### 2. **PRIORIDADE DE REUTILIZAÃ‡ÃƒO**
```python
# âœ… SEMPRE otimizar scripts existentes ao invÃ©s de criar novos
# Exemplo: Se existe test_model_validation.py, EDITAR ele para adicionar novos testes
# ao invÃ©s de criar validate_models_comprehensive.py

# âŒ ERRADO: Criar novo script
# create_file("test_new_validation.py", content="...")

# âœ… CORRETO: Otimizar existente
# edit_file("tests/test_model_validation.py", 
#           old_str="# existing tests", 
#           new_str="# existing tests\n# new validation logic")
```

### 3. **ORGANIZAÃ‡ÃƒO OBRIGATÃ“RIA**
```
âœ… ESTRUTURA CORRETA:
tests/
â”œâ”€â”€ unit/                    # Testes unitÃ¡rios
â”œâ”€â”€ integration/            # Testes de integraÃ§Ã£o
â”œâ”€â”€ analysis/               # Scripts de anÃ¡lise
â”œâ”€â”€ validation/             # Scripts de validaÃ§Ã£o
â””â”€â”€ utils/                  # UtilitÃ¡rios de teste

âŒ EVITAR NA RAIZ:
â”œâ”€â”€ test_something.py       # Deve estar em tests/
â”œâ”€â”€ check_database.py       # Deve estar em tests/analysis/
â”œâ”€â”€ validate_models.py      # Deve estar em tests/validation/
â”œâ”€â”€ analyze_*.py           # Deve estar em tests/analysis/
```

### 4. **SCRIPTS PONTUAIS - LIMPEZA OBRIGATÃ“RIA**
```python
# âœ… Para scripts temporÃ¡rios/pontuais:

# 1. Criar em local adequado
script_path = "tests/temp/temp_analysis_20250107.py"

# 2. Usar com documentaÃ§Ã£o clara
"""
SCRIPT TEMPORÃRIO - DELETAR APÃ“S USO
Criado: 2025-01-07
PropÃ³sito: AnÃ¡lise pontual de performance
Deletar apÃ³s: AnÃ¡lise concluÃ­da
"""

# 3. SEMPRE deletar apÃ³s uso
# TODO: Deletar este script apÃ³s anÃ¡lise concluÃ­da âœ…
```

### 5. **WORKFLOW OBRIGATÃ“RIO**
```bash
# âœ… Antes de criar qualquer script de teste/anÃ¡lise:

# 1. Verificar se existe similar
echo "ðŸ” Verificando scripts existentes..."
find . -name "*.py" -type f | grep -E "(test|check|validate|verify|analyze)" | head -10

# 2. Se existe similar, EDITAR ao invÃ©s de criar novo
echo "âœï¸ Editando script existente ao invÃ©s de criar novo..."

# 3. Se nÃ£o existe, criar em local correto
echo "ðŸ“ Criando em tests/categoria_adequada/"

# 4. Para scripts pontuais, adicionar comentÃ¡rio de limpeza
echo "ðŸ—‘ï¸ Adicionando TODO para limpeza posterior..."

# 5. ApÃ³s uso, SEMPRE limpar
echo "âœ… Limpando scripts temporÃ¡rios..."
```

### 6. **SCRIPTS PERMITIDOS NA RAIZ (APENAS)**
```python
# âœ… ÃšNICOS scripts permitidos na raiz (casos excepcionais):
ALLOWED_ROOT_SCRIPTS = [
    "alembic.ini",           # ConfiguraÃ§Ã£o Alembic
    "dev.sh",                # Script de desenvolvimento
    "prod.sh",               # Script de produÃ§Ã£o  
    "setup.sh",              # Setup inicial do projeto
    "pyproject.toml",        # ConfiguraÃ§Ã£o Python
    "requirements.txt"       # DependÃªncias
]

# âŒ Qualquer outro script de teste/anÃ¡lise DEVE estar em tests/
```

### 7. **VERIFICAÃ‡ÃƒO PRÃ‰-COMMIT**
```bash
# âœ… Checklist obrigatÃ³rio antes de commit:
echo "ðŸ“‹ VerificaÃ§Ãµes de scripts:"
echo "- [ ] NÃ£o criei script duplicado quando jÃ¡ existia similar?"
echo "- [ ] Scripts de teste estÃ£o em tests/ e nÃ£o na raiz?"
echo "- [ ] Scripts pontuais tÃªm TODO para limpeza?"
echo "- [ ] Reutilizei cÃ³digo existente ao mÃ¡ximo?"
echo "- [ ] Scripts temporÃ¡rios foram deletados apÃ³s uso?"
```

### 8. **PENALIDADES POR VIOLAÃ‡ÃƒO**
- **1Âª violaÃ§Ã£o**: Refatorar imediatamente
- **2Âª violaÃ§Ã£o**: Revisar toda estrutura de testes
- **3Âª violaÃ§Ã£o**: Limpeza completa do repositÃ³rio

**ðŸŽ¯ REGRA DE OURO:**
> "Se existe similar, EDITE. Se nÃ£o existe, crie em tests/. Se Ã© pontual, DELETE apÃ³s uso."

---

## ðŸŽ¯ PRINCÃPIOS FUNDAMENTAIS

### 1. **ConfiguraÃ§Ã£o Centralizada**
- SEMPRE use `settings` de `synapse.core.config` para configuraÃ§Ãµes
- NUNCA hardcode valores - use variÃ¡veis de ambiente
- Mantenha compatibilidade com o sistema de configuraÃ§Ã£o existente

### 2. **Estrutura do Projeto**
- Siga a estrutura modular: `src/synapse/api/v1/endpoints/`
- Models em `src/synapse/models/`
- Schemas em `src/synapse/schemas/`
- Services em `src/synapse/services/`

### 3. **PadrÃµes de CÃ³digo**
- Use type hints em TUDO (Python 3.11+)
- Siga convenÃ§Ãµes PEP 8 com Black (line-length=88)
- Docstrings obrigatÃ³rias para classes e funÃ§Ãµes pÃºblicas
- Use async/await para operaÃ§Ãµes I/O
- SEMPRE use f-strings para formataÃ§Ã£o de strings
- Prefira comprehensions sobre loops quando possÃ­vel
- Use context managers para recursos (conexÃµes, arquivos)

### 4. **Taskmaster Integration**
- Use Taskmaster MCP tools para gerenciamento de tarefas
- SEMPRE documente progresso com `update_subtask`
- Use tags para organizar contextos de desenvolvimento
- Mantenha tasks atualizadas durante implementaÃ§Ã£o

### 5. **Workflow de Desenvolvimento com Taskmaster**
```python
# âœ… PadrÃ£o de desenvolvimento com documentaÃ§Ã£o automÃ¡tica

# 1. Inicializar projeto (uma vez)
# task-master init --name="SynapScale Backend" --description="Plataforma de automaÃ§Ã£o com IA"

# 2. Durante implementaÃ§Ã£o, SEMPRE documentar progresso
async def implement_feature_with_logging():
    """Implementa feature documentando cada passo."""
    
    # âœ… Atualizar subtask com descobertas tÃ©cnicas
    await update_subtask(
        task_id="2.3",
        notes="""
        âš¡ ImplementaÃ§Ã£o do sistema de cache Redis:
        
        DESCOBERTAS:
        - Redis configurado na URL: {settings.REDIS_URL}
        - TTL padrÃ£o: 300s (configurÃ¡vel via CACHE_DEFAULT_TTL)
        - Decorator @cached() funcionando perfeitamente
        
        IMPLEMENTADO:
        âœ… CacheService com get/set/delete
        âœ… Decorator para cache automÃ¡tico
        âœ… IntegraÃ§Ã£o com analytics (cache de 10min)
        âœ… Testes unitÃ¡rios passando
        
        PRÃ“XIMOS PASSOS:
        - Implementar cache para consultas de LLM
        - Adicionar mÃ©tricas de hit/miss ratio
        - Configurar eviction policies
        
        PERFORMANCE:
        - Consultas de analytics: 2.3s â†’ 0.05s ðŸš€
        - ReduÃ§Ã£o de 95% no tempo de resposta
        """
    )
    
    # âœ… Marcar como completa quando finalizada
    await set_task_status(task_id="2.3", status="done")

# 3. Usar research para decisÃµes tÃ©cnicas
# task-master research "Como otimizar queries SQLAlchemy com relacionamentos?" 
#   --files=src/synapse/models/,src/synapse/services/ 
#   --save-to=3.1

# 4. Organizar com tags para diferentes contextos
# task-master use-tag feature-cache-redis    # Para nova feature
# task-master use-tag hotfix-performance     # Para otimizaÃ§Ãµes
# task-master use-tag refactor-llm-service   # Para refatoraÃ§Ãµes
# task-master use-tag master                 # Para desenvolvimento principal

# 5. Expandir tasks complexas com research
# task-master expand --id=4 --research --force
```

### 6. **PadrÃµes de Commits com Taskmaster**
```bash
# âœ… Commits estruturados com referÃªncia Ã s tasks

# Feature completa
git commit -m "feat(cache): Implement Redis caching system

- Add CacheService with TTL support
- Create @cached decorator for automatic caching
- Integrate with analytics endpoints (95% perf improvement)
- Add comprehensive unit tests

Closes task: 2.3
Related: 2.1, 2.2"

# Hotfix
git commit -m "fix(auth): Fix JWT token expiration handling

- Update token refresh logic
- Add proper error handling for expired tokens  
- Improve user experience on token expiry

Fixes task: 5.2 (hotfix-security tag)"

# Refactor
git commit -m "refactor(llm): Unify LLM service providers

- Consolidate OpenAI, Claude, Gemini into UnifiedLLMService
- Add automatic fallback mechanism
- Improve error handling and logging
- Reduce code duplication by 60%

Task: 1.4 (refactor-llm-service tag)"
```

## ðŸ”§ REGRAS TÃ‰CNICAS ESPECÃFICAS

### **FastAPI & Endpoints**
```python
# âœ… Estrutura padrÃ£o de endpoint
@router.post("/endpoint", response_model=ResponseSchema, tags=["category"])
async def create_something(
    request: CreateSchema,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
) -> ResponseSchema:
    """
    DescriÃ§Ã£o clara do endpoint.
    
    Args:
        request: Dados da requisiÃ§Ã£o
        db: SessÃ£o do banco de dados
        current_user: UsuÃ¡rio autenticado
        
    Returns:
        ResponseSchema: Dados da resposta
        
    Raises:
        HTTPException: Quando ocorre erro especÃ­fico
    """
```

### **ðŸš¨ REGRA CRÃTICA - AUTENTICAÃ‡ÃƒO OBRIGATÃ“RIA**

**CONTEXTO**: Como todo SaaS profissional, pouquÃ­ssimos endpoints funcionam sem autenticaÃ§Ã£o. A grande maioria requer usuÃ¡rio logado.

**âœ… ENDPOINTS PÃšBLICOS PERMITIDOS (ExceÃ§Ãµes muito limitadas):**
```python
# âœ… AutenticaÃ§Ã£o & ConfiguraÃ§Ã£o do SDK
POST /auth/login          # Inicia sessÃ£o com credenciais
POST /auth/signup         # Cria nova conta de usuÃ¡rio  
POST /auth/refresh        # Gera novo token a partir do refresh
POST /auth/forgot-password # RecuperaÃ§Ã£o de senha
POST /auth/reset-password  # Reset de senha com token

# âœ… Status & ConfiguraÃ§Ã£o PÃºblica
GET  /                    # Root endpoint
GET  /health              # Health check bÃ¡sico
GET  /api/v1/health       # Health check da API
GET  /info                # InformaÃ§Ãµes da API
GET  /public/config       # Configs mÃ­nimas da API (chaves pÃºblicas, limites, branding)
```

**âŒ TODOS OS OUTROS ENDPOINTS REQUEREM AUTENTICAÃ‡ÃƒO:**
```python
# âŒ PROIBIDO: Endpoint sem autenticaÃ§Ã£o (exceto os listados acima)
@router.get("/some-endpoint")
async def endpoint_without_auth(db: Session = Depends(get_db)):
    pass

# âœ… OBRIGATÃ“RIO: Sempre incluir current_user dependency
@router.get("/some-endpoint")
async def endpoint_with_auth(
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)  # âœ… OBRIGATÃ“RIO
):
    pass
```

**ðŸ”’ PROTEÃ‡Ã•ES ADICIONAIS:**
- Mesmo endpoints "pÃºblicos" tÃªm rate limiting agressivo
- Endpoints de auth podem ter reCAPTCHA ou signatures temporÃ¡rias
- Logs de auditoria para todos os acessos
- Rate limiting especÃ­fico por IP para endpoints pÃºblicos

**âš ï¸ VERIFICAÃ‡ÃƒO PRÃ‰-COMMIT:**
- [ ] Novo endpoint tem `current_user: User = Depends(get_current_user)`?
- [ ] Endpoint estÃ¡ na lista de exceÃ§Ãµes pÃºblicas aprovadas?
- [ ] Rate limiting configurado adequadamente?
- [ ] Logs de auditoria implementados?

**ðŸŽ¯ REGRA DE OURO:**
> "Se nÃ£o estÃ¡ na lista de exceÃ§Ãµes pÃºblicas, DEVE ter autenticaÃ§Ã£o obrigatÃ³ria."

### **Models (SQLAlchemy 2.0+)**
```python
# âœ… PadrÃ£o de model completo
from sqlalchemy import String, Integer, DateTime, Boolean, Text, ForeignKey
from sqlalchemy.orm import Mapped, mapped_column, relationship
from sqlalchemy.sql import func
from synapse.database import Base

class MyModel(Base):
    __tablename__ = "my_models"
    __table_args__ = {"schema": "synapscale_db"}
    
    # âœ… Campos obrigatÃ³rios
    id: Mapped[int] = mapped_column(Integer, primary_key=True, index=True)
    name: Mapped[str] = mapped_column(String(255), nullable=False, index=True)
    
    # âœ… Timestamps automÃ¡ticos
    created_at: Mapped[datetime] = mapped_column(
        DateTime(timezone=True), 
        server_default=func.now(),
        nullable=False
    )
    updated_at: Mapped[datetime] = mapped_column(
        DateTime(timezone=True), 
        server_default=func.now(), 
        onupdate=func.now(),
        nullable=False
    )
    
    # âœ… Campos opcionais
    description: Mapped[str | None] = mapped_column(Text, nullable=True)
    is_active: Mapped[bool] = mapped_column(Boolean, default=True, nullable=False)
    
    # âœ… Foreign Keys com relacionamentos
    user_id: Mapped[int] = mapped_column(ForeignKey("synapscale_db.users.id"), nullable=False)
    user: Mapped["User"] = relationship("User", back_populates="my_models")
    
    # âœ… MÃ©todos Ãºteis
    def to_dict(self) -> dict:
        """Converte model para dicionÃ¡rio."""
        return {
            "id": self.id,
            "name": self.name,
            "created_at": self.created_at.isoformat(),
            "updated_at": self.updated_at.isoformat()
        }
    
    def __repr__(self) -> str:
        return f"<MyModel(id={self.id}, name='{self.name}')>"
```

### **Schemas (Pydantic v2)**
```python
# âœ… PadrÃ£o de schema completo
from pydantic import BaseModel, Field, ConfigDict, field_validator
from typing import Optional, List
from datetime import datetime
from enum import Enum

class StatusEnum(str, Enum):
    ACTIVE = "active"
    INACTIVE = "inactive"
    PENDING = "pending"

class BaseSchema(BaseModel):
    model_config = ConfigDict(
        from_attributes=True,
        use_enum_values=True,
        validate_assignment=True,
        arbitrary_types_allowed=False
    )

class CreateSchema(BaseSchema):
    name: str = Field(..., min_length=1, max_length=255, description="Nome do item")
    description: Optional[str] = Field(None, max_length=1000, description="DescriÃ§Ã£o opcional")
    is_active: bool = Field(True, description="Status ativo/inativo")
    
    @field_validator('name')
    @classmethod
    def validate_name(cls, v: str) -> str:
        if not v.strip():
            raise ValueError("Nome nÃ£o pode estar vazio")
        return v.strip()

class UpdateSchema(BaseSchema):
    name: Optional[str] = Field(None, min_length=1, max_length=255)
    description: Optional[str] = Field(None, max_length=1000)
    is_active: Optional[bool] = None

class ResponseSchema(BaseSchema):
    id: int
    name: str
    description: Optional[str] = None
    is_active: bool
    status: StatusEnum
    created_at: datetime
    updated_at: datetime
    
    # âœ… Campos computados
    display_name: str = Field(..., description="Nome formatado para exibiÃ§Ã£o")
    
    @field_validator('display_name', mode='before')
    @classmethod
    def set_display_name(cls, v, info):
        if info.data.get('name'):
            return info.data['name'].title()
        return v

class ListResponseSchema(BaseSchema):
    items: List[ResponseSchema]
    total: int
    page: int = Field(ge=1)
    per_page: int = Field(ge=1, le=100)
    pages: int
```

### **Services**
```python
# âœ… PadrÃ£o de service
class MyService:
    @staticmethod
    async def create_something(
        db: Session,
        data: CreateSchema,
        user_id: int
    ) -> MyModel:
        """
        Cria um novo item.
        """
        try:
            # LÃ³gica do serviÃ§o
            return new_item
        except Exception as e:
            logger.error(f"Erro ao criar item: {e}")
            raise HTTPException(status_code=500, detail="Erro interno")
```

## ðŸ”’ SEGURANÃ‡A & VALIDAÃ‡ÃƒO

### **AutenticaÃ§Ã£o & AutorizaÃ§Ã£o**
```python
# âœ… Estrutura completa de autenticaÃ§Ã£o
from synapse.api.deps import get_current_user, check_workspace_permission, rate_limit
from synapse.core.auth.jwt import verify_token
from synapse.middlewares.rate_limiting import RateLimiter

@router.post("/secure-endpoint")
@rate_limit("10/minute")  # Rate limiting especÃ­fico
async def secure_endpoint(
    request: CreateSchema,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user),
    workspace_id: int = Path(..., description="ID do workspace")
):
    # âœ… Verificar permissÃ£o no workspace
    await check_workspace_permission(db, current_user.id, workspace_id, "read")
    
    # âœ… Log de auditoria
    logger.info(f"User {current_user.id} accessed workspace {workspace_id}")
```

### **ValidaÃ§Ã£o de Dados & SanitizaÃ§Ã£o**
```python
# âœ… ValidaÃ§Ã£o robusta
import bleach
from pydantic import field_validator
from typing import Any

class SecureSchema(BaseModel):
    content: str = Field(..., max_length=10000)
    email: str = Field(..., regex=r'^[^@]+@[^@]+\.[^@]+$')
    
    @field_validator('content')
    @classmethod
    def sanitize_content(cls, v: str) -> str:
        """Remove HTML tags maliciosos."""
        return bleach.clean(v, tags=['p', 'br', 'strong', 'em'])
    
    @field_validator('email')
    @classmethod
    def validate_email_domain(cls, v: str) -> str:
        """Validar domÃ­nios permitidos."""
        allowed_domains = ['company.com', 'trusted.org']
        domain = v.split('@')[1]
        if domain not in allowed_domains:
            raise ValueError(f"DomÃ­nio {domain} nÃ£o permitido")
        return v.lower()
```

### **Rate Limiting AvanÃ§ado**
```python
# âœ… Rate limiting por tipo de operaÃ§Ã£o
from synapse.middlewares.rate_limiting import RateLimiter

@router.post("/upload")
@rate_limit("5/minute", key_func=lambda request: f"upload:{request.client.host}")
async def upload_file(file: UploadFile):
    pass

@router.post("/llm/generate")
@rate_limit("20/hour", key_func=lambda request: f"llm:{request.user.id}")
async def generate_response():
    pass
```

### **Tratamento de Erros**
```python
# âœ… PadrÃ£o de tratamento de erro
try:
    result = await some_operation()
except SpecificException as e:
    logger.error(f"Erro especÃ­fico: {e}")
    raise HTTPException(status_code=400, detail="Mensagem clara para o usuÃ¡rio")
except Exception as e:
    logger.error(f"Erro inesperado: {e}")
    raise HTTPException(status_code=500, detail="Erro interno do servidor")
```

## ðŸ—„ï¸ BANCO DE DADOS & MIGRAÃ‡Ã•ES

### **Consultas Otimizadas**
```python
# âœ… PadrÃµes de consulta eficientes
from sqlalchemy.orm import selectinload, joinedload
from sqlalchemy import select, and_, or_, func

# Busca simples por ID
user = await session.get(User, user_id)

# Consulta com relacionamentos (N+1 problem solved)
stmt = select(User).options(
    joinedload(User.workspaces),
    selectinload(User.files)
).where(User.is_active == True)
users = await session.execute(stmt)

# Consulta com filtros complexos
stmt = select(Workflow).where(
    and_(
        Workflow.user_id == user_id,
        or_(
            Workflow.status == "active",
            Workflow.status == "pending"
        ),
        Workflow.created_at >= datetime.now() - timedelta(days=30)
    )
).order_by(Workflow.created_at.desc()).limit(10)

# AgregaÃ§Ãµes
stmt = select(
    func.count(Workflow.id).label('total'),
    func.avg(Workflow.execution_time).label('avg_time')
).where(Workflow.user_id == user_id)
```

### **TransaÃ§Ãµes & Context Managers**
```python
# âœ… TransaÃ§Ãµes seguras
from sqlalchemy.exc import SQLAlchemyError

async def complex_operation(db: Session, data: dict):
    try:
        async with db.begin():
            # OperaÃ§Ã£o 1
            user = User(**data['user'])
            db.add(user)
            await db.flush()  # Para obter o ID
            
            # OperaÃ§Ã£o 2 dependente
            workspace = Workspace(user_id=user.id, **data['workspace'])
            db.add(workspace)
            
            # OperaÃ§Ã£o 3
            await create_default_settings(db, user.id)
            
            # Commit automÃ¡tico se tudo der certo
            return user
    except SQLAlchemyError as e:
        logger.error(f"Database error: {e}")
        # Rollback automÃ¡tico
        raise HTTPException(status_code=500, detail="Database operation failed")
```

### **MigraÃ§Ãµes Alembic**
```python
# âœ… Estrutura de migraÃ§Ã£o padrÃ£o
"""add_user_preferences_table

Revision ID: 2024_01_15_user_preferences
Revises: 2024_01_10_workspace_settings
Create Date: 2024-01-15 10:30:00.000000

"""
from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers
revision = '2024_01_15_user_preferences'
down_revision = '2024_01_10_workspace_settings'
branch_labels = None
depends_on = None

def upgrade():
    # âœ… Criar tabela com todos os indexes necessÃ¡rios
    op.create_table(
        'user_preferences',
        sa.Column('id', sa.Integer(), nullable=False),
        sa.Column('user_id', sa.Integer(), nullable=False),
        sa.Column('theme', sa.String(20), nullable=False, default='light'),
        sa.Column('language', sa.String(10), nullable=False, default='pt-BR'),
        sa.Column('notifications', postgresql.JSONB(), nullable=True),
        sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.func.now()),
        sa.Column('updated_at', sa.DateTime(timezone=True), server_default=sa.func.now()),
        sa.PrimaryKeyConstraint('id'),
        sa.ForeignKeyConstraint(['user_id'], ['synapscale_db.users.id'], ondelete='CASCADE'),
        schema='synapscale_db'
    )
    
    # âœ… Indexes para performance
    op.create_index('ix_user_preferences_user_id', 'user_preferences', ['user_id'], schema='synapscale_db')
    op.create_index('ix_user_preferences_theme', 'user_preferences', ['theme'], schema='synapscale_db')

def downgrade():
    op.drop_table('user_preferences', schema='synapscale_db')
```

### **Schema Management**
- SEMPRE usar schema `synapscale_db` em production
- Testar migraÃ§Ãµes em development primeiro
- Fazer backup antes de migrations em production
- Usar `--dry-run` para validar migraÃ§Ãµes complexas
- Documentar breaking changes no commit message

## ðŸ¤– INTEGRAÃ‡ÃƒO COM LLM UNIFICADA

### **Provedores Suportados**
- **OpenAI**: GPT-3.5, GPT-4, GPT-4-turbo, GPT-4o
- **Anthropic**: Claude 3 (Haiku, Sonnet, Opus), Claude 3.5
- **Google**: Gemini Pro, Gemini Pro Vision
- **Grok**: XAI Grok models
- **DeepSeek**: DeepSeek Coder, DeepSeek Chat
- **Llama**: Meta Llama 2, Llama 3
- **Tess**: Pareto Tess (modelo nacional)

### **Unified LLM Service**
```python
# âœ… ServiÃ§o unificado com tratamento de erros
from synapse.core.llm.unified_service import UnifiedLLMService
from synapse.models.conversation_llm import ConversationLLM
from synapse.models.usage_log import UsageLog

async def generate_ai_response(
    user_id: int,
    messages: List[dict],
    provider: str = "claude",
    model: str = "claude-3-sonnet",
    temperature: float = 0.7,
    max_tokens: int = 2000
) -> dict:
    """
    Gera resposta usando o serviÃ§o unificado de LLM.
    """
    llm_service = UnifiedLLMService()
    
    try:
        # âœ… Log de inÃ­cio da operaÃ§Ã£o
        logger.info(f"Generating LLM response for user {user_id} with {provider}/{model}")
        
        # âœ… Verificar cota do usuÃ¡rio
        usage_today = await check_daily_usage(user_id)
        if usage_today > settings.MAX_DAILY_LLM_REQUESTS:
            raise HTTPException(status_code=429, detail="Daily LLM quota exceeded")
        
        # âœ… Gerar resposta
        response = await llm_service.generate_response(
            provider=provider,
            model=model,
            messages=messages,
            user_id=user_id,
            temperature=temperature,
            max_tokens=max_tokens,
            stream=False
        )
        
        # âœ… Salvar conversa no banco
        conversation = ConversationLLM(
            user_id=user_id,
            provider=provider,
            model=model,
            input_tokens=response.get("usage", {}).get("prompt_tokens", 0),
            output_tokens=response.get("usage", {}).get("completion_tokens", 0),
            total_tokens=response.get("usage", {}).get("total_tokens", 0),
            cost=calculate_cost(provider, model, response.get("usage", {})),
            response_time=response.get("response_time", 0)
        )
        await db.add(conversation)
        
        # âœ… Log de uso para analytics
        await log_usage(
            user_id=user_id,
            action="llm_generation",
            provider=provider,
            model=model,
            tokens_used=response.get("usage", {}).get("total_tokens", 0)
        )
        
        return {
            "content": response["content"],
            "model": model,
            "provider": provider,
            "usage": response.get("usage"),
            "conversation_id": conversation.id
        }
        
    except Exception as e:
        logger.error(f"LLM generation failed for user {user_id}: {e}")
        
        # âœ… Tentar fallback provider
        if provider != "claude":  # Fallback padrÃ£o
            logger.info(f"Attempting fallback to Claude for user {user_id}")
            return await generate_ai_response(
                user_id=user_id,
                messages=messages,
                provider="claude",
                model="claude-3-sonnet",
                temperature=temperature,
                max_tokens=max_tokens
            )
        
        raise HTTPException(
            status_code=500, 
            detail=f"LLM service unavailable: {str(e)}"
        )

# âœ… Streaming responses
async def stream_ai_response(
    user_id: int,
    messages: List[dict],
    provider: str = "openai",
    model: str = "gpt-4"
):
    """Stream de resposta LLM em tempo real."""
    llm_service = UnifiedLLMService()
    
    async for chunk in llm_service.generate_response_stream(
        provider=provider,
        model=model,
        messages=messages,
        user_id=user_id
    ):
        yield f"data: {json.dumps(chunk)}\n\n"
```

### **Gerenciamento de API Keys por UsuÃ¡rio**
```python
# âœ… Sistema de chaves personalizadas
from synapse.models.user_variable import UserVariable
from synapse.core.security import encrypt_value, decrypt_value

async def get_user_api_key(
    db: Session, 
    user_id: int, 
    provider: str
) -> Optional[str]:
    """Busca chave API especÃ­fica do usuÃ¡rio ou usa a padrÃ£o."""
    
    # Tentar chave do usuÃ¡rio primeiro
    user_key = await db.execute(
        select(UserVariable).where(
            and_(
                UserVariable.user_id == user_id,
                UserVariable.key == f"{provider.upper()}_API_KEY",
                UserVariable.is_active == True
            )
        )
    )
    
    if user_key.scalar_one_or_none():
        return decrypt_value(user_key.value)
    
    # Fallback para chave padrÃ£o do sistema
    return settings.get(f"{provider.upper()}_API_KEY")

async def save_user_api_key(
    db: Session,
    user_id: int,
    provider: str,
    api_key: str
) -> UserVariable:
    """Salva chave API criptografada do usuÃ¡rio."""
    
    encrypted_key = encrypt_value(api_key)
    
    user_var = UserVariable(
        user_id=user_id,
        key=f"{provider.upper()}_API_KEY",
        value=encrypted_key,
        category="llm_config",
        is_encrypted=True,
        description=f"Chave API pessoal para {provider.title()}"
    )
    
    db.add(user_var)
    await db.commit()
    
    logger.info(f"API key saved for user {user_id}, provider {provider}")
    return user_var
```

## ðŸ”Œ WEBSOCKETS & TEMPO REAL

### **Connection Manager**
```python
# âœ… Gerenciador de conexÃµes WebSocket
from synapse.core.websockets.manager import ConnectionManager
from synapse.core.websockets.execution_manager import ExecutionManager

class WorkspaceConnectionManager:
    def __init__(self):
        self.active_connections: Dict[int, List[WebSocket]] = {}
        self.user_connections: Dict[int, List[WebSocket]] = {}
    
    async def connect(self, websocket: WebSocket, user_id: int, workspace_id: int):
        await websocket.accept()
        
        # âœ… Limitar conexÃµes por usuÃ¡rio
        if user_id not in self.user_connections:
            self.user_connections[user_id] = []
        
        if len(self.user_connections[user_id]) >= settings.WS_MAX_CONNECTIONS_PER_USER:
            await websocket.close(code=1013, reason="Too many connections")
            return
        
        # âœ… Adicionar Ã  lista de conexÃµes
        if workspace_id not in self.active_connections:
            self.active_connections[workspace_id] = []
        
        self.active_connections[workspace_id].append(websocket)
        self.user_connections[user_id].append(websocket)
        
        logger.info(f"WebSocket connected: user={user_id}, workspace={workspace_id}")
    
    async def disconnect(self, websocket: WebSocket, user_id: int, workspace_id: int):
        # âœ… Remover das listas
        if workspace_id in self.active_connections:
            self.active_connections[workspace_id].remove(websocket)
        
        if user_id in self.user_connections:
            self.user_connections[user_id].remove(websocket)
        
        logger.info(f"WebSocket disconnected: user={user_id}, workspace={workspace_id}")
    
    async def broadcast_to_workspace(self, workspace_id: int, message: dict):
        """Envia mensagem para todos os conectados no workspace."""
        if workspace_id not in self.active_connections:
            return
        
        dead_connections = []
        for connection in self.active_connections[workspace_id]:
            try:
                await connection.send_json(message)
            except ConnectionClosedError:
                dead_connections.append(connection)
        
        # âœ… Limpar conexÃµes mortas
        for dead_conn in dead_connections:
            self.active_connections[workspace_id].remove(dead_conn)

# âœ… InstÃ¢ncia global
connection_manager = WorkspaceConnectionManager()
```

### **WebSocket Endpoints**
```python
# âœ… Endpoint WebSocket para execuÃ§Ã£o de workflows
@router.websocket("/ws/execution/{execution_id}")
async def websocket_execution(
    websocket: WebSocket,
    execution_id: str,
    token: str = Query(...),
    db: Session = Depends(get_db)
):
    # âœ… Validar token
    try:
        payload = verify_token(token)
        user_id = payload.get("sub")
        current_user = await get_user_by_id(db, user_id)
    except JWTError:
        await websocket.close(code=1008, reason="Invalid token")
        return
    
    # âœ… Verificar permissÃ£o na execuÃ§Ã£o
    execution = await get_execution_by_id(db, execution_id)
    if not execution or execution.user_id != current_user.id:
        await websocket.close(code=1008, reason="Unauthorized")
        return
    
    await connection_manager.connect(websocket, current_user.id, execution.workspace_id)
    
    try:
        while True:
            # âœ… Heartbeat para manter conexÃ£o viva
            await websocket.send_json({
                "type": "heartbeat",
                "timestamp": datetime.now().isoformat()
            })
            await asyncio.sleep(30)
            
    except WebSocketDisconnect:
        await connection_manager.disconnect(websocket, current_user.id, execution.workspace_id)
```

## ðŸ“Š ANALYTICS & MÃ‰TRICAS

### **Sistema de Analytics**
```python
# âœ… Service de analytics completo
from synapse.services.analytics_service import AnalyticsService
from synapse.models.usage_log import UsageLog

class AnalyticsService:
    @staticmethod
    async def track_event(
        db: Session,
        user_id: int,
        event_type: str,
        metadata: dict = None,
        workspace_id: int = None
    ):
        """Registra evento para analytics."""
        usage_log = UsageLog(
            user_id=user_id,
            action=event_type,
            resource_type="analytics",
            metadata=metadata or {},
            workspace_id=workspace_id
        )
        
        db.add(usage_log)
        await db.commit()
        
        # âœ… Cache para analytics em tempo real
        await cache_analytics_event(event_type, metadata)
    
    @staticmethod
    async def get_dashboard_metrics(
        db: Session,
        user_id: int,
        workspace_id: int,
        period: str = "30d"
    ) -> dict:
        """MÃ©tricas para dashboard."""
        
        # âœ… Calcular perÃ­odo
        end_date = datetime.now()
        if period == "7d":
            start_date = end_date - timedelta(days=7)
        elif period == "30d":
            start_date = end_date - timedelta(days=30)
        else:
            start_date = end_date - timedelta(days=90)
        
        # âœ… MÃ©tricas de workflows
        workflow_stats = await db.execute(
            select(
                func.count(Workflow.id).label('total_workflows'),
                func.count(case((Workflow.status == 'completed', 1))).label('completed'),
                func.count(case((Workflow.status == 'failed', 1))).label('failed'),
                func.avg(Workflow.execution_time).label('avg_execution_time')
            ).where(
                and_(
                    Workflow.workspace_id == workspace_id,
                    Workflow.created_at >= start_date
                )
            )
        )
        
        # âœ… MÃ©tricas de LLM
        llm_stats = await db.execute(
            select(
                func.count(ConversationLLM.id).label('total_llm_calls'),
                func.sum(ConversationLLM.total_tokens).label('total_tokens'),
                func.sum(ConversationLLM.cost).label('total_cost')
            ).where(
                and_(
                    ConversationLLM.user_id == user_id,
                    ConversationLLM.created_at >= start_date
                )
            )
        )
        
        return {
            "period": period,
            "workflows": workflow_stats.first()._asdict(),
            "llm_usage": llm_stats.first()._asdict(),
            "generated_at": datetime.now().isoformat()
        }

# âœ… Middleware de mÃ©tricas
from synapse.middlewares.metrics import MetricsMiddleware

@router.middleware("http")
async def metrics_middleware(request: Request, call_next):
    start_time = time.time()
    
    response = await call_next(request)
    
    duration = time.time() - start_time
    
    # âœ… Log mÃ©tricas de performance
    await log_request_metrics(
        method=request.method,
        url=str(request.url),
        status_code=response.status_code,
        duration=duration,
        user_id=getattr(request.state, 'user_id', None)
    )
    
    return response
```

## ðŸ·ï¸ SISTEMA DE TAGS

### **Tag Management**
```python
# âœ… Sistema completo de tags
from synapse.models.tag import Tag
from synapse.services.tag_service import TagService

class TagService:
    @staticmethod
    async def create_tag(
        db: Session,
        user_id: int,
        workspace_id: int,
        name: str,
        color: str = "#3B82F6",
        description: str = None
    ) -> Tag:
        """Cria nova tag com validaÃ§Ãµes."""
        
        # âœ… Validar se tag jÃ¡ existe
        existing = await db.execute(
            select(Tag).where(
                and_(
                    Tag.workspace_id == workspace_id,
                    func.lower(Tag.name) == name.lower()
                )
            )
        )
        
        if existing.scalar_one_or_none():
            raise HTTPException(
                status_code=400, 
                detail=f"Tag '{name}' jÃ¡ existe neste workspace"
            )
        
        # âœ… Validar limite de tags por workspace
        tag_count = await db.execute(
            select(func.count(Tag.id)).where(Tag.workspace_id == workspace_id)
        )
        
        if tag_count.scalar() >= settings.MAX_TAGS_PER_WORKSPACE:
            raise HTTPException(
                status_code=400,
                detail="Limite de tags por workspace atingido"
            )
        
        tag = Tag(
            name=name.strip(),
            color=color,
            description=description,
            workspace_id=workspace_id,
            created_by=user_id
        )
        
        db.add(tag)
        await db.commit()
        
        logger.info(f"Tag created: {name} in workspace {workspace_id}")
        return tag
    
    @staticmethod
    async def apply_tags(
        db: Session,
        resource_type: str,  # 'workflow', 'file', 'conversation'
        resource_id: int,
        tag_ids: List[int],
        workspace_id: int
    ):
        """Aplica tags a um recurso."""
        
        # âœ… Validar que todas as tags pertencem ao workspace
        tags = await db.execute(
            select(Tag).where(
                and_(
                    Tag.id.in_(tag_ids),
                    Tag.workspace_id == workspace_id
                )
            )
        )
        
        valid_tags = tags.scalars().all()
        if len(valid_tags) != len(tag_ids):
            raise HTTPException(
                status_code=400,
                detail="Algumas tags nÃ£o pertencem ao workspace"
            )
        
        # âœ… Aplicar tags (usando tabela de associaÃ§Ã£o)
        for tag in valid_tags:
            association = ResourceTag(
                resource_type=resource_type,
                resource_id=resource_id,
                tag_id=tag.id
            )
            db.add(association)
        
        await db.commit()

# âœ… Endpoint para busca por tags
@router.get("/search/by-tags")
async def search_by_tags(
    tag_ids: List[int] = Query(...),
    resource_type: str = Query(...),
    workspace_id: int = Query(...),
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """Busca recursos por tags."""
    
    # âœ… Query complexa com tags
    resources = await db.execute(
        select(ResourceTag.resource_id)
        .where(
            and_(
                ResourceTag.tag_id.in_(tag_ids),
                ResourceTag.resource_type == resource_type
            )
        )
        .group_by(ResourceTag.resource_id)
        .having(func.count(ResourceTag.tag_id) == len(tag_ids))  # AND logic
    )
    
    return {"resource_ids": [r[0] for r in resources.fetchall()]}
```

## ðŸ“ FILE UPLOAD & STORAGE

### **Storage Manager**
```python
# âœ… Sistema unificado de storage
from synapse.core.storage.storage_manager import StorageManager
from synapse.models.file import File

class StorageManager:
    def __init__(self):
        self.storage_type = settings.STORAGE_TYPE
        self.base_path = settings.STORAGE_BASE_PATH
    
    async def upload_file(
        self,
        file: UploadFile,
        user_id: int,
        workspace_id: int,
        category: str = "general"
    ) -> File:
        """Upload com validaÃ§Ãµes completas."""
        
        # âœ… ValidaÃ§Ãµes de seguranÃ§a
        await self._validate_file(file)
        
        # âœ… Gerar caminho Ãºnico
        file_id = str(uuid.uuid4())
        file_extension = Path(file.filename).suffix
        filename = f"{file_id}{file_extension}"
        
        # âœ… Organizar por categoria e data
        today = datetime.now()
        file_path = f"{category}/{today.year}/{today.month:02d}/{filename}"
        
        try:
            # âœ… Salvar arquivo
            if self.storage_type == "local":
                await self._save_local(file, file_path)
            elif self.storage_type == "s3":
                await self._save_s3(file, file_path)
            elif self.storage_type == "gcs":
                await self._save_gcs(file, file_path)
            
            # âœ… Criar registro no banco
            file_record = File(
                id=file_id,
                filename=file.filename,
                original_filename=file.filename,
                file_path=file_path,
                file_size=file.size,
                mime_type=file.content_type,
                category=category,
                user_id=user_id,
                workspace_id=workspace_id,
                storage_type=self.storage_type
            )
            
            return file_record
            
        except Exception as e:
            logger.error(f"File upload failed: {e}")
            raise HTTPException(status_code=500, detail="Upload failed")
    
    async def _validate_file(self, file: UploadFile):
        """ValidaÃ§Ãµes de seguranÃ§a do arquivo."""
        
        # âœ… Validar tamanho
        if file.size > settings.MAX_FILE_SIZE:
            raise HTTPException(
                status_code=413,
                detail=f"File too large. Max size: {settings.MAX_FILE_SIZE} bytes"
            )
        
        # âœ… Validar extensÃ£o
        file_extension = Path(file.filename).suffix.lower()
        if file_extension not in settings.ALLOWED_EXTENSIONS:
            raise HTTPException(
                status_code=400,
                detail=f"File type not allowed: {file_extension}"
            )
        
        # âœ… Validar MIME type
        if file.content_type not in settings.ALLOWED_MIME_TYPES:
            raise HTTPException(
                status_code=400,
                detail=f"MIME type not allowed: {file.content_type}"
            )
        
        # âœ… Scan bÃ¡sico de malware (se configurado)
        if settings.ENABLE_MALWARE_SCAN:
            await self._scan_malware(file)

# âœ… Endpoint de upload com mÃºltiplos arquivos
@router.post("/upload/multiple")
async def upload_multiple_files(
    files: List[UploadFile] = File(...),
    workspace_id: int = Form(...),
    category: str = Form("general"),
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """Upload de mÃºltiplos arquivos."""
    
    if len(files) > settings.MAX_FILES_PER_UPLOAD:
        raise HTTPException(
            status_code=400,
            detail=f"Too many files. Max: {settings.MAX_FILES_PER_UPLOAD}"
        )
    
    storage_manager = StorageManager()
    uploaded_files = []
    
    for file in files:
        try:
            file_record = await storage_manager.upload_file(
                file=file,
                user_id=current_user.id,
                workspace_id=workspace_id,
                category=category
            )
            
            db.add(file_record)
            uploaded_files.append(file_record)
            
        except HTTPException:
            # âœ… Se um arquivo falhar, continuar com os outros
            logger.warning(f"Failed to upload file: {file.filename}")
            continue
    
    await db.commit()
    
    return {
        "uploaded_files": len(uploaded_files),
        "total_files": len(files),
        "files": [f.to_dict() for f in uploaded_files]
    }
```

## ðŸ“Š SISTEMA DE LOGGING UNIFICADO - REGRAS CRÃTICAS

### **ðŸš¨ ÃšNICA FONTE DE VERDADE: `src/synapse/logger_config.py`**

**CONTEXTO**: O SynapScale possui um sistema de logging unificado que integra as melhores funcionalidades de logging estruturado, mantendo compatibilidade total com cÃ³digo existente.

**âŒ ARQUIVOS REMOVIDOS (NUNCA RECRIAR):**
- `src/synapse/core/logging_system.py` - DELETADO (era sistema avanÃ§ado Ã³rfÃ£o)
- `src/synapse/middlewares/logging.py` - DELETADO (funcionalidade duplicada)

**âœ… ÃšNICO ARQUIVO VÃLIDO:**
- `src/synapse/logger_config.py` - Sistema unificado principal

### **ImportaÃ§Ã£o ObrigatÃ³ria**
```python
# âœ… SEMPRE use esta importaÃ§Ã£o Ãºnica
from synapse.logger_config import get_logger, get_error_stats

# âœ… Instanciar logger
logger = get_logger(__name__)  # Retorna UnifiedLogger com todas as funcionalidades

# âŒ NUNCA use estas importaÃ§Ãµes (arquivos deletados)
from synapse.core.logging_system import get_logger  # âŒ PROIBIDO
from synapse.middlewares.logging import LoggingMiddleware  # âŒ PROIBIDO
```

### **Sistema Unificado - Funcionalidades Integradas**

**ðŸ”§ CARACTERÃSTICAS TÃ‰CNICAS:**
- **UnifiedLogger**: Combina funcionalidades bÃ¡sicas + avanÃ§adas
- **ErrorTracker**: Rastreamento automÃ¡tico de erros com estatÃ­sticas
- **UnifiedJSONFormatter**: FormataÃ§Ã£o JSON estruturada para produÃ§Ã£o
- **Handlers Rotativos**: 10MB principal, 5MB erros (automÃ¡tico em produÃ§Ã£o)
- **Compatibilidade Total**: Interface `get_logger()` inalterada

### **PadrÃµes de Uso Corretos**

```python
# âœ… Logging bÃ¡sico (interface compatÃ­vel)
from synapse.logger_config import get_logger

logger = get_logger(__name__)

# Funciona exatamente como antes
logger.logger.info("OperaÃ§Ã£o realizada com sucesso")
logger.logger.error("Erro encontrado", extra={"user_id": 123})

# âœ… Logging estruturado avanÃ§ado (JSON automÃ¡tico em produÃ§Ã£o)
logger.logger.info("User action performed", extra={
    "user_id": user.id,
    "workspace_id": workspace.id,
    "action": "create_workflow",
    "execution_time": 0.245,
    "ip_address": request.client.host,
    "endpoint_category": "workflows"
})

# âœ… Logging de erro com rastreamento automÃ¡tico
try:
    result = await complex_operation()
except Exception as e:
    logger.logger.error("Operation failed", extra={
        "user_id": user.id,
        "operation": "complex_operation",
        "error_type": type(e).__name__,
        "error_count": 1,
        "url": "/api/v1/operation",
        "method": "POST"
    })
    raise

# âœ… Acessar estatÃ­sticas de erro
from synapse.logger_config import get_error_stats

stats = get_error_stats()
# Retorna: uptime, total_errors, error_rate, error_counts_by_type, etc.
```

### **Context Fields Suportados**

O sistema reconhece automaticamente estes campos no `extra`:
```python
context_fields = [
    "request_id",        # ID Ãºnico da requisiÃ§Ã£o
    "user_id",          # ID do usuÃ¡rio
    "endpoint_category", # Categoria do endpoint (workflows, llm, etc.)
    "url",              # URL da requisiÃ§Ã£o
    "method",           # MÃ©todo HTTP
    "status_code",      # CÃ³digo de status HTTP
    "process_time",     # Tempo de processamento
    "error_type",       # Tipo do erro (ValidationError, DatabaseError)
    "error_count"       # Contador de erros
]

# âœ… Exemplo completo
logger.logger.warning("Rate limit approached", extra={
    "request_id": "req-123",
    "user_id": user.id,
    "endpoint_category": "llm",
    "url": "/api/v1/llm/generate",
    "method": "POST",
    "error_type": "RateLimitWarning",
    "remaining_requests": 5
})
```

### **Ambientes e FormataÃ§Ã£o**

```python
# ðŸ—ï¸ DESENVOLVIMENTO: Formato legÃ­vel
# 2024-01-15 10:30:00,123 - INFO - synapse.api.endpoints - User action performed

# ðŸš€ PRODUÃ‡ÃƒO: JSON estruturado automÃ¡tico
{
    "timestamp": "2024-01-15T10:30:00.123Z",
    "level": "INFO",
    "logger": "synapse.api.endpoints",
    "message": "User action performed",
    "module": "endpoints",
    "function": "create_workflow",
    "line": 45,
    "user_id": 123,
    "workspace_id": 456,
    "action": "create_workflow",
    "execution_time": 0.245
}
```

### **Performance Logging Pattern**

```python
# âœ… Context manager para performance
import time
from contextlib import asynccontextmanager

@asynccontextmanager
async def log_performance(operation: str, user_id: int = None):
    start_time = time.time()
    try:
        yield
    finally:
        duration = time.time() - start_time
        logger.logger.info("Performance metric", extra={
            "operation": operation,
            "duration": duration,
            "user_id": user_id,
            "slow": duration > 1.0  # Flag para operaÃ§Ãµes lentas
        })

# âœ… Uso do context manager
async def slow_operation():
    async with log_performance("database_migration", user_id=123):
        await run_migration()
```

### **Error Tracking AutomÃ¡tico**

```python
# âœ… O ErrorTracker funciona automaticamente
# Cada erro logado Ã© automaticamente rastreado

# Exemplo: Endpoint com erro
@router.post("/api/v1/workflows")
async def create_workflow(request: CreateWorkflowRequest):
    try:
        # ... lÃ³gica ...
        pass
    except ValidationError as e:
        # âœ… Erro Ã© automaticamente rastreado pelo sistema
        logger.logger.error("Validation failed", extra={
            "error_type": "ValidationError",
            "endpoint_category": "workflows",
            "url": "/api/v1/workflows",
            "user_id": request.user_id
        })
        raise HTTPException(status_code=400, detail=str(e))

# âœ… Acessar estatÃ­sticas
stats = get_error_stats()
print(f"Taxa de erro: {stats['error_rate']:.2f} erros/segundo")
print(f"Total de erros: {stats['total_errors']}")
print(f"Erros por tipo: {stats['error_counts_by_type']}")
```

### **Handlers de Arquivo (ProduÃ§Ã£o)**

```python
# âœ… ConfiguraÃ§Ã£o automÃ¡tica em produÃ§Ã£o
# - logs/synapse.log (10MB, 5 rotaÃ§Ãµes)
# - logs/synapse_errors.log (5MB, 3 rotaÃ§Ãµes)

# AtivaÃ§Ã£o manual em desenvolvimento:
# export ENABLE_FILE_LOGGING=true

# âœ… Estrutura de logs
logs/
â”œâ”€â”€ synapse.log              # Logs principais
â”œâ”€â”€ synapse.log.1           # RotaÃ§Ã£o 1
â”œâ”€â”€ synapse.log.2           # RotaÃ§Ã£o 2
â”œâ”€â”€ synapse_errors.log      # Apenas erros (ERROR+)
â””â”€â”€ synapse_errors.log.1    # RotaÃ§Ã£o de erros
```

### **Middleware Integration**

```python
# âœ… IntegraÃ§Ã£o com middleware de requisiÃ§Ãµes
from synapse.logger_config import RequestContextFilter

# O sistema automaticamente captura:
# - request_id de headers X-Request-ID
# - user_id de token JWT
# - timing de requisiÃ§Ãµes
# - status codes
# - URLs e mÃ©todos HTTP

# âœ… Exemplo de log automÃ¡tico de middleware
# INFO: Request completed {"request_id": "req-123", "user_id": 456, "method": "POST", 
#       "url": "/api/v1/workflows", "status_code": 201, "process_time": 0.245}
```

### **Regras de MigraÃ§Ã£o (Para Desenvolvedores)**

```python
# âœ… ANTES (cÃ³digo antigo compatÃ­vel)
from synapse.logger_config import get_logger
logger = get_logger("my_module")
logger.logger.info("Message")

# âœ… DEPOIS (mesmo cÃ³digo, funcionalidades extras automÃ¡ticas)
from synapse.logger_config import get_logger
logger = get_logger("my_module")  # Agora retorna UnifiedLogger
logger.logger.info("Message", extra={"user_id": 123})  # JSON em produÃ§Ã£o

# âŒ NUNCA FAZER
from synapse.core.logging_system import get_logger  # Arquivo deletado
from synapse.middlewares.logging import LoggingMiddleware  # Arquivo deletado
```

### **MÃ©tricas & Monitoramento**
- ErrorTracker integrado com estatÃ­sticas em tempo real
- Logs rotativos automÃ¡ticos em produÃ§Ã£o
- JSON estruturado para integraÃ§Ã£o com Loki/Grafana
- Context fields padronizados para correlaÃ§Ã£o
- Performance tracking com flags automÃ¡ticas

### **Testes do Sistema Unificado**

```python
# âœ… Executar testes do sistema unificado
python test_error_handling_system.py       # Teste completo de error handling + logging
python scripts/test_centralized_logging.py # Teste especÃ­fico de logging centralizado

# âœ… Ambos devem passar com sucesso (exit code 0)
# âœ… Logs devem aparecer no formato correto (desenvolvimento vs produÃ§Ã£o)
# âœ… ErrorTracker deve funcionar automaticamente
# âœ… Arquivos de log devem ser criados em logs/ (se ENABLE_FILE_LOGGING=true)
```

### **Estrutura Final Consolidada**

```
src/synapse/
â”œâ”€â”€ logger_config.py          # âœ… SISTEMA UNIFICADO ÃšNICO
â”œâ”€â”€ error_handlers.py         # âœ… Handlers de erro integrados
â””â”€â”€ exceptions.py             # âœ… ExceÃ§Ãµes customizadas

âŒ ARQUIVOS REMOVIDOS:
â”œâ”€â”€ core/logging_system.py    # âŒ DELETADO (redundante)
â””â”€â”€ middlewares/logging.py    # âŒ DELETADO (integrado)

logs/                         # âœ… Logs automÃ¡ticos em produÃ§Ã£o
â”œâ”€â”€ synapse.log              # Logs principais (10MB rotativo)
â”œâ”€â”€ synapse_errors.log       # Apenas erros (5MB rotativo)
â””â”€â”€ *.log.1, *.log.2        # RotaÃ§Ãµes automÃ¡ticas
```

### **Regra de Ouro: NUNCA RECRIAR**
Se vocÃª encontrar cÃ³digo tentando importar dos arquivos deletados, **SEMPRE** migre para o sistema unificado. **NUNCA** recrie os arquivos antigos. Use apenas:

```python
from synapse.logger_config import get_logger, get_error_stats
```

---

## ðŸ—„ï¸ SISTEMA DE DATABASE UNIFICADO - REGRAS CRÃTICAS

### **ðŸš¨ ÃšNICA FONTE DE VERDADE: `src/synapse/database.py`**

**CONTEXTO**: O SynapScale possui um sistema de database unificado que centraliza toda a configuraÃ§Ã£o de conexÃ£o, sessÃµes e engine SQLAlchemy, mantendo compatibilidade total e performance otimizada.

**âŒ ARQUIVOS PROIBIDOS (NUNCA CRIAR):**
- `src/synapse/core/database.py` - PROIBIDO (duplicaÃ§Ã£o)
- `src/synapse/core/database/` - PROIBIDO (diretÃ³rio desnecessÃ¡rio)
- `src/synapse/db.py` - PROIBIDO (nome nÃ£o padronizado)
- `src/synapse/core/db/` - PROIBIDO (estrutura antiga)
- `src/synapse/database/database.py` - PROIBIDO (duplicaÃ§Ã£o aninhada)

**âœ… ÃšNICO ARQUIVO VÃLIDO:**
- `src/synapse/database.py` - Sistema unificado de database

### **ImportaÃ§Ã£o ObrigatÃ³ria**
```python
# âœ… SEMPRE use estas importaÃ§Ãµes Ãºnicas
from synapse.database import (
    get_db,           # Dependency para FastAPI endpoints
    get_async_db,     # SessÃ£o async para operaÃ§Ãµes complexas
    engine,           # Engine SQLAlchemy para operaÃ§Ãµes diretas
    Base,             # Base class para todos os models
    SessionLocal      # Classe de sessÃ£o local
)

# âŒ NUNCA use estas importaÃ§Ãµes (localizaÃ§Ãµes proibidas)
from synapse.core.database import get_db  # âŒ PROIBIDO
from synapse.core.database.connection import engine  # âŒ PROIBIDO
from synapse.db import Base  # âŒ PROIBIDO
from synapse.core.db.session import SessionLocal  # âŒ PROIBIDO
```

### **Sistema Unificado - Funcionalidades Integradas**

**ðŸ”§ CARACTERÃSTICAS TÃ‰CNICAS:**
- **Engine Unificado**: PostgreSQL com asyncpg + psycopg2 para compatibilidade
- **Connection Pooling**: Pool otimizado com configuraÃ§Ãµes de produÃ§Ã£o
- **Session Management**: Async/sync sessions com context managers
- **Base Declarativa**: Base Ãºnica para todos os models
- **Transaction Support**: TransaÃ§Ãµes automÃ¡ticas e manuais
- **Health Checks**: VerificaÃ§Ã£o de conectividade integrada

### **PadrÃµes de Uso Corretos**

```python
# âœ… Dependency para endpoints FastAPI
from synapse.database import get_db
from sqlalchemy.orm import Session

@router.get("/users/{user_id}")
async def get_user(
    user_id: int,
    db: Session = Depends(get_db)  # âœ… PadrÃ£o correto
):
    user = db.query(User).filter(User.id == user_id).first()
    return user

# âœ… SessÃ£o async para operaÃ§Ãµes complexas
from synapse.database import get_async_db
from sqlalchemy.ext.asyncio import AsyncSession

async def complex_operation():
    async with get_async_db() as db:
        # OperaÃ§Ãµes async complexas
        result = await db.execute(select(User).where(User.is_active == True))
        users = result.scalars().all()
        return users

# âœ… TransaÃ§Ãµes manuais
from synapse.database import SessionLocal
from sqlalchemy.exc import SQLAlchemyError

async def transaction_example():
    db = SessionLocal()
    try:
        async with db.begin():
            # OperaÃ§Ãµes transacionais
            user = User(email="test@example.com")
            db.add(user)
            await db.flush()  # Para obter ID
            
            profile = UserProfile(user_id=user.id, name="Test")
            db.add(profile)
            # Commit automÃ¡tico se nÃ£o houver exceÃ§Ã£o
    except SQLAlchemyError as e:
        logger.error(f"Transaction failed: {e}")
        # Rollback automÃ¡tico
        raise
    finally:
        await db.close()

# âœ… Models com Base unificada
from synapse.database import Base
from sqlalchemy import Column, Integer, String, DateTime
from sqlalchemy.sql import func

class MyModel(Base):
    __tablename__ = "my_models"
    __table_args__ = {"schema": "synapscale_db"}
    
    id = Column(Integer, primary_key=True, index=True)
    name = Column(String(255), nullable=False)
    created_at = Column(DateTime(timezone=True), server_default=func.now())
```

### **ConfiguraÃ§Ãµes de ConexÃ£o**

```python
# âœ… Pool de conexÃµes otimizado (configurado automaticamente)
# - pool_size: 20 conexÃµes base
# - max_overflow: 30 conexÃµes extras
# - pool_timeout: 30 segundos
# - pool_recycle: 3600 segundos (1 hora)
# - pool_pre_ping: True (verificaÃ§Ã£o de conexÃ£o)

# âœ… Engines configurados
# - engine: Sync engine para FastAPI dependencies
# - async_engine: Async engine para operaÃ§Ãµes complexas
# - Ambos conectam na mesma DATABASE_URL

# âœ… Health check automÃ¡tico
from synapse.database import check_db_health

async def verify_database():
    is_healthy = await check_db_health()
    if not is_healthy:
        raise HTTPException(status_code=503, detail="Database unavailable")
```

### **PadrÃµes de Query Otimizados**

```python
# âœ… Queries eficientes com relacionamentos
from sqlalchemy.orm import selectinload, joinedload
from sqlalchemy import select, and_, or_

# Evitar N+1 queries
async def get_users_with_workspaces():
    async with get_async_db() as db:
        stmt = select(User).options(
            joinedload(User.workspaces),      # 1:N relationship
            selectinload(User.files)          # 1:N relationship com muitos itens
        ).where(User.is_active == True)
        
        result = await db.execute(stmt)
        return result.unique().scalars().all()

# âœ… Queries complexas com filtros
async def search_workflows(user_id: int, status: List[str], date_from: datetime):
    async with get_async_db() as db:
        stmt = select(Workflow).where(
            and_(
                Workflow.user_id == user_id,
                Workflow.status.in_(status),
                Workflow.created_at >= date_from
            )
        ).order_by(Workflow.created_at.desc()).limit(50)
        
        result = await db.execute(stmt)
        return result.scalars().all()

# âœ… AgregaÃ§Ãµes eficientes
async def get_user_stats(user_id: int):
    async with get_async_db() as db:
        stmt = select(
            func.count(Workflow.id).label('total_workflows'),
            func.avg(Workflow.execution_time).label('avg_time'),
            func.sum(
                case((Workflow.status == 'completed', 1), else_=0)
            ).label('completed_workflows')
        ).where(Workflow.user_id == user_id)
        
        result = await db.execute(stmt)
        return result.first()._asdict()
```

### **Migration & Schema Management**

```python
# âœ… Todas as migraÃ§Ãµes devem usar a Base unificada
from synapse.database import Base

# âœ… Comando para gerar migraÃ§Ã£o
# python -m alembic revision --autogenerate -m "add_new_table"

# âœ… Schema obrigatÃ³rio para todas as tabelas
__table_args__ = {"schema": "synapscale_db"}

# âœ… Imports corretos em migraÃ§Ãµes
# from synapse.database import Base
# from synapse.models.user import User  # Para relacionamentos
```

### **Error Handling & Recovery**

```python
# âœ… Tratamento robusto de erros de database
from sqlalchemy.exc import (
    SQLAlchemyError, 
    IntegrityError, 
    OperationalError,
    DisconnectionError
)

async def robust_database_operation():
    max_retries = 3
    for attempt in range(max_retries):
        try:
            async with get_async_db() as db:
                # OperaÃ§Ã£o de database
                result = await db.execute(stmt)
                await db.commit()
                return result
                
        except DisconnectionError as e:
            logger.warning(f"Database disconnection, retry {attempt + 1}/{max_retries}")
            if attempt == max_retries - 1:
                raise HTTPException(status_code=503, detail="Database unavailable")
            await asyncio.sleep(2 ** attempt)  # Backoff exponencial
            
        except IntegrityError as e:
            logger.error(f"Data integrity error: {e}")
            raise HTTPException(status_code=400, detail="Data conflict")
            
        except OperationalError as e:
            logger.error(f"Database operational error: {e}")
            raise HTTPException(status_code=503, detail="Database error")
            
        except SQLAlchemyError as e:
            logger.error(f"Unexpected database error: {e}")
            raise HTTPException(status_code=500, detail="Internal database error")
```

### **Performance & Monitoring**

```python
# âœ… Monitoring de conexÃµes
from synapse.database import engine

async def get_connection_stats():
    pool = engine.pool
    return {
        "pool_size": pool.size(),
        "checked_in": pool.checkedin(),
        "checked_out": pool.checkedout(),
        "overflow": pool.overflow(),
        "invalid": pool.invalid()
    }

# âœ… Query performance logging
import time
from contextlib import asynccontextmanager

@asynccontextmanager
async def log_query_performance(operation: str):
    start_time = time.time()
    try:
        yield
    finally:
        duration = time.time() - start_time
        if duration > 1.0:  # Log queries lentas
            logger.warning(f"Slow query detected", extra={
                "operation": operation,
                "duration": duration,
                "slow_query": True
            })
```

### **Regras de ImportaÃ§Ã£o Database**

```python
# âœ… SEMPRE usar importaÃ§Ãµes do arquivo unificado
from synapse.database import get_db, get_async_db, Base, engine, SessionLocal

# âœ… Para models
from synapse.database import Base

class MyModel(Base):
    # definiÃ§Ã£o do model

# âœ… Para endpoints
from synapse.database import get_db

@router.get("/endpoint")
async def my_endpoint(db: Session = Depends(get_db)):
    # lÃ³gica do endpoint

# âŒ NUNCA usar estas importaÃ§Ãµes
from synapse.core.database import get_db  # âŒ LocalizaÃ§Ã£o proibida
from synapse.db import Base  # âŒ Arquivo nÃ£o padrÃ£o
from synapse.core.db.session import SessionLocal  # âŒ Estrutura antiga
```

---

## âš™ï¸ SISTEMA DE CONFIGURAÃ‡ÃƒO UNIFICADO - REGRAS CRÃTICAS

### **ðŸš¨ ÃšNICA FONTE DE VERDADE: `src/synapse/core/config.py`**

**CONTEXTO**: O SynapScale possui um sistema de configuraÃ§Ã£o centralizado usando Pydantic v2 que gerencia todas as configuraÃ§Ãµes do sistema, incluindo database, LLM providers, storage, security, e muito mais.

**âŒ ARQUIVOS PROIBIDOS (NUNCA CRIAR):**
- `src/synapse/config.py` - PROIBIDO (Ã³rfÃ£o deletado)
- `src/synapse/core/config_new.py` - PROIBIDO (arquivo temporÃ¡rio deletado)
- `src/synapse/core/config/config.py` - PROIBIDO (diretÃ³rio desnecessÃ¡rio)
- `src/synapse/core/config/constants.py` - PROIBIDO (constantes estÃ£o no config principal)
- `src/synapse/core/unified_config.py` - PROIBIDO (erro temporÃ¡rio deletado)
- `src/synapse/settings.py` - PROIBIDO (nome nÃ£o padronizado)

**âœ… ÃšNICO ARQUIVO VÃLIDO:**
- `src/synapse/core/config.py` - Sistema unificado de configuraÃ§Ã£o

### **ImportaÃ§Ã£o ObrigatÃ³ria**
```python
# âœ… SEMPRE use estas importaÃ§Ãµes Ãºnicas
from synapse.core.config import (
    settings,           # InstÃ¢ncia principal de configuraÃ§Ã£o
    FILE_CATEGORIES,    # Constantes de categorias de arquivo
    setup_logging,      # ConfiguraÃ§Ã£o de logging
    validate_settings,  # ValidaÃ§Ã£o de configuraÃ§Ãµes
    get_settings        # Factory function para settings
)

# âŒ NUNCA use estas importaÃ§Ãµes (arquivos deletados/proibidos)
from synapse.config import settings  # âŒ PROIBIDO - arquivo Ã³rfÃ£o deletado
from synapse.core.config_new import settings  # âŒ PROIBIDO - arquivo removido
from synapse.core.config.constants import FILE_CATEGORIES  # âŒ PROIBIDO - diretÃ³rio deletado
from synapse.core.unified_config import settings  # âŒ PROIBIDO - arquivo temporÃ¡rio deletado
from synapse.settings import settings  # âŒ PROIBIDO - nome nÃ£o padronizado
```

### **Sistema Unificado - Funcionalidades Integradas**

**ðŸ”§ CARACTERÃSTICAS TÃ‰CNICAS:**
- **Pydantic v2**: ValidaÃ§Ã£o robusta e type safety
- **Environment Variables**: Carregamento automÃ¡tico de .env
- **Multiple Providers**: LLM, Storage, Database, Email, etc.
- **Validation**: ValidaÃ§Ã£o automÃ¡tica de URLs, emails, tokens
- **Security**: ConfiguraÃ§Ãµes de JWT, CORS, encryption
- **Performance**: Cache de configuraÃ§Ãµes, lazy loading
- **Development/Production**: ConfiguraÃ§Ãµes especÃ­ficas por ambiente

### **ConfiguraÃ§Ãµes DisponÃ­veis**

```python
# âœ… ConfiguraÃ§Ãµes de Database
settings.DATABASE_URL          # URL completa do PostgreSQL
settings.DB_POOL_SIZE          # Tamanho do pool de conexÃµes
settings.DB_MAX_OVERFLOW       # ConexÃµes extras permitidas
settings.DB_POOL_TIMEOUT       # Timeout para obter conexÃ£o

# âœ… ConfiguraÃ§Ãµes de Redis
settings.REDIS_URL             # URL do Redis para cache/sessions
settings.CACHE_DEFAULT_TTL     # TTL padrÃ£o para cache (300s)
settings.REDIS_MAX_CONNECTIONS # MÃ¡ximo de conexÃµes Redis

# âœ… ConfiguraÃ§Ãµes de LLM Providers
settings.OPENAI_API_KEY        # Chave API OpenAI
settings.ANTHROPIC_API_KEY     # Chave API Anthropic/Claude
settings.GOOGLE_API_KEY        # Chave API Google/Gemini
settings.GROK_API_KEY          # Chave API Grok/XAI
settings.DEEPSEEK_API_KEY      # Chave API DeepSeek
settings.TESS_API_KEY          # Chave API Tess (nacional)

# âœ… ConfiguraÃ§Ãµes de Storage
settings.STORAGE_TYPE          # "local", "s3", "gcs"
settings.STORAGE_BASE_PATH     # Caminho base para arquivos
settings.AWS_ACCESS_KEY_ID     # Credenciais AWS S3
settings.AWS_SECRET_ACCESS_KEY
settings.AWS_S3_BUCKET
settings.GCS_BUCKET_NAME       # Google Cloud Storage

# âœ… ConfiguraÃ§Ãµes de Security
settings.SECRET_KEY            # Chave secreta JWT
settings.JWT_ALGORITHM         # Algoritmo JWT (HS256)
settings.ACCESS_TOKEN_EXPIRE   # ExpiraÃ§Ã£o token (30 min)
settings.REFRESH_TOKEN_EXPIRE  # ExpiraÃ§Ã£o refresh (7 dias)
settings.ENCRYPTION_KEY        # Chave para criptografia simÃ©trica

# âœ… ConfiguraÃ§Ãµes de Email
settings.SMTP_HOST             # Servidor SMTP
settings.SMTP_PORT             # Porta SMTP
settings.SMTP_USERNAME         # UsuÃ¡rio SMTP
settings.SMTP_PASSWORD         # Senha SMTP
settings.EMAIL_FROM            # Email remetente padrÃ£o

# âœ… ConfiguraÃ§Ãµes de Rate Limiting
settings.RATE_LIMIT_ENABLED    # Ativar rate limiting
settings.DEFAULT_RATE_LIMIT    # Limite padrÃ£o (100/minute)
settings.LLM_RATE_LIMIT        # Limite para LLM (20/hour)
settings.UPLOAD_RATE_LIMIT     # Limite para uploads (10/minute)

# âœ… ConfiguraÃ§Ãµes de Files
settings.MAX_FILE_SIZE         # Tamanho mÃ¡ximo arquivo (50MB)
settings.ALLOWED_EXTENSIONS    # ExtensÃµes permitidas
settings.ALLOWED_MIME_TYPES    # MIME types permitidos
settings.ENABLE_MALWARE_SCAN   # Ativar scan de malware

# âœ… ConfiguraÃ§Ãµes de Development
settings.DEBUG                 # Modo debug
settings.ENVIRONMENT           # "development", "staging", "production"
settings.LOG_LEVEL             # NÃ­vel de log
settings.ENABLE_CORS           # Ativar CORS
settings.CORS_ORIGINS          # Origens permitidas CORS
```

### **PadrÃµes de Uso Corretos**

```python
# âœ… Acessar configuraÃ§Ãµes
from synapse.core.config import settings

# Usar configuraÃ§Ãµes em services
class LLMService:
    def __init__(self):
        self.openai_key = settings.OPENAI_API_KEY
        self.anthropic_key = settings.ANTHROPIC_API_KEY
        self.max_tokens = settings.LLM_MAX_TOKENS
    
    async def generate_response(self, provider: str):
        if provider == "openai" and not self.openai_key:
            raise ValueError("OpenAI API key not configured")
        # ... lÃ³gica

# âœ… ValidaÃ§Ã£o de configuraÃ§Ãµes obrigatÃ³rias
from synapse.core.config import validate_settings

async def startup_validation():
    """Validar configuraÃ§Ãµes na inicializaÃ§Ã£o."""
    try:
        validate_settings()
        logger.info("All settings validated successfully")
    except ValueError as e:
        logger.error(f"Configuration error: {e}")
        raise

# âœ… ConfiguraÃ§Ãµes por ambiente
if settings.ENVIRONMENT == "development":
    # ConfiguraÃ§Ãµes especÃ­ficas de desenvolvimento
    enable_debug_mode()
elif settings.ENVIRONMENT == "production":
    # ConfiguraÃ§Ãµes especÃ­ficas de produÃ§Ã£o
    setup_monitoring()

# âœ… Usar constantes definidas
from synapse.core.config import FILE_CATEGORIES

def validate_file_category(category: str):
    if category not in FILE_CATEGORIES:
        raise ValueError(f"Invalid category. Must be one of: {FILE_CATEGORIES}")
    return category

# âœ… Setup de logging usando configuraÃ§Ãµes
from synapse.core.config import setup_logging

def configure_application():
    setup_logging()  # Usa settings.LOG_LEVEL automaticamente
```

### **ConfiguraÃ§Ãµes de Providers LLM**

```python
# âœ… ConfiguraÃ§Ã£o completa de providers
from synapse.core.config import settings

class UnifiedLLMConfig:
    """ConfiguraÃ§Ã£o centralizada para todos os providers LLM."""
    
    @property
    def openai_config(self) -> dict:
        return {
            "api_key": settings.OPENAI_API_KEY,
            "base_url": settings.OPENAI_BASE_URL,
            "max_tokens": settings.OPENAI_MAX_TOKENS,
            "timeout": settings.LLM_TIMEOUT
        }
    
    @property
    def anthropic_config(self) -> dict:
        return {
            "api_key": settings.ANTHROPIC_API_KEY,
            "base_url": settings.ANTHROPIC_BASE_URL,
            "max_tokens": settings.ANTHROPIC_MAX_TOKENS,
            "timeout": settings.LLM_TIMEOUT
        }
    
    @property
    def google_config(self) -> dict:
        return {
            "api_key": settings.GOOGLE_API_KEY,
            "project_id": settings.GOOGLE_PROJECT_ID,
            "location": settings.GOOGLE_LOCATION,
            "max_tokens": settings.GOOGLE_MAX_TOKENS
        }
    
    def get_provider_config(self, provider: str) -> dict:
        """Obter configuraÃ§Ã£o especÃ­fica do provider."""
        configs = {
            "openai": self.openai_config,
            "anthropic": self.anthropic_config,
            "google": self.google_config,
            "grok": self.grok_config,
            "deepseek": self.deepseek_config,
            "tess": self.tess_config
        }
        
        if provider not in configs:
            raise ValueError(f"Unsupported provider: {provider}")
        
        config = configs[provider]
        if not config.get("api_key"):
            raise ValueError(f"API key not configured for provider: {provider}")
        
        return config
```

### **ValidaÃ§Ã£o AvanÃ§ada de ConfiguraÃ§Ãµes**

```python
# âœ… ValidaÃ§Ãµes customizadas
from pydantic import field_validator, model_validator
from typing import List, Optional

class Settings(BaseSettings):
    # ... outros campos ...
    
    @field_validator('DATABASE_URL')
    @classmethod
    def validate_database_url(cls, v: str) -> str:
        if not v.startswith(('postgresql://', 'postgresql+asyncpg://', 'postgresql+psycopg2://')):
            raise ValueError('DATABASE_URL must be a PostgreSQL URL')
        return v
    
    @field_validator('REDIS_URL')
    @classmethod
    def validate_redis_url(cls, v: str) -> str:
        if not v.startswith('redis://'):
            raise ValueError('REDIS_URL must be a Redis URL')
        return v
    
    @field_validator('CORS_ORIGINS')
    @classmethod
    def validate_cors_origins(cls, v: List[str]) -> List[str]:
        for origin in v:
            if not origin.startswith(('http://', 'https://')):
                raise ValueError(f'Invalid CORS origin: {origin}')
        return v
    
    @model_validator(mode='after')
    def validate_llm_providers(self):
        """Validar que pelo menos um provider LLM estÃ¡ configurado."""
        llm_keys = [
            self.OPENAI_API_KEY,
            self.ANTHROPIC_API_KEY,
            self.GOOGLE_API_KEY,
            self.GROK_API_KEY
        ]
        
        if not any(llm_keys):
            raise ValueError('At least one LLM provider API key must be configured')
        
        return self
    
    @model_validator(mode='after')
    def validate_storage_config(self):
        """Validar configuraÃ§Ã£o de storage."""
        if self.STORAGE_TYPE == "s3":
            if not all([self.AWS_ACCESS_KEY_ID, self.AWS_SECRET_ACCESS_KEY, self.AWS_S3_BUCKET]):
                raise ValueError('S3 storage requires AWS credentials and bucket name')
        elif self.STORAGE_TYPE == "gcs":
            if not self.GCS_BUCKET_NAME:
                raise ValueError('GCS storage requires bucket name')
        
        return self
```

### **Environment Variables & .env**

```python
# âœ… ConfiguraÃ§Ã£o de .env
from pydantic_settings import BaseSettings, SettingsConfigDict

class Settings(BaseSettings):
    model_config = SettingsConfigDict(
        env_file=".env",
        env_file_encoding="utf-8",
        case_sensitive=True,
        extra="ignore"  # Ignorar variÃ¡veis extras
    )
    
    # âœ… Valores padrÃ£o para desenvolvimento
    ENVIRONMENT: str = "development"
    DEBUG: bool = False
    LOG_LEVEL: str = "INFO"
    
    # âœ… ConfiguraÃ§Ãµes obrigatÃ³rias
    DATABASE_URL: str
    REDIS_URL: str
    SECRET_KEY: str
    
    # âœ… ConfiguraÃ§Ãµes opcionais com padrÃµes
    MAX_FILE_SIZE: int = 50 * 1024 * 1024  # 50MB
    CACHE_DEFAULT_TTL: int = 300  # 5 minutos
    ACCESS_TOKEN_EXPIRE: int = 30  # 30 minutos

# âœ… InstÃ¢ncia singleton
settings = Settings()
```

### **Helpers e Utilities**

```python
# âœ… FunÃ§Ãµes helper para configuraÃ§Ãµes
from synapse.core.config import settings

def get_database_config() -> dict:
    """ConfiguraÃ§Ã£o completa do database."""
    return {
        "url": settings.DATABASE_URL,
        "pool_size": settings.DB_POOL_SIZE,
        "max_overflow": settings.DB_MAX_OVERFLOW,
        "pool_timeout": settings.DB_POOL_TIMEOUT,
        "pool_recycle": settings.DB_POOL_RECYCLE,
        "echo": settings.DEBUG  # Log SQL queries em debug
    }

def get_cors_config() -> dict:
    """ConfiguraÃ§Ã£o do CORS."""
    return {
        "allow_origins": settings.CORS_ORIGINS,
        "allow_credentials": True,
        "allow_methods": ["GET", "POST", "PUT", "DELETE", "PATCH"],
        "allow_headers": ["*"]
    }

def get_security_config() -> dict:
    """ConfiguraÃ§Ã£o de seguranÃ§a."""
    return {
        "secret_key": settings.SECRET_KEY,
        "algorithm": settings.JWT_ALGORITHM,
        "access_token_expire": settings.ACCESS_TOKEN_EXPIRE,
        "refresh_token_expire": settings.REFRESH_TOKEN_EXPIRE
    }

def is_production() -> bool:
    """Verificar se estÃ¡ em produÃ§Ã£o."""
    return settings.ENVIRONMENT == "production"

def is_development() -> bool:
    """Verificar se estÃ¡ em desenvolvimento."""
    return settings.ENVIRONMENT == "development"
```

### **Regras de ImportaÃ§Ã£o Config**

```python
# âœ… SEMPRE usar importaÃ§Ãµes do arquivo unificado
from synapse.core.config import settings, FILE_CATEGORIES, setup_logging

# âœ… Para validaÃ§Ã£o
from synapse.core.config import validate_settings, get_settings

# âœ… Para helpers
from synapse.core.config import (
    get_database_config,
    get_cors_config,
    is_production
)

# âŒ NUNCA usar estas importaÃ§Ãµes (arquivos deletados/proibidos)
from synapse.config import settings  # âŒ Arquivo Ã³rfÃ£o deletado
from synapse.core.config_new import settings  # âŒ Arquivo temporÃ¡rio deletado
from synapse.core.config.constants import FILE_CATEGORIES  # âŒ DiretÃ³rio deletado
from synapse.core.unified_config import settings  # âŒ Erro temporÃ¡rio deletado
from synapse.settings import settings  # âŒ Nome nÃ£o padronizado
```

### **Regra de Ouro: UMA CONFIGURAÃ‡ÃƒO, UMA FONTE**

**ðŸŽ¯ PRINCÃPIO FUNDAMENTAL:**
> "Um repositÃ³rio, uma configuraÃ§Ã£o, uma fonte de verdade."
> 
> Se vocÃª estÃ¡ pensando em criar outro arquivo de configuraÃ§Ã£o,
> **PARE** e adicione ao `src/synapse/core/config.py` existente.

**âœ… SEMPRE:**
- Adicionar novas configuraÃ§Ãµes no arquivo principal
- Usar validaÃ§Ã£o Pydantic para novos campos
- Documentar configuraÃ§Ãµes com comments
- Testar configuraÃ§Ãµes em desenvolvimento

**âŒ NUNCA:**
- Criar arquivos de configuraÃ§Ã£o duplicados
- Hardcoding de valores que deveriam ser configurÃ¡veis
- Ignorar validaÃ§Ã£o de configuraÃ§Ãµes crÃ­ticas
- Deixar configuraÃ§Ãµes sensÃ­veis sem variÃ¡veis de ambiente

## âš¡ CACHE REDIS & BACKGROUND TASKS

### **Cache Redis**
```python
# âœ… Cache service com TTL configurÃ¡vel
from synapse.core.cache import cache_service
import json
from typing import Any, Optional

class CacheService:
    def __init__(self):
        self.redis = redis.Redis.from_url(settings.REDIS_URL)
        self.default_ttl = settings.CACHE_DEFAULT_TTL
    
    async def get_or_set(
        self,
        key: str,
        func: callable,
        ttl: int = None,
        *args,
        **kwargs
    ) -> Any:
        """Cache pattern: busca ou calcula e armazena."""
        cached = await self.get(key)
        if cached is not None:
            return cached
        
        value = await func(*args, **kwargs) if asyncio.iscoroutinefunction(func) else func(*args, **kwargs)
        await self.set(key, value, ttl)
        return value

# âœ… Decorator para cache automÃ¡tico
def cached(ttl: int = 300, key_prefix: str = ""):
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            cache_key = f"{key_prefix}:{func.__name__}:{hash(str(args) + str(kwargs))}"
            return await cache_service.get_or_set(cache_key, func, ttl, *args, **kwargs)
        return wrapper
    return decorator
```

### **Background Tasks com Celery**
```python
# âœ… Tasks em background
from celery import Celery
from synapse.core.celery_app import celery_app

@celery_app.task(bind=True, max_retries=3)
def process_file_upload(self, file_id: str, user_id: int):
    """Processa upload de arquivo em background."""
    try:
        update_file_status(file_id, "processing")
        file_info = process_file_content(file_id)
        notify_user_file_ready(user_id, file_id, file_info)
        update_file_status(file_id, "ready")
    except Exception as e:
        logger.error(f"File processing failed: {file_id}, error: {e}")
        self.retry(countdown=2 ** self.request.retries)

# âœ… Trigger em endpoints
@router.post("/files/upload")
async def upload_file_endpoint():
    # ... upload logic ...
    process_file_upload.delay(file.id, current_user.id)
    return {"message": "File uploaded, processing in background"}
```

## ðŸ§ª TESTES AVANÃ‡ADOS

### **Estrutura Completa de Testes**
```python
# âœ… ConfiguraÃ§Ã£o com fixtures
import pytest
from httpx import AsyncClient
from sqlalchemy.ext.asyncio import AsyncSession

@pytest.fixture
async def client():
    """Cliente HTTP para testes."""
    async with AsyncClient(app=app, base_url="http://test") as ac:
        yield ac

@pytest.fixture
async def test_user(db_session):
    """UsuÃ¡rio de teste."""
    user = User(email="test@example.com", full_name="Test User", is_active=True)
    db_session.add(user)
    await db_session.commit()
    return user

@pytest.fixture
async def auth_headers(test_user):
    """Headers de autenticaÃ§Ã£o."""
    token = create_access_token(data={"sub": str(test_user.id)})
    return {"Authorization": f"Bearer {token}"}

# âœ… Testes de integraÃ§Ã£o completos
@pytest.mark.asyncio
async def test_create_workflow_complete_flow(
    client: AsyncClient,
    auth_headers: dict,
    test_user: User
):
    """Testa fluxo completo de criaÃ§Ã£o de workflow."""
    
    # Criar workspace
    workspace_response = await client.post(
        "/api/v1/workspaces/",
        json={"name": "Test Workspace"},
        headers=auth_headers
    )
    assert workspace_response.status_code == 201
    workspace = workspace_response.json()
    
    # Criar workflow
    workflow_response = await client.post(
        "/api/v1/workflows/",
        json={
            "name": "Test Workflow",
            "workspace_id": workspace["id"],
            "steps": [{"name": "Step 1", "type": "llm_generation"}]
        },
        headers=auth_headers
    )
    
    assert workflow_response.status_code == 201
    assert workflow_response.json()["name"] == "Test Workflow"

# âœ… Testes de performance
@pytest.mark.performance
async def test_endpoint_performance(client: AsyncClient, auth_headers: dict):
    """Testa performance de endpoints crÃ­ticos."""
    start_time = time.time()
    response = await client.get("/api/v1/analytics/dashboard", headers=auth_headers)
    duration = time.time() - start_time
    
    assert response.status_code == 200
    assert duration < 2.0  # Deve responder em menos de 2 segundos

# âœ… Factories para dados de teste
class UserFactory(SQLAlchemyModelFactory):
    class Meta:
        model = User
        sqlalchemy_session_persistence = "commit"
    
    email = factory.Sequence(lambda n: f"user{n}@example.com")
    full_name = factory.Faker("name")
    is_active = True
```

### **Cobertura & Qualidade**
- Manter cobertura > 85%
- Testes unitÃ¡rios, integraÃ§Ã£o e e2e
- Mock serviÃ§os externos (LLM, email, etc.)
- Testes de casos extremos e erros

## ðŸš€ PERFORMANCE

### **OtimizaÃ§Ãµes**
- Use conexÃµes assÃ­ncronas (asyncpg para PostgreSQL)
- Implemente cache Redis para dados frequentemente acessados
- Pagine resultados grandes (limit/offset)
- Use background tasks para operaÃ§Ãµes demoradas

### **WebSockets**
- Use ConnectionManager para gerenciar conexÃµes
- Implemente heartbeat para detectar conexÃµes mortas
- Limite nÃºmero de conexÃµes por usuÃ¡rio

## ðŸ“± VERSIONAMENTO & DEPLOY

### **API Versioning**
- Mantenha compatibilidade com versÃµes anteriores
- Use deprecation warnings antes de remover features
- Documente breaking changes no CHANGELOG.md

### **Deploy**
- SEMPRE testar em ambiente de staging
- Usar variÃ¡veis de ambiente para configuraÃ§Ã£o
- Configurar health checks apropriados

## ðŸ”„ WORKFLOWS & EXECUÃ‡ÃƒO

### **Engine de ExecuÃ§Ã£o**
- Use sistema de filas para execuÃ§Ãµes longas
- Implemente retry com backoff exponencial
- Monitor timeouts e recursos utilizados

### **Nodes**
- Valide entradas antes da execuÃ§Ã£o
- Mantenha estado consistente
- Log detalhado para debugging

## ðŸ“ ARQUIVOS & STORAGE

### **Upload**
- Valide tipo e tamanho de arquivo
- Use storage configurÃ¡vel (local/S3/GCS)
- Implementar scan de malware em produÃ§Ã£o

### **Processamento**
- Use workers assÃ­ncronos para processamento
- Mantenha referÃªncias consistentes no banco
- Cleanup de arquivos temporÃ¡rios

## ðŸ› ï¸ DEBUGGING & TROUBLESHOOTING

### **Logs Estruturados**
- Include user_id, request_id nos logs
- Use structured logging (JSON) em produÃ§Ã£o
- Correlacione logs com mÃ©tricas

### **Health Checks**
- `/health` para status bÃ¡sico
- `/health/detailed` para diagnÃ³stico completo
- Monitor dependÃªncias externas

## ðŸŽ¨ CÃ“DIGO LIMPO

### **Naming Conventions**
- FunÃ§Ãµes: `verbo_objeto()` (ex: `create_user()`)
- Classes: `PascalCase` (ex: `UserService`)
- Constantes: `UPPER_CASE` (ex: `MAX_FILE_SIZE`)
- VariÃ¡veis: `snake_case` (ex: `user_data`)

### **DocumentaÃ§Ã£o**
- README.md atualizado
- Docstrings em inglÃªs
- ComentÃ¡rios explicativos para lÃ³gica complexa
- OpenAPI documentation completa

## âš¡ COMANDOS ÃšTEIS & PRODUÃ‡ÃƒO

### **Desenvolvimento Local**
```bash
# Servidor de desenvolvimento
./dev.sh                            # Iniciar com hot-reload
./prod.sh                           # Simular ambiente de produÃ§Ã£o

# Banco de dados
python -m alembic upgrade head      # Aplicar migraÃ§Ãµes
python -m alembic revision --autogenerate -m "descriÃ§Ã£o"  # Nova migraÃ§Ã£o
python -m alembic downgrade -1      # Rollback Ãºltima migraÃ§Ã£o

# Testes & Qualidade
python -m pytest                   # Todos os testes
python -m pytest --cov=src --cov-report=html  # Com cobertura HTML
python scripts/run_tests.py        # Teste completo de endpoints
black src/ && isort src/           # FormataÃ§Ã£o + imports
mypy src/                          # VerificaÃ§Ã£o de tipos
```

### **ProduÃ§Ã£o & Deploy**
```bash
# Health checks
curl https://api.synapscale.com/health
curl https://api.synapscale.com/health/detailed

# Monitoring
docker logs synapscale-backend --tail=100 -f
docker stats synapscale-backend

# Database backup (Render PostgreSQL)
pg_dump $DATABASE_URL > backup_$(date +%Y%m%d_%H%M%S).sql

# Deploy checklist
git tag v1.0.1                     # Tag da versÃ£o
git push origin v1.0.1            # Push da tag
# Verificar CI/CD pipeline no Render
# Confirmar deploy atravÃ©s dos logs
# Executar smoke tests bÃ¡sicos
```

### **Debug & Troubleshooting**
```bash
# Logs estruturados
tail -f logs/app.log | grep ERROR
tail -f logs/app.log | grep "user_id=123"

# Database queries em desenvolvimento  
export SQLALCHEMY_ECHO=True        # Mostrar queries SQL

# Performance profiling
python -m cProfile -o profile.stats main.py
python -c "import pstats; pstats.Stats('profile.stats').sort_stats('cumulative').print_stats(20)"

# Memory debugging
python -m memory_profiler src/synapse/main.py

# Redis debugging
redis-cli ping                     # Verificar conexÃ£o
redis-cli info memory            # Status de memÃ³ria
redis-cli --scan --pattern "synapscale:*" | head -10  # Listar chaves
```

## ðŸš¨ NUNCA FAÃ‡A

**âŒ CRÃTICO - ARQUIVO PRINCIPAL:**
âŒ Criar `src/synapse/app.py` (arquivo removido, usar apenas main.py)
âŒ Duplicar instÃ¢ncia FastAPI fora de `src/synapse/main.py`
âŒ Referenciar qualquer arquivo que nÃ£o seja `src.synapse.main:app` em deploy/scripts

**âŒ OUTRAS REGRAS CRÃTICAS:**
âŒ Hardcoding de valores sensÃ­veis
âŒ Commit de credenciais ou .env
âŒ Queries SQL diretas sem validaÃ§Ã£o
âŒ OperaÃ§Ãµes sÃ­ncronas em endpoints assÃ­ncronos
âŒ Ignorar tratamento de erros
âŒ Deixar TODO comments no cÃ³digo de produÃ§Ã£o
âŒ Usar `import *`
âŒ Modificar banco diretamente sem migraÃ§Ã£o

## âœ… SEMPRE FAÃ‡A

**âœ… CRÃTICO - ARQUIVO PRINCIPAL:**
âœ… Usar APENAS `src/synapse/main.py` para inicializaÃ§Ã£o FastAPI
âœ… Referenciar `src.synapse.main:app` em scripts, deploy e documentaÃ§Ã£o
âœ… Modificar aplicaÃ§Ã£o principal SOMENTE em `main.py`

**âœ… OUTRAS REGRAS ESSENCIAIS:**
âœ… Validar inputs do usuÃ¡rio
âœ… Usar transaÃ§Ãµes para operaÃ§Ãµes crÃ­ticas
âœ… Logs informativos mas nÃ£o verbosos demais
âœ… Testes para novas funcionalidades
âœ… Documentar APIs com exemplos
âœ… Revisar cÃ³digo antes do commit
âœ… Considerar impacto de performance
âœ… Pensar em seguranÃ§a primeiro

## ðŸŽ¯ RESUMO DAS MELHORES PRÃTICAS

### **ðŸš€ Prioridades de Desenvolvimento**
1. **SeguranÃ§a Primeiro**: ValidaÃ§Ã£o, autenticaÃ§Ã£o, rate limiting
2. **Performance**: Cache Redis, queries otimizadas, background tasks
3. **Monitoramento**: Logs estruturados, mÃ©tricas, health checks
4. **Qualidade**: Testes > 85%, code review, type hints
5. **DocumentaÃ§Ã£o**: Taskmaster logs, API docs, commits claros

### **âš¡ Quick Wins para Performance**
- Use `@cached()` decorator para dados repetitivos
- Implemente `joinedload`/`selectinload` para relacionamentos
- Background tasks para operaÃ§Ãµes > 2 segundos
- Rate limiting em endpoints sensÃ­veis
- Pagination para listas grandes

### **ðŸ”’ Checklist de SeguranÃ§a**
- [ ] Input validation com Pydantic
- [ ] SanitizaÃ§Ã£o de HTML com bleach
- [ ] Rate limiting configurado
- [ ] Logs de auditoria para aÃ§Ãµes crÃ­ticas
- [ ] API keys criptografadas no banco
- [ ] HTTPS em produÃ§Ã£o
- [ ] Backup automÃ¡tico do banco

### **ðŸ“Š Monitoring Essencial**
- Logs estruturados com user_id/request_id
- MÃ©tricas de performance por endpoint
- Usage tracking para LLM (tokens/cost)
- Health checks para dependÃªncias
- Alertas para erros crÃ­ticos

### **ðŸ”„ Workflow DiÃ¡rio Recomendado**
```bash
# 1. ComeÃ§ar sessÃ£o
task-master list                    # Ver tasks pendentes
task-master next                    # PrÃ³xima task a trabalhar

# 2. Durante desenvolvimento
task-master research "pergunta tÃ©cnica" --files=src/relevante/
# Implementar feature
task-master update-subtask --id=X.Y --prompt="Progresso detalhado..."

# 3. Finalizar
task-master set-status --id=X.Y --status=done
git commit -m "feat: descriÃ§Ã£o\n\nTask: X.Y"
```

### **ðŸ§  Mindset SynapScale**
- **Think Production First**: Todo cÃ³digo deve estar pronto para produÃ§Ã£o
- **Document Everything**: Use Taskmaster para manter histÃ³rico de decisÃµes
- **Performance Matters**: Sistema serve milhares de usuÃ¡rios
- **Security is Critical**: Dados sensÃ­veis de empresas
- **Quality Over Speed**: Melhor fazer certo na primeira vez

---

**ðŸŽ¯ OBJETIVO: Construir a melhor plataforma de automaÃ§Ã£o com IA do mercado - cada linha de cÃ³digo conta!**

*Ãšltima atualizaÃ§Ã£o: Janeiro 2024 | SynapScale Backend v2.0* 