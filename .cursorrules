# üöÄ SynapScale Backend - Cursor Rules v2.0
# Plataforma de Automa√ß√£o com IA - Regras completas e otimizadas para desenvolvimento

## üìã CONTEXTO DO PROJETO

Voc√™ est√° trabalhando no **SynapScale Backend**, uma plataforma empresarial de automa√ß√£o com IA de n√≠vel production. Sistema robusto e escal√°vel com:

- **Backend**: FastAPI 0.110+ + SQLAlchemy 2.0+ + PostgreSQL + Redis + Alembic
- **IA**: Integra√ß√£o multi-provedor (OpenAI, Anthropic, Google, Grok, DeepSeek, Llama, Tess)
- **Arquitetura**: API REST + WebSockets + Workflows + Marketplace + Analytics + Billing
- **Produ√ß√£o**: Sistema est√°vel com 242 endpoints, 70.7% de taxa de sucesso, cobertura de testes
- **Ferramentas**: Taskmaster (task management), Cursor MCP, sistema de tags, rate limiting
- **Infraestrutura**: Docker, Render, S3/GCS storage, Sentry monitoring

## ‚ö†Ô∏è REGRA CR√çTICA - ARQUIVO PRINCIPAL DA APLICA√á√ÉO

### **üö® √öNICA FONTE DE VERDADE: `src/synapse/main.py`**

**CONTEXTO**: Este projeto teve conflitos entre `app.py` e `main.py` causando duplica√ß√£o de c√≥digo e confus√£o. Foi estabelecido `main.py` como arquivo principal √∫nico.

**‚ùå PROIBIDO:**
- Criar `src/synapse/app.py` ou recriar esse arquivo
- Criar outros arquivos que instanciem o FastAPI app (ex: `server.py`, `application.py`)
- Duplicar as fun√ß√µes de inicializa√ß√£o do FastAPI que est√£o em `main.py`
- Modificar refer√™ncias de deploy para apontar para qualquer arquivo que n√£o seja `main.py`

**‚úÖ OBRIGAT√ìRIO:**
- TODA inicializa√ß√£o do FastAPI deve estar APENAS em `src/synapse/main.py`
- Deploy, scripts e documenta√ß√£o devem referenciar APENAS `src.synapse.main:app`
- Qualquer altera√ß√£o na aplica√ß√£o principal deve ser feita SOMENTE em `main.py`

**üéØ VERIFICA√á√ÉO PR√â-COMMIT:**
- [ ] `src/synapse/app.py` n√£o existe
- [ ] Refer√™ncias de deploy apontam para `src.synapse.main:app`
- [ ] N√£o h√° duplica√ß√£o de inst√¢ncia FastAPI em outros arquivos

---

## üéØ PRINC√çPIOS FUNDAMENTAIS

### 1. **Configura√ß√£o Centralizada**
- SEMPRE use `settings` de `synapse.core.config` para configura√ß√µes
- NUNCA hardcode valores - use vari√°veis de ambiente
- Mantenha compatibilidade com o sistema de configura√ß√£o existente

### 2. **Estrutura do Projeto**
- Siga a estrutura modular: `src/synapse/api/v1/endpoints/`
- Models em `src/synapse/models/`
- Schemas em `src/synapse/schemas/`
- Services em `src/synapse/services/`

### 3. **Padr√µes de C√≥digo**
- Use type hints em TUDO (Python 3.11+)
- Siga conven√ß√µes PEP 8 com Black (line-length=88)
- Docstrings obrigat√≥rias para classes e fun√ß√µes p√∫blicas
- Use async/await para opera√ß√µes I/O
- SEMPRE use f-strings para formata√ß√£o de strings
- Prefira comprehensions sobre loops quando poss√≠vel
- Use context managers para recursos (conex√µes, arquivos)

### 4. **Taskmaster Integration**
- Use Taskmaster MCP tools para gerenciamento de tarefas
- SEMPRE documente progresso com `update_subtask`
- Use tags para organizar contextos de desenvolvimento
- Mantenha tasks atualizadas durante implementa√ß√£o

### 5. **Workflow de Desenvolvimento com Taskmaster**
```python
# ‚úÖ Padr√£o de desenvolvimento com documenta√ß√£o autom√°tica

# 1. Inicializar projeto (uma vez)
# task-master init --name="SynapScale Backend" --description="Plataforma de automa√ß√£o com IA"

# 2. Durante implementa√ß√£o, SEMPRE documentar progresso
async def implement_feature_with_logging():
    """Implementa feature documentando cada passo."""
    
    # ‚úÖ Atualizar subtask com descobertas t√©cnicas
    await update_subtask(
        task_id="2.3",
        notes="""
        ‚ö° Implementa√ß√£o do sistema de cache Redis:
        
        DESCOBERTAS:
        - Redis configurado na URL: {settings.REDIS_URL}
        - TTL padr√£o: 300s (configur√°vel via CACHE_DEFAULT_TTL)
        - Decorator @cached() funcionando perfeitamente
        
        IMPLEMENTADO:
        ‚úÖ CacheService com get/set/delete
        ‚úÖ Decorator para cache autom√°tico
        ‚úÖ Integra√ß√£o com analytics (cache de 10min)
        ‚úÖ Testes unit√°rios passando
        
        PR√ìXIMOS PASSOS:
        - Implementar cache para consultas de LLM
        - Adicionar m√©tricas de hit/miss ratio
        - Configurar eviction policies
        
        PERFORMANCE:
        - Consultas de analytics: 2.3s ‚Üí 0.05s üöÄ
        - Redu√ß√£o de 95% no tempo de resposta
        """
    )
    
    # ‚úÖ Marcar como completa quando finalizada
    await set_task_status(task_id="2.3", status="done")

# 3. Usar research para decis√µes t√©cnicas
# task-master research "Como otimizar queries SQLAlchemy com relacionamentos?" 
#   --files=src/synapse/models/,src/synapse/services/ 
#   --save-to=3.1

# 4. Organizar com tags para diferentes contextos
# task-master use-tag feature-cache-redis    # Para nova feature
# task-master use-tag hotfix-performance     # Para otimiza√ß√µes
# task-master use-tag refactor-llm-service   # Para refatora√ß√µes
# task-master use-tag master                 # Para desenvolvimento principal

# 5. Expandir tasks complexas com research
# task-master expand --id=4 --research --force
```

### 6. **Padr√µes de Commits com Taskmaster**
```bash
# ‚úÖ Commits estruturados com refer√™ncia √†s tasks

# Feature completa
git commit -m "feat(cache): Implement Redis caching system

- Add CacheService with TTL support
- Create @cached decorator for automatic caching
- Integrate with analytics endpoints (95% perf improvement)
- Add comprehensive unit tests

Closes task: 2.3
Related: 2.1, 2.2"

# Hotfix
git commit -m "fix(auth): Fix JWT token expiration handling

- Update token refresh logic
- Add proper error handling for expired tokens  
- Improve user experience on token expiry

Fixes task: 5.2 (hotfix-security tag)"

# Refactor
git commit -m "refactor(llm): Unify LLM service providers

- Consolidate OpenAI, Claude, Gemini into UnifiedLLMService
- Add automatic fallback mechanism
- Improve error handling and logging
- Reduce code duplication by 60%

Task: 1.4 (refactor-llm-service tag)"
```

## üîß REGRAS T√âCNICAS ESPEC√çFICAS

### **FastAPI & Endpoints**
```python
# ‚úÖ Estrutura padr√£o de endpoint
@router.post("/endpoint", response_model=ResponseSchema, tags=["category"])
async def create_something(
    request: CreateSchema,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
) -> ResponseSchema:
    """
    Descri√ß√£o clara do endpoint.
    
    Args:
        request: Dados da requisi√ß√£o
        db: Sess√£o do banco de dados
        current_user: Usu√°rio autenticado
        
    Returns:
        ResponseSchema: Dados da resposta
        
    Raises:
        HTTPException: Quando ocorre erro espec√≠fico
    """
```

### **üö® REGRA CR√çTICA - AUTENTICA√á√ÉO OBRIGAT√ìRIA**

**CONTEXTO**: Como todo SaaS profissional, pouqu√≠ssimos endpoints funcionam sem autentica√ß√£o. A grande maioria requer usu√°rio logado.

**‚úÖ ENDPOINTS P√öBLICOS PERMITIDOS (Exce√ß√µes muito limitadas):**
```python
# ‚úÖ Autentica√ß√£o & Configura√ß√£o do SDK
POST /auth/login          # Inicia sess√£o com credenciais
POST /auth/signup         # Cria nova conta de usu√°rio  
POST /auth/refresh        # Gera novo token a partir do refresh
POST /auth/forgot-password # Recupera√ß√£o de senha
POST /auth/reset-password  # Reset de senha com token

# ‚úÖ Status & Configura√ß√£o P√∫blica
GET  /                    # Root endpoint
GET  /health              # Health check b√°sico
GET  /api/v1/health       # Health check da API
GET  /info                # Informa√ß√µes da API
GET  /public/config       # Configs m√≠nimas da API (chaves p√∫blicas, limites, branding)
```

**‚ùå TODOS OS OUTROS ENDPOINTS REQUEREM AUTENTICA√á√ÉO:**
```python
# ‚ùå PROIBIDO: Endpoint sem autentica√ß√£o (exceto os listados acima)
@router.get("/some-endpoint")
async def endpoint_without_auth(db: Session = Depends(get_db)):
    pass

# ‚úÖ OBRIGAT√ìRIO: Sempre incluir current_user dependency
@router.get("/some-endpoint")
async def endpoint_with_auth(
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)  # ‚úÖ OBRIGAT√ìRIO
):
    pass
```

**üîí PROTE√á√ïES ADICIONAIS:**
- Mesmo endpoints "p√∫blicos" t√™m rate limiting agressivo
- Endpoints de auth podem ter reCAPTCHA ou signatures tempor√°rias
- Logs de auditoria para todos os acessos
- Rate limiting espec√≠fico por IP para endpoints p√∫blicos

**‚ö†Ô∏è VERIFICA√á√ÉO PR√â-COMMIT:**
- [ ] Novo endpoint tem `current_user: User = Depends(get_current_user)`?
- [ ] Endpoint est√° na lista de exce√ß√µes p√∫blicas aprovadas?
- [ ] Rate limiting configurado adequadamente?
- [ ] Logs de auditoria implementados?

**üéØ REGRA DE OURO:**
> "Se n√£o est√° na lista de exce√ß√µes p√∫blicas, DEVE ter autentica√ß√£o obrigat√≥ria."

### **Models (SQLAlchemy 2.0+)**
```python
# ‚úÖ Padr√£o de model completo
from sqlalchemy import String, Integer, DateTime, Boolean, Text, ForeignKey
from sqlalchemy.orm import Mapped, mapped_column, relationship
from sqlalchemy.sql import func
from synapse.database import Base

class MyModel(Base):
    __tablename__ = "my_models"
    __table_args__ = {"schema": "synapscale_db"}
    
    # ‚úÖ Campos obrigat√≥rios
    id: Mapped[int] = mapped_column(Integer, primary_key=True, index=True)
    name: Mapped[str] = mapped_column(String(255), nullable=False, index=True)
    
    # ‚úÖ Timestamps autom√°ticos
    created_at: Mapped[datetime] = mapped_column(
        DateTime(timezone=True), 
        server_default=func.now(),
        nullable=False
    )
    updated_at: Mapped[datetime] = mapped_column(
        DateTime(timezone=True), 
        server_default=func.now(), 
        onupdate=func.now(),
        nullable=False
    )
    
    # ‚úÖ Campos opcionais
    description: Mapped[str | None] = mapped_column(Text, nullable=True)
    is_active: Mapped[bool] = mapped_column(Boolean, default=True, nullable=False)
    
    # ‚úÖ Foreign Keys com relacionamentos
    user_id: Mapped[int] = mapped_column(ForeignKey("synapscale_db.users.id"), nullable=False)
    user: Mapped["User"] = relationship("User", back_populates="my_models")
    
    # ‚úÖ M√©todos √∫teis
    def to_dict(self) -> dict:
        """Converte model para dicion√°rio."""
        return {
            "id": self.id,
            "name": self.name,
            "created_at": self.created_at.isoformat(),
            "updated_at": self.updated_at.isoformat()
        }
    
    def __repr__(self) -> str:
        return f"<MyModel(id={self.id}, name='{self.name}')>"
```

### **Schemas (Pydantic v2)**
```python
# ‚úÖ Padr√£o de schema completo
from pydantic import BaseModel, Field, ConfigDict, field_validator
from typing import Optional, List
from datetime import datetime
from enum import Enum

class StatusEnum(str, Enum):
    ACTIVE = "active"
    INACTIVE = "inactive"
    PENDING = "pending"

class BaseSchema(BaseModel):
    model_config = ConfigDict(
        from_attributes=True,
        use_enum_values=True,
        validate_assignment=True,
        arbitrary_types_allowed=False
    )

class CreateSchema(BaseSchema):
    name: str = Field(..., min_length=1, max_length=255, description="Nome do item")
    description: Optional[str] = Field(None, max_length=1000, description="Descri√ß√£o opcional")
    is_active: bool = Field(True, description="Status ativo/inativo")
    
    @field_validator('name')
    @classmethod
    def validate_name(cls, v: str) -> str:
        if not v.strip():
            raise ValueError("Nome n√£o pode estar vazio")
        return v.strip()

class UpdateSchema(BaseSchema):
    name: Optional[str] = Field(None, min_length=1, max_length=255)
    description: Optional[str] = Field(None, max_length=1000)
    is_active: Optional[bool] = None

class ResponseSchema(BaseSchema):
    id: int
    name: str
    description: Optional[str] = None
    is_active: bool
    status: StatusEnum
    created_at: datetime
    updated_at: datetime
    
    # ‚úÖ Campos computados
    display_name: str = Field(..., description="Nome formatado para exibi√ß√£o")
    
    @field_validator('display_name', mode='before')
    @classmethod
    def set_display_name(cls, v, info):
        if info.data.get('name'):
            return info.data['name'].title()
        return v

class ListResponseSchema(BaseSchema):
    items: List[ResponseSchema]
    total: int
    page: int = Field(ge=1)
    per_page: int = Field(ge=1, le=100)
    pages: int
```

### **Services**
```python
# ‚úÖ Padr√£o de service
class MyService:
    @staticmethod
    async def create_something(
        db: Session,
        data: CreateSchema,
        user_id: int
    ) -> MyModel:
        """
        Cria um novo item.
        """
        try:
            # L√≥gica do servi√ßo
            return new_item
        except Exception as e:
            logger.error(f"Erro ao criar item: {e}")
            raise HTTPException(status_code=500, detail="Erro interno")
```

## üîí SEGURAN√áA & VALIDA√á√ÉO

### **Autentica√ß√£o & Autoriza√ß√£o**
```python
# ‚úÖ Estrutura completa de autentica√ß√£o
from synapse.api.deps import get_current_user, check_workspace_permission, rate_limit
from synapse.core.auth.jwt import verify_token
from synapse.middlewares.rate_limiting import RateLimiter

@router.post("/secure-endpoint")
@rate_limit("10/minute")  # Rate limiting espec√≠fico
async def secure_endpoint(
    request: CreateSchema,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user),
    workspace_id: int = Path(..., description="ID do workspace")
):
    # ‚úÖ Verificar permiss√£o no workspace
    await check_workspace_permission(db, current_user.id, workspace_id, "read")
    
    # ‚úÖ Log de auditoria
    logger.info(f"User {current_user.id} accessed workspace {workspace_id}")
```

### **Valida√ß√£o de Dados & Sanitiza√ß√£o**
```python
# ‚úÖ Valida√ß√£o robusta
import bleach
from pydantic import field_validator
from typing import Any

class SecureSchema(BaseModel):
    content: str = Field(..., max_length=10000)
    email: str = Field(..., regex=r'^[^@]+@[^@]+\.[^@]+$')
    
    @field_validator('content')
    @classmethod
    def sanitize_content(cls, v: str) -> str:
        """Remove HTML tags maliciosos."""
        return bleach.clean(v, tags=['p', 'br', 'strong', 'em'])
    
    @field_validator('email')
    @classmethod
    def validate_email_domain(cls, v: str) -> str:
        """Validar dom√≠nios permitidos."""
        allowed_domains = ['company.com', 'trusted.org']
        domain = v.split('@')[1]
        if domain not in allowed_domains:
            raise ValueError(f"Dom√≠nio {domain} n√£o permitido")
        return v.lower()
```

### **Rate Limiting Avan√ßado**
```python
# ‚úÖ Rate limiting por tipo de opera√ß√£o
from synapse.middlewares.rate_limiting import RateLimiter

@router.post("/upload")
@rate_limit("5/minute", key_func=lambda request: f"upload:{request.client.host}")
async def upload_file(file: UploadFile):
    pass

@router.post("/llm/generate")
@rate_limit("20/hour", key_func=lambda request: f"llm:{request.user.id}")
async def generate_response():
    pass
```

### **Tratamento de Erros**
```python
# ‚úÖ Padr√£o de tratamento de erro
try:
    result = await some_operation()
except SpecificException as e:
    logger.error(f"Erro espec√≠fico: {e}")
    raise HTTPException(status_code=400, detail="Mensagem clara para o usu√°rio")
except Exception as e:
    logger.error(f"Erro inesperado: {e}")
    raise HTTPException(status_code=500, detail="Erro interno do servidor")
```

## üóÑÔ∏è BANCO DE DADOS & MIGRA√á√ïES

### **Consultas Otimizadas**
```python
# ‚úÖ Padr√µes de consulta eficientes
from sqlalchemy.orm import selectinload, joinedload
from sqlalchemy import select, and_, or_, func

# Busca simples por ID
user = await session.get(User, user_id)

# Consulta com relacionamentos (N+1 problem solved)
stmt = select(User).options(
    joinedload(User.workspaces),
    selectinload(User.files)
).where(User.is_active == True)
users = await session.execute(stmt)

# Consulta com filtros complexos
stmt = select(Workflow).where(
    and_(
        Workflow.user_id == user_id,
        or_(
            Workflow.status == "active",
            Workflow.status == "pending"
        ),
        Workflow.created_at >= datetime.now() - timedelta(days=30)
    )
).order_by(Workflow.created_at.desc()).limit(10)

# Agrega√ß√µes
stmt = select(
    func.count(Workflow.id).label('total'),
    func.avg(Workflow.execution_time).label('avg_time')
).where(Workflow.user_id == user_id)
```

### **Transa√ß√µes & Context Managers**
```python
# ‚úÖ Transa√ß√µes seguras
from sqlalchemy.exc import SQLAlchemyError

async def complex_operation(db: Session, data: dict):
    try:
        async with db.begin():
            # Opera√ß√£o 1
            user = User(**data['user'])
            db.add(user)
            await db.flush()  # Para obter o ID
            
            # Opera√ß√£o 2 dependente
            workspace = Workspace(user_id=user.id, **data['workspace'])
            db.add(workspace)
            
            # Opera√ß√£o 3
            await create_default_settings(db, user.id)
            
            # Commit autom√°tico se tudo der certo
            return user
    except SQLAlchemyError as e:
        logger.error(f"Database error: {e}")
        # Rollback autom√°tico
        raise HTTPException(status_code=500, detail="Database operation failed")
```

### **Migra√ß√µes Alembic**
```python
# ‚úÖ Estrutura de migra√ß√£o padr√£o
"""add_user_preferences_table

Revision ID: 2024_01_15_user_preferences
Revises: 2024_01_10_workspace_settings
Create Date: 2024-01-15 10:30:00.000000

"""
from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers
revision = '2024_01_15_user_preferences'
down_revision = '2024_01_10_workspace_settings'
branch_labels = None
depends_on = None

def upgrade():
    # ‚úÖ Criar tabela com todos os indexes necess√°rios
    op.create_table(
        'user_preferences',
        sa.Column('id', sa.Integer(), nullable=False),
        sa.Column('user_id', sa.Integer(), nullable=False),
        sa.Column('theme', sa.String(20), nullable=False, default='light'),
        sa.Column('language', sa.String(10), nullable=False, default='pt-BR'),
        sa.Column('notifications', postgresql.JSONB(), nullable=True),
        sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.func.now()),
        sa.Column('updated_at', sa.DateTime(timezone=True), server_default=sa.func.now()),
        sa.PrimaryKeyConstraint('id'),
        sa.ForeignKeyConstraint(['user_id'], ['synapscale_db.users.id'], ondelete='CASCADE'),
        schema='synapscale_db'
    )
    
    # ‚úÖ Indexes para performance
    op.create_index('ix_user_preferences_user_id', 'user_preferences', ['user_id'], schema='synapscale_db')
    op.create_index('ix_user_preferences_theme', 'user_preferences', ['theme'], schema='synapscale_db')

def downgrade():
    op.drop_table('user_preferences', schema='synapscale_db')
```

### **Schema Management**
- SEMPRE usar schema `synapscale_db` em production
- Testar migra√ß√µes em development primeiro
- Fazer backup antes de migrations em production
- Usar `--dry-run` para validar migra√ß√µes complexas
- Documentar breaking changes no commit message

## ü§ñ INTEGRA√á√ÉO COM LLM UNIFICADA

### **Provedores Suportados**
- **OpenAI**: GPT-3.5, GPT-4, GPT-4-turbo, GPT-4o
- **Anthropic**: Claude 3 (Haiku, Sonnet, Opus), Claude 3.5
- **Google**: Gemini Pro, Gemini Pro Vision
- **Grok**: XAI Grok models
- **DeepSeek**: DeepSeek Coder, DeepSeek Chat
- **Llama**: Meta Llama 2, Llama 3
- **Tess**: Pareto Tess (modelo nacional)

### **Unified LLM Service**
```python
# ‚úÖ Servi√ßo unificado com tratamento de erros
from synapse.core.llm.unified_service import UnifiedLLMService
from synapse.models.conversation_llm import ConversationLLM
from synapse.models.usage_log import UsageLog

async def generate_ai_response(
    user_id: int,
    messages: List[dict],
    provider: str = "claude",
    model: str = "claude-3-sonnet",
    temperature: float = 0.7,
    max_tokens: int = 2000
) -> dict:
    """
    Gera resposta usando o servi√ßo unificado de LLM.
    """
    llm_service = UnifiedLLMService()
    
    try:
        # ‚úÖ Log de in√≠cio da opera√ß√£o
        logger.info(f"Generating LLM response for user {user_id} with {provider}/{model}")
        
        # ‚úÖ Verificar cota do usu√°rio
        usage_today = await check_daily_usage(user_id)
        if usage_today > settings.MAX_DAILY_LLM_REQUESTS:
            raise HTTPException(status_code=429, detail="Daily LLM quota exceeded")
        
        # ‚úÖ Gerar resposta
        response = await llm_service.generate_response(
            provider=provider,
            model=model,
            messages=messages,
            user_id=user_id,
            temperature=temperature,
            max_tokens=max_tokens,
            stream=False
        )
        
        # ‚úÖ Salvar conversa no banco
        conversation = ConversationLLM(
            user_id=user_id,
            provider=provider,
            model=model,
            input_tokens=response.get("usage", {}).get("prompt_tokens", 0),
            output_tokens=response.get("usage", {}).get("completion_tokens", 0),
            total_tokens=response.get("usage", {}).get("total_tokens", 0),
            cost=calculate_cost(provider, model, response.get("usage", {})),
            response_time=response.get("response_time", 0)
        )
        await db.add(conversation)
        
        # ‚úÖ Log de uso para analytics
        await log_usage(
            user_id=user_id,
            action="llm_generation",
            provider=provider,
            model=model,
            tokens_used=response.get("usage", {}).get("total_tokens", 0)
        )
        
        return {
            "content": response["content"],
            "model": model,
            "provider": provider,
            "usage": response.get("usage"),
            "conversation_id": conversation.id
        }
        
    except Exception as e:
        logger.error(f"LLM generation failed for user {user_id}: {e}")
        
        # ‚úÖ Tentar fallback provider
        if provider != "claude":  # Fallback padr√£o
            logger.info(f"Attempting fallback to Claude for user {user_id}")
            return await generate_ai_response(
                user_id=user_id,
                messages=messages,
                provider="claude",
                model="claude-3-sonnet",
                temperature=temperature,
                max_tokens=max_tokens
            )
        
        raise HTTPException(
            status_code=500, 
            detail=f"LLM service unavailable: {str(e)}"
        )

# ‚úÖ Streaming responses
async def stream_ai_response(
    user_id: int,
    messages: List[dict],
    provider: str = "openai",
    model: str = "gpt-4"
):
    """Stream de resposta LLM em tempo real."""
    llm_service = UnifiedLLMService()
    
    async for chunk in llm_service.generate_response_stream(
        provider=provider,
        model=model,
        messages=messages,
        user_id=user_id
    ):
        yield f"data: {json.dumps(chunk)}\n\n"
```

### **Gerenciamento de API Keys por Usu√°rio**
```python
# ‚úÖ Sistema de chaves personalizadas
from synapse.models.user_variable import UserVariable
from synapse.core.security import encrypt_value, decrypt_value

async def get_user_api_key(
    db: Session, 
    user_id: int, 
    provider: str
) -> Optional[str]:
    """Busca chave API espec√≠fica do usu√°rio ou usa a padr√£o."""
    
    # Tentar chave do usu√°rio primeiro
    user_key = await db.execute(
        select(UserVariable).where(
            and_(
                UserVariable.user_id == user_id,
                UserVariable.key == f"{provider.upper()}_API_KEY",
                UserVariable.is_active == True
            )
        )
    )
    
    if user_key.scalar_one_or_none():
        return decrypt_value(user_key.value)
    
    # Fallback para chave padr√£o do sistema
    return settings.get(f"{provider.upper()}_API_KEY")

async def save_user_api_key(
    db: Session,
    user_id: int,
    provider: str,
    api_key: str
) -> UserVariable:
    """Salva chave API criptografada do usu√°rio."""
    
    encrypted_key = encrypt_value(api_key)
    
    user_var = UserVariable(
        user_id=user_id,
        key=f"{provider.upper()}_API_KEY",
        value=encrypted_key,
        category="llm_config",
        is_encrypted=True,
        description=f"Chave API pessoal para {provider.title()}"
    )
    
    db.add(user_var)
    await db.commit()
    
    logger.info(f"API key saved for user {user_id}, provider {provider}")
    return user_var
```

## üîå WEBSOCKETS & TEMPO REAL

### **Connection Manager**
```python
# ‚úÖ Gerenciador de conex√µes WebSocket
from synapse.core.websockets.manager import ConnectionManager
from synapse.core.websockets.execution_manager import ExecutionManager

class WorkspaceConnectionManager:
    def __init__(self):
        self.active_connections: Dict[int, List[WebSocket]] = {}
        self.user_connections: Dict[int, List[WebSocket]] = {}
    
    async def connect(self, websocket: WebSocket, user_id: int, workspace_id: int):
        await websocket.accept()
        
        # ‚úÖ Limitar conex√µes por usu√°rio
        if user_id not in self.user_connections:
            self.user_connections[user_id] = []
        
        if len(self.user_connections[user_id]) >= settings.WS_MAX_CONNECTIONS_PER_USER:
            await websocket.close(code=1013, reason="Too many connections")
            return
        
        # ‚úÖ Adicionar √† lista de conex√µes
        if workspace_id not in self.active_connections:
            self.active_connections[workspace_id] = []
        
        self.active_connections[workspace_id].append(websocket)
        self.user_connections[user_id].append(websocket)
        
        logger.info(f"WebSocket connected: user={user_id}, workspace={workspace_id}")
    
    async def disconnect(self, websocket: WebSocket, user_id: int, workspace_id: int):
        # ‚úÖ Remover das listas
        if workspace_id in self.active_connections:
            self.active_connections[workspace_id].remove(websocket)
        
        if user_id in self.user_connections:
            self.user_connections[user_id].remove(websocket)
        
        logger.info(f"WebSocket disconnected: user={user_id}, workspace={workspace_id}")
    
    async def broadcast_to_workspace(self, workspace_id: int, message: dict):
        """Envia mensagem para todos os conectados no workspace."""
        if workspace_id not in self.active_connections:
            return
        
        dead_connections = []
        for connection in self.active_connections[workspace_id]:
            try:
                await connection.send_json(message)
            except ConnectionClosedError:
                dead_connections.append(connection)
        
        # ‚úÖ Limpar conex√µes mortas
        for dead_conn in dead_connections:
            self.active_connections[workspace_id].remove(dead_conn)

# ‚úÖ Inst√¢ncia global
connection_manager = WorkspaceConnectionManager()
```

### **WebSocket Endpoints**
```python
# ‚úÖ Endpoint WebSocket para execu√ß√£o de workflows
@router.websocket("/ws/execution/{execution_id}")
async def websocket_execution(
    websocket: WebSocket,
    execution_id: str,
    token: str = Query(...),
    db: Session = Depends(get_db)
):
    # ‚úÖ Validar token
    try:
        payload = verify_token(token)
        user_id = payload.get("sub")
        current_user = await get_user_by_id(db, user_id)
    except JWTError:
        await websocket.close(code=1008, reason="Invalid token")
        return
    
    # ‚úÖ Verificar permiss√£o na execu√ß√£o
    execution = await get_execution_by_id(db, execution_id)
    if not execution or execution.user_id != current_user.id:
        await websocket.close(code=1008, reason="Unauthorized")
        return
    
    await connection_manager.connect(websocket, current_user.id, execution.workspace_id)
    
    try:
        while True:
            # ‚úÖ Heartbeat para manter conex√£o viva
            await websocket.send_json({
                "type": "heartbeat",
                "timestamp": datetime.now().isoformat()
            })
            await asyncio.sleep(30)
            
    except WebSocketDisconnect:
        await connection_manager.disconnect(websocket, current_user.id, execution.workspace_id)
```

## üìä ANALYTICS & M√âTRICAS

### **Sistema de Analytics**
```python
# ‚úÖ Service de analytics completo
from synapse.services.analytics_service import AnalyticsService
from synapse.models.usage_log import UsageLog

class AnalyticsService:
    @staticmethod
    async def track_event(
        db: Session,
        user_id: int,
        event_type: str,
        metadata: dict = None,
        workspace_id: int = None
    ):
        """Registra evento para analytics."""
        usage_log = UsageLog(
            user_id=user_id,
            action=event_type,
            resource_type="analytics",
            metadata=metadata or {},
            workspace_id=workspace_id
        )
        
        db.add(usage_log)
        await db.commit()
        
        # ‚úÖ Cache para analytics em tempo real
        await cache_analytics_event(event_type, metadata)
    
    @staticmethod
    async def get_dashboard_metrics(
        db: Session,
        user_id: int,
        workspace_id: int,
        period: str = "30d"
    ) -> dict:
        """M√©tricas para dashboard."""
        
        # ‚úÖ Calcular per√≠odo
        end_date = datetime.now()
        if period == "7d":
            start_date = end_date - timedelta(days=7)
        elif period == "30d":
            start_date = end_date - timedelta(days=30)
        else:
            start_date = end_date - timedelta(days=90)
        
        # ‚úÖ M√©tricas de workflows
        workflow_stats = await db.execute(
            select(
                func.count(Workflow.id).label('total_workflows'),
                func.count(case((Workflow.status == 'completed', 1))).label('completed'),
                func.count(case((Workflow.status == 'failed', 1))).label('failed'),
                func.avg(Workflow.execution_time).label('avg_execution_time')
            ).where(
                and_(
                    Workflow.workspace_id == workspace_id,
                    Workflow.created_at >= start_date
                )
            )
        )
        
        # ‚úÖ M√©tricas de LLM
        llm_stats = await db.execute(
            select(
                func.count(ConversationLLM.id).label('total_llm_calls'),
                func.sum(ConversationLLM.total_tokens).label('total_tokens'),
                func.sum(ConversationLLM.cost).label('total_cost')
            ).where(
                and_(
                    ConversationLLM.user_id == user_id,
                    ConversationLLM.created_at >= start_date
                )
            )
        )
        
        return {
            "period": period,
            "workflows": workflow_stats.first()._asdict(),
            "llm_usage": llm_stats.first()._asdict(),
            "generated_at": datetime.now().isoformat()
        }

# ‚úÖ Middleware de m√©tricas
from synapse.middlewares.metrics import MetricsMiddleware

@router.middleware("http")
async def metrics_middleware(request: Request, call_next):
    start_time = time.time()
    
    response = await call_next(request)
    
    duration = time.time() - start_time
    
    # ‚úÖ Log m√©tricas de performance
    await log_request_metrics(
        method=request.method,
        url=str(request.url),
        status_code=response.status_code,
        duration=duration,
        user_id=getattr(request.state, 'user_id', None)
    )
    
    return response
```

## üè∑Ô∏è SISTEMA DE TAGS

### **Tag Management**
```python
# ‚úÖ Sistema completo de tags
from synapse.models.tag import Tag
from synapse.services.tag_service import TagService

class TagService:
    @staticmethod
    async def create_tag(
        db: Session,
        user_id: int,
        workspace_id: int,
        name: str,
        color: str = "#3B82F6",
        description: str = None
    ) -> Tag:
        """Cria nova tag com valida√ß√µes."""
        
        # ‚úÖ Validar se tag j√° existe
        existing = await db.execute(
            select(Tag).where(
                and_(
                    Tag.workspace_id == workspace_id,
                    func.lower(Tag.name) == name.lower()
                )
            )
        )
        
        if existing.scalar_one_or_none():
            raise HTTPException(
                status_code=400, 
                detail=f"Tag '{name}' j√° existe neste workspace"
            )
        
        # ‚úÖ Validar limite de tags por workspace
        tag_count = await db.execute(
            select(func.count(Tag.id)).where(Tag.workspace_id == workspace_id)
        )
        
        if tag_count.scalar() >= settings.MAX_TAGS_PER_WORKSPACE:
            raise HTTPException(
                status_code=400,
                detail="Limite de tags por workspace atingido"
            )
        
        tag = Tag(
            name=name.strip(),
            color=color,
            description=description,
            workspace_id=workspace_id,
            created_by=user_id
        )
        
        db.add(tag)
        await db.commit()
        
        logger.info(f"Tag created: {name} in workspace {workspace_id}")
        return tag
    
    @staticmethod
    async def apply_tags(
        db: Session,
        resource_type: str,  # 'workflow', 'file', 'conversation'
        resource_id: int,
        tag_ids: List[int],
        workspace_id: int
    ):
        """Aplica tags a um recurso."""
        
        # ‚úÖ Validar que todas as tags pertencem ao workspace
        tags = await db.execute(
            select(Tag).where(
                and_(
                    Tag.id.in_(tag_ids),
                    Tag.workspace_id == workspace_id
                )
            )
        )
        
        valid_tags = tags.scalars().all()
        if len(valid_tags) != len(tag_ids):
            raise HTTPException(
                status_code=400,
                detail="Algumas tags n√£o pertencem ao workspace"
            )
        
        # ‚úÖ Aplicar tags (usando tabela de associa√ß√£o)
        for tag in valid_tags:
            association = ResourceTag(
                resource_type=resource_type,
                resource_id=resource_id,
                tag_id=tag.id
            )
            db.add(association)
        
        await db.commit()

# ‚úÖ Endpoint para busca por tags
@router.get("/search/by-tags")
async def search_by_tags(
    tag_ids: List[int] = Query(...),
    resource_type: str = Query(...),
    workspace_id: int = Query(...),
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """Busca recursos por tags."""
    
    # ‚úÖ Query complexa com tags
    resources = await db.execute(
        select(ResourceTag.resource_id)
        .where(
            and_(
                ResourceTag.tag_id.in_(tag_ids),
                ResourceTag.resource_type == resource_type
            )
        )
        .group_by(ResourceTag.resource_id)
        .having(func.count(ResourceTag.tag_id) == len(tag_ids))  # AND logic
    )
    
    return {"resource_ids": [r[0] for r in resources.fetchall()]}
```

## üìÅ FILE UPLOAD & STORAGE

### **Storage Manager**
```python
# ‚úÖ Sistema unificado de storage
from synapse.core.storage.storage_manager import StorageManager
from synapse.models.file import File

class StorageManager:
    def __init__(self):
        self.storage_type = settings.STORAGE_TYPE
        self.base_path = settings.STORAGE_BASE_PATH
    
    async def upload_file(
        self,
        file: UploadFile,
        user_id: int,
        workspace_id: int,
        category: str = "general"
    ) -> File:
        """Upload com valida√ß√µes completas."""
        
        # ‚úÖ Valida√ß√µes de seguran√ßa
        await self._validate_file(file)
        
        # ‚úÖ Gerar caminho √∫nico
        file_id = str(uuid.uuid4())
        file_extension = Path(file.filename).suffix
        filename = f"{file_id}{file_extension}"
        
        # ‚úÖ Organizar por categoria e data
        today = datetime.now()
        file_path = f"{category}/{today.year}/{today.month:02d}/{filename}"
        
        try:
            # ‚úÖ Salvar arquivo
            if self.storage_type == "local":
                await self._save_local(file, file_path)
            elif self.storage_type == "s3":
                await self._save_s3(file, file_path)
            elif self.storage_type == "gcs":
                await self._save_gcs(file, file_path)
            
            # ‚úÖ Criar registro no banco
            file_record = File(
                id=file_id,
                filename=file.filename,
                original_filename=file.filename,
                file_path=file_path,
                file_size=file.size,
                mime_type=file.content_type,
                category=category,
                user_id=user_id,
                workspace_id=workspace_id,
                storage_type=self.storage_type
            )
            
            return file_record
            
        except Exception as e:
            logger.error(f"File upload failed: {e}")
            raise HTTPException(status_code=500, detail="Upload failed")
    
    async def _validate_file(self, file: UploadFile):
        """Valida√ß√µes de seguran√ßa do arquivo."""
        
        # ‚úÖ Validar tamanho
        if file.size > settings.MAX_FILE_SIZE:
            raise HTTPException(
                status_code=413,
                detail=f"File too large. Max size: {settings.MAX_FILE_SIZE} bytes"
            )
        
        # ‚úÖ Validar extens√£o
        file_extension = Path(file.filename).suffix.lower()
        if file_extension not in settings.ALLOWED_EXTENSIONS:
            raise HTTPException(
                status_code=400,
                detail=f"File type not allowed: {file_extension}"
            )
        
        # ‚úÖ Validar MIME type
        if file.content_type not in settings.ALLOWED_MIME_TYPES:
            raise HTTPException(
                status_code=400,
                detail=f"MIME type not allowed: {file.content_type}"
            )
        
        # ‚úÖ Scan b√°sico de malware (se configurado)
        if settings.ENABLE_MALWARE_SCAN:
            await self._scan_malware(file)

# ‚úÖ Endpoint de upload com m√∫ltiplos arquivos
@router.post("/upload/multiple")
async def upload_multiple_files(
    files: List[UploadFile] = File(...),
    workspace_id: int = Form(...),
    category: str = Form("general"),
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """Upload de m√∫ltiplos arquivos."""
    
    if len(files) > settings.MAX_FILES_PER_UPLOAD:
        raise HTTPException(
            status_code=400,
            detail=f"Too many files. Max: {settings.MAX_FILES_PER_UPLOAD}"
        )
    
    storage_manager = StorageManager()
    uploaded_files = []
    
    for file in files:
        try:
            file_record = await storage_manager.upload_file(
                file=file,
                user_id=current_user.id,
                workspace_id=workspace_id,
                category=category
            )
            
            db.add(file_record)
            uploaded_files.append(file_record)
            
        except HTTPException:
            # ‚úÖ Se um arquivo falhar, continuar com os outros
            logger.warning(f"Failed to upload file: {file.filename}")
            continue
    
    await db.commit()
    
    return {
        "uploaded_files": len(uploaded_files),
        "total_files": len(files),
        "files": [f.to_dict() for f in uploaded_files]
    }
```

## üìä SISTEMA DE LOGGING UNIFICADO - REGRAS CR√çTICAS

### **üö® √öNICA FONTE DE VERDADE: `src/synapse/logger_config.py`**

**CONTEXTO**: O SynapScale possui um sistema de logging unificado que integra as melhores funcionalidades de logging estruturado, mantendo compatibilidade total com c√≥digo existente.

**‚ùå ARQUIVOS REMOVIDOS (NUNCA RECRIAR):**
- `src/synapse/core/logging_system.py` - DELETADO (era sistema avan√ßado √≥rf√£o)
- `src/synapse/middlewares/logging.py` - DELETADO (funcionalidade duplicada)

**‚úÖ √öNICO ARQUIVO V√ÅLIDO:**
- `src/synapse/logger_config.py` - Sistema unificado principal

### **Importa√ß√£o Obrigat√≥ria**
```python
# ‚úÖ SEMPRE use esta importa√ß√£o √∫nica
from synapse.logger_config import get_logger, get_error_stats

# ‚úÖ Instanciar logger
logger = get_logger(__name__)  # Retorna UnifiedLogger com todas as funcionalidades

# ‚ùå NUNCA use estas importa√ß√µes (arquivos deletados)
from synapse.core.logging_system import get_logger  # ‚ùå PROIBIDO
from synapse.middlewares.logging import LoggingMiddleware  # ‚ùå PROIBIDO
```

### **Sistema Unificado - Funcionalidades Integradas**

**üîß CARACTER√çSTICAS T√âCNICAS:**
- **UnifiedLogger**: Combina funcionalidades b√°sicas + avan√ßadas
- **ErrorTracker**: Rastreamento autom√°tico de erros com estat√≠sticas
- **UnifiedJSONFormatter**: Formata√ß√£o JSON estruturada para produ√ß√£o
- **Handlers Rotativos**: 10MB principal, 5MB erros (autom√°tico em produ√ß√£o)
- **Compatibilidade Total**: Interface `get_logger()` inalterada

### **Padr√µes de Uso Corretos**

```python
# ‚úÖ Logging b√°sico (interface compat√≠vel)
from synapse.logger_config import get_logger

logger = get_logger(__name__)

# Funciona exatamente como antes
logger.logger.info("Opera√ß√£o realizada com sucesso")
logger.logger.error("Erro encontrado", extra={"user_id": 123})

# ‚úÖ Logging estruturado avan√ßado (JSON autom√°tico em produ√ß√£o)
logger.logger.info("User action performed", extra={
    "user_id": user.id,
    "workspace_id": workspace.id,
    "action": "create_workflow",
    "execution_time": 0.245,
    "ip_address": request.client.host,
    "endpoint_category": "workflows"
})

# ‚úÖ Logging de erro com rastreamento autom√°tico
try:
    result = await complex_operation()
except Exception as e:
    logger.logger.error("Operation failed", extra={
        "user_id": user.id,
        "operation": "complex_operation",
        "error_type": type(e).__name__,
        "error_count": 1,
        "url": "/api/v1/operation",
        "method": "POST"
    })
    raise

# ‚úÖ Acessar estat√≠sticas de erro
from synapse.logger_config import get_error_stats

stats = get_error_stats()
# Retorna: uptime, total_errors, error_rate, error_counts_by_type, etc.
```

### **Context Fields Suportados**

O sistema reconhece automaticamente estes campos no `extra`:
```python
context_fields = [
    "request_id",        # ID √∫nico da requisi√ß√£o
    "user_id",          # ID do usu√°rio
    "endpoint_category", # Categoria do endpoint (workflows, llm, etc.)
    "url",              # URL da requisi√ß√£o
    "method",           # M√©todo HTTP
    "status_code",      # C√≥digo de status HTTP
    "process_time",     # Tempo de processamento
    "error_type",       # Tipo do erro (ValidationError, DatabaseError)
    "error_count"       # Contador de erros
]

# ‚úÖ Exemplo completo
logger.logger.warning("Rate limit approached", extra={
    "request_id": "req-123",
    "user_id": user.id,
    "endpoint_category": "llm",
    "url": "/api/v1/llm/generate",
    "method": "POST",
    "error_type": "RateLimitWarning",
    "remaining_requests": 5
})
```

### **Ambientes e Formata√ß√£o**

```python
# üèóÔ∏è DESENVOLVIMENTO: Formato leg√≠vel
# 2024-01-15 10:30:00,123 - INFO - synapse.api.endpoints - User action performed

# üöÄ PRODU√á√ÉO: JSON estruturado autom√°tico
{
    "timestamp": "2024-01-15T10:30:00.123Z",
    "level": "INFO",
    "logger": "synapse.api.endpoints",
    "message": "User action performed",
    "module": "endpoints",
    "function": "create_workflow",
    "line": 45,
    "user_id": 123,
    "workspace_id": 456,
    "action": "create_workflow",
    "execution_time": 0.245
}
```

### **Performance Logging Pattern**

```python
# ‚úÖ Context manager para performance
import time
from contextlib import asynccontextmanager

@asynccontextmanager
async def log_performance(operation: str, user_id: int = None):
    start_time = time.time()
    try:
        yield
    finally:
        duration = time.time() - start_time
        logger.logger.info("Performance metric", extra={
            "operation": operation,
            "duration": duration,
            "user_id": user_id,
            "slow": duration > 1.0  # Flag para opera√ß√µes lentas
        })

# ‚úÖ Uso do context manager
async def slow_operation():
    async with log_performance("database_migration", user_id=123):
        await run_migration()
```

### **Error Tracking Autom√°tico**

```python
# ‚úÖ O ErrorTracker funciona automaticamente
# Cada erro logado √© automaticamente rastreado

# Exemplo: Endpoint com erro
@router.post("/api/v1/workflows")
async def create_workflow(request: CreateWorkflowRequest):
    try:
        # ... l√≥gica ...
        pass
    except ValidationError as e:
        # ‚úÖ Erro √© automaticamente rastreado pelo sistema
        logger.logger.error("Validation failed", extra={
            "error_type": "ValidationError",
            "endpoint_category": "workflows",
            "url": "/api/v1/workflows",
            "user_id": request.user_id
        })
        raise HTTPException(status_code=400, detail=str(e))

# ‚úÖ Acessar estat√≠sticas
stats = get_error_stats()
print(f"Taxa de erro: {stats['error_rate']:.2f} erros/segundo")
print(f"Total de erros: {stats['total_errors']}")
print(f"Erros por tipo: {stats['error_counts_by_type']}")
```

### **Handlers de Arquivo (Produ√ß√£o)**

```python
# ‚úÖ Configura√ß√£o autom√°tica em produ√ß√£o
# - logs/synapse.log (10MB, 5 rota√ß√µes)
# - logs/synapse_errors.log (5MB, 3 rota√ß√µes)

# Ativa√ß√£o manual em desenvolvimento:
# export ENABLE_FILE_LOGGING=true

# ‚úÖ Estrutura de logs
logs/
‚îú‚îÄ‚îÄ synapse.log              # Logs principais
‚îú‚îÄ‚îÄ synapse.log.1           # Rota√ß√£o 1
‚îú‚îÄ‚îÄ synapse.log.2           # Rota√ß√£o 2
‚îú‚îÄ‚îÄ synapse_errors.log      # Apenas erros (ERROR+)
‚îî‚îÄ‚îÄ synapse_errors.log.1    # Rota√ß√£o de erros
```

### **Middleware Integration**

```python
# ‚úÖ Integra√ß√£o com middleware de requisi√ß√µes
from synapse.logger_config import RequestContextFilter

# O sistema automaticamente captura:
# - request_id de headers X-Request-ID
# - user_id de token JWT
# - timing de requisi√ß√µes
# - status codes
# - URLs e m√©todos HTTP

# ‚úÖ Exemplo de log autom√°tico de middleware
# INFO: Request completed {"request_id": "req-123", "user_id": 456, "method": "POST", 
#       "url": "/api/v1/workflows", "status_code": 201, "process_time": 0.245}
```

### **Regras de Migra√ß√£o (Para Desenvolvedores)**

```python
# ‚úÖ ANTES (c√≥digo antigo compat√≠vel)
from synapse.logger_config import get_logger
logger = get_logger("my_module")
logger.logger.info("Message")

# ‚úÖ DEPOIS (mesmo c√≥digo, funcionalidades extras autom√°ticas)
from synapse.logger_config import get_logger
logger = get_logger("my_module")  # Agora retorna UnifiedLogger
logger.logger.info("Message", extra={"user_id": 123})  # JSON em produ√ß√£o

# ‚ùå NUNCA FAZER
from synapse.core.logging_system import get_logger  # Arquivo deletado
from synapse.middlewares.logging import LoggingMiddleware  # Arquivo deletado
```

### **M√©tricas & Monitoramento**
- ErrorTracker integrado com estat√≠sticas em tempo real
- Logs rotativos autom√°ticos em produ√ß√£o
- JSON estruturado para integra√ß√£o com Loki/Grafana
- Context fields padronizados para correla√ß√£o
- Performance tracking com flags autom√°ticas

### **Testes do Sistema Unificado**

```python
# ‚úÖ Executar testes do sistema unificado
python test_error_handling_system.py       # Teste completo de error handling + logging
python scripts/test_centralized_logging.py # Teste espec√≠fico de logging centralizado

# ‚úÖ Ambos devem passar com sucesso (exit code 0)
# ‚úÖ Logs devem aparecer no formato correto (desenvolvimento vs produ√ß√£o)
# ‚úÖ ErrorTracker deve funcionar automaticamente
# ‚úÖ Arquivos de log devem ser criados em logs/ (se ENABLE_FILE_LOGGING=true)
```

### **Estrutura Final Consolidada**

```
src/synapse/
‚îú‚îÄ‚îÄ logger_config.py          # ‚úÖ SISTEMA UNIFICADO √öNICO
‚îú‚îÄ‚îÄ error_handlers.py         # ‚úÖ Handlers de erro integrados
‚îî‚îÄ‚îÄ exceptions.py             # ‚úÖ Exce√ß√µes customizadas

‚ùå ARQUIVOS REMOVIDOS:
‚îú‚îÄ‚îÄ core/logging_system.py    # ‚ùå DELETADO (redundante)
‚îî‚îÄ‚îÄ middlewares/logging.py    # ‚ùå DELETADO (integrado)

logs/                         # ‚úÖ Logs autom√°ticos em produ√ß√£o
‚îú‚îÄ‚îÄ synapse.log              # Logs principais (10MB rotativo)
‚îú‚îÄ‚îÄ synapse_errors.log       # Apenas erros (5MB rotativo)
‚îî‚îÄ‚îÄ *.log.1, *.log.2        # Rota√ß√µes autom√°ticas
```

### **Regra de Ouro: NUNCA RECRIAR**
Se voc√™ encontrar c√≥digo tentando importar dos arquivos deletados, **SEMPRE** migre para o sistema unificado. **NUNCA** recrie os arquivos antigos. Use apenas:

```python
from synapse.logger_config import get_logger, get_error_stats
```

---

## üóÑÔ∏è SISTEMA DE DATABASE UNIFICADO - REGRAS CR√çTICAS

### **üö® √öNICA FONTE DE VERDADE: `src/synapse/database.py`**

**CONTEXTO**: O SynapScale possui um sistema de database unificado que centraliza toda a configura√ß√£o de conex√£o, sess√µes e engine SQLAlchemy, mantendo compatibilidade total e performance otimizada.

**‚ùå ARQUIVOS PROIBIDOS (NUNCA CRIAR):**
- `src/synapse/core/database.py` - PROIBIDO (duplica√ß√£o)
- `src/synapse/core/database/` - PROIBIDO (diret√≥rio desnecess√°rio)
- `src/synapse/db.py` - PROIBIDO (nome n√£o padronizado)
- `src/synapse/core/db/` - PROIBIDO (estrutura antiga)
- `src/synapse/database/database.py` - PROIBIDO (duplica√ß√£o aninhada)

**‚úÖ √öNICO ARQUIVO V√ÅLIDO:**
- `src/synapse/database.py` - Sistema unificado de database

### **Importa√ß√£o Obrigat√≥ria**
```python
# ‚úÖ SEMPRE use estas importa√ß√µes √∫nicas
from synapse.database import (
    get_db,           # Dependency para FastAPI endpoints
    get_async_db,     # Sess√£o async para opera√ß√µes complexas
    engine,           # Engine SQLAlchemy para opera√ß√µes diretas
    Base,             # Base class para todos os models
    SessionLocal      # Classe de sess√£o local
)

# ‚ùå NUNCA use estas importa√ß√µes (localiza√ß√µes proibidas)
from synapse.core.database import get_db  # ‚ùå PROIBIDO
from synapse.core.database.connection import engine  # ‚ùå PROIBIDO
from synapse.db import Base  # ‚ùå PROIBIDO
from synapse.core.db.session import SessionLocal  # ‚ùå PROIBIDO
```

### **Sistema Unificado - Funcionalidades Integradas**

**üîß CARACTER√çSTICAS T√âCNICAS:**
- **Engine Unificado**: PostgreSQL com asyncpg + psycopg2 para compatibilidade
- **Connection Pooling**: Pool otimizado com configura√ß√µes de produ√ß√£o
- **Session Management**: Async/sync sessions com context managers
- **Base Declarativa**: Base √∫nica para todos os models
- **Transaction Support**: Transa√ß√µes autom√°ticas e manuais
- **Health Checks**: Verifica√ß√£o de conectividade integrada

### **Padr√µes de Uso Corretos**

```python
# ‚úÖ Dependency para endpoints FastAPI
from synapse.database import get_db
from sqlalchemy.orm import Session

@router.get("/users/{user_id}")
async def get_user(
    user_id: int,
    db: Session = Depends(get_db)  # ‚úÖ Padr√£o correto
):
    user = db.query(User).filter(User.id == user_id).first()
    return user

# ‚úÖ Sess√£o async para opera√ß√µes complexas
from synapse.database import get_async_db
from sqlalchemy.ext.asyncio import AsyncSession

async def complex_operation():
    async with get_async_db() as db:
        # Opera√ß√µes async complexas
        result = await db.execute(select(User).where(User.is_active == True))
        users = result.scalars().all()
        return users

# ‚úÖ Transa√ß√µes manuais
from synapse.database import SessionLocal
from sqlalchemy.exc import SQLAlchemyError

async def transaction_example():
    db = SessionLocal()
    try:
        async with db.begin():
            # Opera√ß√µes transacionais
            user = User(email="test@example.com")
            db.add(user)
            await db.flush()  # Para obter ID
            
            profile = UserProfile(user_id=user.id, name="Test")
            db.add(profile)
            # Commit autom√°tico se n√£o houver exce√ß√£o
    except SQLAlchemyError as e:
        logger.error(f"Transaction failed: {e}")
        # Rollback autom√°tico
        raise
    finally:
        await db.close()

# ‚úÖ Models com Base unificada
from synapse.database import Base
from sqlalchemy import Column, Integer, String, DateTime
from sqlalchemy.sql import func

class MyModel(Base):
    __tablename__ = "my_models"
    __table_args__ = {"schema": "synapscale_db"}
    
    id = Column(Integer, primary_key=True, index=True)
    name = Column(String(255), nullable=False)
    created_at = Column(DateTime(timezone=True), server_default=func.now())
```

### **Configura√ß√µes de Conex√£o**

```python
# ‚úÖ Pool de conex√µes otimizado (configurado automaticamente)
# - pool_size: 20 conex√µes base
# - max_overflow: 30 conex√µes extras
# - pool_timeout: 30 segundos
# - pool_recycle: 3600 segundos (1 hora)
# - pool_pre_ping: True (verifica√ß√£o de conex√£o)

# ‚úÖ Engines configurados
# - engine: Sync engine para FastAPI dependencies
# - async_engine: Async engine para opera√ß√µes complexas
# - Ambos conectam na mesma DATABASE_URL

# ‚úÖ Health check autom√°tico
from synapse.database import check_db_health

async def verify_database():
    is_healthy = await check_db_health()
    if not is_healthy:
        raise HTTPException(status_code=503, detail="Database unavailable")
```

### **Padr√µes de Query Otimizados**

```python
# ‚úÖ Queries eficientes com relacionamentos
from sqlalchemy.orm import selectinload, joinedload
from sqlalchemy import select, and_, or_

# Evitar N+1 queries
async def get_users_with_workspaces():
    async with get_async_db() as db:
        stmt = select(User).options(
            joinedload(User.workspaces),      # 1:N relationship
            selectinload(User.files)          # 1:N relationship com muitos itens
        ).where(User.is_active == True)
        
        result = await db.execute(stmt)
        return result.unique().scalars().all()

# ‚úÖ Queries complexas com filtros
async def search_workflows(user_id: int, status: List[str], date_from: datetime):
    async with get_async_db() as db:
        stmt = select(Workflow).where(
            and_(
                Workflow.user_id == user_id,
                Workflow.status.in_(status),
                Workflow.created_at >= date_from
            )
        ).order_by(Workflow.created_at.desc()).limit(50)
        
        result = await db.execute(stmt)
        return result.scalars().all()

# ‚úÖ Agrega√ß√µes eficientes
async def get_user_stats(user_id: int):
    async with get_async_db() as db:
        stmt = select(
            func.count(Workflow.id).label('total_workflows'),
            func.avg(Workflow.execution_time).label('avg_time'),
            func.sum(
                case((Workflow.status == 'completed', 1), else_=0)
            ).label('completed_workflows')
        ).where(Workflow.user_id == user_id)
        
        result = await db.execute(stmt)
        return result.first()._asdict()
```

### **Migration & Schema Management**

```python
# ‚úÖ Todas as migra√ß√µes devem usar a Base unificada
from synapse.database import Base

# ‚úÖ Comando para gerar migra√ß√£o
# python -m alembic revision --autogenerate -m "add_new_table"

# ‚úÖ Schema obrigat√≥rio para todas as tabelas
__table_args__ = {"schema": "synapscale_db"}

# ‚úÖ Imports corretos em migra√ß√µes
# from synapse.database import Base
# from synapse.models.user import User  # Para relacionamentos
```

### **Error Handling & Recovery**

```python
# ‚úÖ Tratamento robusto de erros de database
from sqlalchemy.exc import (
    SQLAlchemyError, 
    IntegrityError, 
    OperationalError,
    DisconnectionError
)

async def robust_database_operation():
    max_retries = 3
    for attempt in range(max_retries):
        try:
            async with get_async_db() as db:
                # Opera√ß√£o de database
                result = await db.execute(stmt)
                await db.commit()
                return result
                
        except DisconnectionError as e:
            logger.warning(f"Database disconnection, retry {attempt + 1}/{max_retries}")
            if attempt == max_retries - 1:
                raise HTTPException(status_code=503, detail="Database unavailable")
            await asyncio.sleep(2 ** attempt)  # Backoff exponencial
            
        except IntegrityError as e:
            logger.error(f"Data integrity error: {e}")
            raise HTTPException(status_code=400, detail="Data conflict")
            
        except OperationalError as e:
            logger.error(f"Database operational error: {e}")
            raise HTTPException(status_code=503, detail="Database error")
            
        except SQLAlchemyError as e:
            logger.error(f"Unexpected database error: {e}")
            raise HTTPException(status_code=500, detail="Internal database error")
```

### **Performance & Monitoring**

```python
# ‚úÖ Monitoring de conex√µes
from synapse.database import engine

async def get_connection_stats():
    pool = engine.pool
    return {
        "pool_size": pool.size(),
        "checked_in": pool.checkedin(),
        "checked_out": pool.checkedout(),
        "overflow": pool.overflow(),
        "invalid": pool.invalid()
    }

# ‚úÖ Query performance logging
import time
from contextlib import asynccontextmanager

@asynccontextmanager
async def log_query_performance(operation: str):
    start_time = time.time()
    try:
        yield
    finally:
        duration = time.time() - start_time
        if duration > 1.0:  # Log queries lentas
            logger.warning(f"Slow query detected", extra={
                "operation": operation,
                "duration": duration,
                "slow_query": True
            })
```

### **Regras de Importa√ß√£o Database**

```python
# ‚úÖ SEMPRE usar importa√ß√µes do arquivo unificado
from synapse.database import get_db, get_async_db, Base, engine, SessionLocal

# ‚úÖ Para models
from synapse.database import Base

class MyModel(Base):
    # defini√ß√£o do model

# ‚úÖ Para endpoints
from synapse.database import get_db

@router.get("/endpoint")
async def my_endpoint(db: Session = Depends(get_db)):
    # l√≥gica do endpoint

# ‚ùå NUNCA usar estas importa√ß√µes
from synapse.core.database import get_db  # ‚ùå Localiza√ß√£o proibida
from synapse.db import Base  # ‚ùå Arquivo n√£o padr√£o
from synapse.core.db.session import SessionLocal  # ‚ùå Estrutura antiga
```

---

## ‚öôÔ∏è SISTEMA DE CONFIGURA√á√ÉO UNIFICADO - REGRAS CR√çTICAS

### **üö® √öNICA FONTE DE VERDADE: `src/synapse/core/config.py`**

**CONTEXTO**: O SynapScale possui um sistema de configura√ß√£o centralizado usando Pydantic v2 que gerencia todas as configura√ß√µes do sistema, incluindo database, LLM providers, storage, security, e muito mais.

**‚ùå ARQUIVOS PROIBIDOS (NUNCA CRIAR):**
- `src/synapse/config.py` - PROIBIDO (√≥rf√£o deletado)
- `src/synapse/core/config_new.py` - PROIBIDO (arquivo tempor√°rio deletado)
- `src/synapse/core/config/config.py` - PROIBIDO (diret√≥rio desnecess√°rio)
- `src/synapse/core/config/constants.py` - PROIBIDO (constantes est√£o no config principal)
- `src/synapse/core/unified_config.py` - PROIBIDO (erro tempor√°rio deletado)
- `src/synapse/settings.py` - PROIBIDO (nome n√£o padronizado)

**‚úÖ √öNICO ARQUIVO V√ÅLIDO:**
- `src/synapse/core/config.py` - Sistema unificado de configura√ß√£o

### **Importa√ß√£o Obrigat√≥ria**
```python
# ‚úÖ SEMPRE use estas importa√ß√µes √∫nicas
from synapse.core.config import (
    settings,           # Inst√¢ncia principal de configura√ß√£o
    FILE_CATEGORIES,    # Constantes de categorias de arquivo
    setup_logging,      # Configura√ß√£o de logging
    validate_settings,  # Valida√ß√£o de configura√ß√µes
    get_settings        # Factory function para settings
)

# ‚ùå NUNCA use estas importa√ß√µes (arquivos deletados/proibidos)
from synapse.config import settings  # ‚ùå PROIBIDO - arquivo √≥rf√£o deletado
from synapse.core.config_new import settings  # ‚ùå PROIBIDO - arquivo removido
from synapse.core.config.constants import FILE_CATEGORIES  # ‚ùå PROIBIDO - diret√≥rio deletado
from synapse.core.unified_config import settings  # ‚ùå PROIBIDO - arquivo tempor√°rio deletado
from synapse.settings import settings  # ‚ùå PROIBIDO - nome n√£o padronizado
```

### **Sistema Unificado - Funcionalidades Integradas**

**üîß CARACTER√çSTICAS T√âCNICAS:**
- **Pydantic v2**: Valida√ß√£o robusta e type safety
- **Environment Variables**: Carregamento autom√°tico de .env
- **Multiple Providers**: LLM, Storage, Database, Email, etc.
- **Validation**: Valida√ß√£o autom√°tica de URLs, emails, tokens
- **Security**: Configura√ß√µes de JWT, CORS, encryption
- **Performance**: Cache de configura√ß√µes, lazy loading
- **Development/Production**: Configura√ß√µes espec√≠ficas por ambiente

### **Configura√ß√µes Dispon√≠veis**

```python
# ‚úÖ Configura√ß√µes de Database
settings.DATABASE_URL          # URL completa do PostgreSQL
settings.DB_POOL_SIZE          # Tamanho do pool de conex√µes
settings.DB_MAX_OVERFLOW       # Conex√µes extras permitidas
settings.DB_POOL_TIMEOUT       # Timeout para obter conex√£o

# ‚úÖ Configura√ß√µes de Redis
settings.REDIS_URL             # URL do Redis para cache/sessions
settings.CACHE_DEFAULT_TTL     # TTL padr√£o para cache (300s)
settings.REDIS_MAX_CONNECTIONS # M√°ximo de conex√µes Redis

# ‚úÖ Configura√ß√µes de LLM Providers
settings.OPENAI_API_KEY        # Chave API OpenAI
settings.ANTHROPIC_API_KEY     # Chave API Anthropic/Claude
settings.GOOGLE_API_KEY        # Chave API Google/Gemini
settings.GROK_API_KEY          # Chave API Grok/XAI
settings.DEEPSEEK_API_KEY      # Chave API DeepSeek
settings.TESS_API_KEY          # Chave API Tess (nacional)

# ‚úÖ Configura√ß√µes de Storage
settings.STORAGE_TYPE          # "local", "s3", "gcs"
settings.STORAGE_BASE_PATH     # Caminho base para arquivos
settings.AWS_ACCESS_KEY_ID     # Credenciais AWS S3
settings.AWS_SECRET_ACCESS_KEY
settings.AWS_S3_BUCKET
settings.GCS_BUCKET_NAME       # Google Cloud Storage

# ‚úÖ Configura√ß√µes de Security
settings.SECRET_KEY            # Chave secreta JWT
settings.JWT_ALGORITHM         # Algoritmo JWT (HS256)
settings.ACCESS_TOKEN_EXPIRE   # Expira√ß√£o token (30 min)
settings.REFRESH_TOKEN_EXPIRE  # Expira√ß√£o refresh (7 dias)
settings.ENCRYPTION_KEY        # Chave para criptografia sim√©trica

# ‚úÖ Configura√ß√µes de Email
settings.SMTP_HOST             # Servidor SMTP
settings.SMTP_PORT             # Porta SMTP
settings.SMTP_USERNAME         # Usu√°rio SMTP
settings.SMTP_PASSWORD         # Senha SMTP
settings.EMAIL_FROM            # Email remetente padr√£o

# ‚úÖ Configura√ß√µes de Rate Limiting
settings.RATE_LIMIT_ENABLED    # Ativar rate limiting
settings.DEFAULT_RATE_LIMIT    # Limite padr√£o (100/minute)
settings.LLM_RATE_LIMIT        # Limite para LLM (20/hour)
settings.UPLOAD_RATE_LIMIT     # Limite para uploads (10/minute)

# ‚úÖ Configura√ß√µes de Files
settings.MAX_FILE_SIZE         # Tamanho m√°ximo arquivo (50MB)
settings.ALLOWED_EXTENSIONS    # Extens√µes permitidas
settings.ALLOWED_MIME_TYPES    # MIME types permitidos
settings.ENABLE_MALWARE_SCAN   # Ativar scan de malware

# ‚úÖ Configura√ß√µes de Development
settings.DEBUG                 # Modo debug
settings.ENVIRONMENT           # "development", "staging", "production"
settings.LOG_LEVEL             # N√≠vel de log
settings.ENABLE_CORS           # Ativar CORS
settings.CORS_ORIGINS          # Origens permitidas CORS
```

### **Padr√µes de Uso Corretos**

```python
# ‚úÖ Acessar configura√ß√µes
from synapse.core.config import settings

# Usar configura√ß√µes em services
class LLMService:
    def __init__(self):
        self.openai_key = settings.OPENAI_API_KEY
        self.anthropic_key = settings.ANTHROPIC_API_KEY
        self.max_tokens = settings.LLM_MAX_TOKENS
    
    async def generate_response(self, provider: str):
        if provider == "openai" and not self.openai_key:
            raise ValueError("OpenAI API key not configured")
        # ... l√≥gica

# ‚úÖ Valida√ß√£o de configura√ß√µes obrigat√≥rias
from synapse.core.config import validate_settings

async def startup_validation():
    """Validar configura√ß√µes na inicializa√ß√£o."""
    try:
        validate_settings()
        logger.info("All settings validated successfully")
    except ValueError as e:
        logger.error(f"Configuration error: {e}")
        raise

# ‚úÖ Configura√ß√µes por ambiente
if settings.ENVIRONMENT == "development":
    # Configura√ß√µes espec√≠ficas de desenvolvimento
    enable_debug_mode()
elif settings.ENVIRONMENT == "production":
    # Configura√ß√µes espec√≠ficas de produ√ß√£o
    setup_monitoring()

# ‚úÖ Usar constantes definidas
from synapse.core.config import FILE_CATEGORIES

def validate_file_category(category: str):
    if category not in FILE_CATEGORIES:
        raise ValueError(f"Invalid category. Must be one of: {FILE_CATEGORIES}")
    return category

# ‚úÖ Setup de logging usando configura√ß√µes
from synapse.core.config import setup_logging

def configure_application():
    setup_logging()  # Usa settings.LOG_LEVEL automaticamente
```

### **Configura√ß√µes de Providers LLM**

```python
# ‚úÖ Configura√ß√£o completa de providers
from synapse.core.config import settings

class UnifiedLLMConfig:
    """Configura√ß√£o centralizada para todos os providers LLM."""
    
    @property
    def openai_config(self) -> dict:
        return {
            "api_key": settings.OPENAI_API_KEY,
            "base_url": settings.OPENAI_BASE_URL,
            "max_tokens": settings.OPENAI_MAX_TOKENS,
            "timeout": settings.LLM_TIMEOUT
        }
    
    @property
    def anthropic_config(self) -> dict:
        return {
            "api_key": settings.ANTHROPIC_API_KEY,
            "base_url": settings.ANTHROPIC_BASE_URL,
            "max_tokens": settings.ANTHROPIC_MAX_TOKENS,
            "timeout": settings.LLM_TIMEOUT
        }
    
    @property
    def google_config(self) -> dict:
        return {
            "api_key": settings.GOOGLE_API_KEY,
            "project_id": settings.GOOGLE_PROJECT_ID,
            "location": settings.GOOGLE_LOCATION,
            "max_tokens": settings.GOOGLE_MAX_TOKENS
        }
    
    def get_provider_config(self, provider: str) -> dict:
        """Obter configura√ß√£o espec√≠fica do provider."""
        configs = {
            "openai": self.openai_config,
            "anthropic": self.anthropic_config,
            "google": self.google_config,
            "grok": self.grok_config,
            "deepseek": self.deepseek_config,
            "tess": self.tess_config
        }
        
        if provider not in configs:
            raise ValueError(f"Unsupported provider: {provider}")
        
        config = configs[provider]
        if not config.get("api_key"):
            raise ValueError(f"API key not configured for provider: {provider}")
        
        return config
```

### **Valida√ß√£o Avan√ßada de Configura√ß√µes**

```python
# ‚úÖ Valida√ß√µes customizadas
from pydantic import field_validator, model_validator
from typing import List, Optional

class Settings(BaseSettings):
    # ... outros campos ...
    
    @field_validator('DATABASE_URL')
    @classmethod
    def validate_database_url(cls, v: str) -> str:
        if not v.startswith(('postgresql://', 'postgresql+asyncpg://', 'postgresql+psycopg2://')):
            raise ValueError('DATABASE_URL must be a PostgreSQL URL')
        return v
    
    @field_validator('REDIS_URL')
    @classmethod
    def validate_redis_url(cls, v: str) -> str:
        if not v.startswith('redis://'):
            raise ValueError('REDIS_URL must be a Redis URL')
        return v
    
    @field_validator('CORS_ORIGINS')
    @classmethod
    def validate_cors_origins(cls, v: List[str]) -> List[str]:
        for origin in v:
            if not origin.startswith(('http://', 'https://')):
                raise ValueError(f'Invalid CORS origin: {origin}')
        return v
    
    @model_validator(mode='after')
    def validate_llm_providers(self):
        """Validar que pelo menos um provider LLM est√° configurado."""
        llm_keys = [
            self.OPENAI_API_KEY,
            self.ANTHROPIC_API_KEY,
            self.GOOGLE_API_KEY,
            self.GROK_API_KEY
        ]
        
        if not any(llm_keys):
            raise ValueError('At least one LLM provider API key must be configured')
        
        return self
    
    @model_validator(mode='after')
    def validate_storage_config(self):
        """Validar configura√ß√£o de storage."""
        if self.STORAGE_TYPE == "s3":
            if not all([self.AWS_ACCESS_KEY_ID, self.AWS_SECRET_ACCESS_KEY, self.AWS_S3_BUCKET]):
                raise ValueError('S3 storage requires AWS credentials and bucket name')
        elif self.STORAGE_TYPE == "gcs":
            if not self.GCS_BUCKET_NAME:
                raise ValueError('GCS storage requires bucket name')
        
        return self
```

### **Environment Variables & .env**

```python
# ‚úÖ Configura√ß√£o de .env
from pydantic_settings import BaseSettings, SettingsConfigDict

class Settings(BaseSettings):
    model_config = SettingsConfigDict(
        env_file=".env",
        env_file_encoding="utf-8",
        case_sensitive=True,
        extra="ignore"  # Ignorar vari√°veis extras
    )
    
    # ‚úÖ Valores padr√£o para desenvolvimento
    ENVIRONMENT: str = "development"
    DEBUG: bool = False
    LOG_LEVEL: str = "INFO"
    
    # ‚úÖ Configura√ß√µes obrigat√≥rias
    DATABASE_URL: str
    REDIS_URL: str
    SECRET_KEY: str
    
    # ‚úÖ Configura√ß√µes opcionais com padr√µes
    MAX_FILE_SIZE: int = 50 * 1024 * 1024  # 50MB
    CACHE_DEFAULT_TTL: int = 300  # 5 minutos
    ACCESS_TOKEN_EXPIRE: int = 30  # 30 minutos

# ‚úÖ Inst√¢ncia singleton
settings = Settings()
```

### **Helpers e Utilities**

```python
# ‚úÖ Fun√ß√µes helper para configura√ß√µes
from synapse.core.config import settings

def get_database_config() -> dict:
    """Configura√ß√£o completa do database."""
    return {
        "url": settings.DATABASE_URL,
        "pool_size": settings.DB_POOL_SIZE,
        "max_overflow": settings.DB_MAX_OVERFLOW,
        "pool_timeout": settings.DB_POOL_TIMEOUT,
        "pool_recycle": settings.DB_POOL_RECYCLE,
        "echo": settings.DEBUG  # Log SQL queries em debug
    }

def get_cors_config() -> dict:
    """Configura√ß√£o do CORS."""
    return {
        "allow_origins": settings.CORS_ORIGINS,
        "allow_credentials": True,
        "allow_methods": ["GET", "POST", "PUT", "DELETE", "PATCH"],
        "allow_headers": ["*"]
    }

def get_security_config() -> dict:
    """Configura√ß√£o de seguran√ßa."""
    return {
        "secret_key": settings.SECRET_KEY,
        "algorithm": settings.JWT_ALGORITHM,
        "access_token_expire": settings.ACCESS_TOKEN_EXPIRE,
        "refresh_token_expire": settings.REFRESH_TOKEN_EXPIRE
    }

def is_production() -> bool:
    """Verificar se est√° em produ√ß√£o."""
    return settings.ENVIRONMENT == "production"

def is_development() -> bool:
    """Verificar se est√° em desenvolvimento."""
    return settings.ENVIRONMENT == "development"
```

### **Regras de Importa√ß√£o Config**

```python
# ‚úÖ SEMPRE usar importa√ß√µes do arquivo unificado
from synapse.core.config import settings, FILE_CATEGORIES, setup_logging

# ‚úÖ Para valida√ß√£o
from synapse.core.config import validate_settings, get_settings

# ‚úÖ Para helpers
from synapse.core.config import (
    get_database_config,
    get_cors_config,
    is_production
)

# ‚ùå NUNCA usar estas importa√ß√µes (arquivos deletados/proibidos)
from synapse.config import settings  # ‚ùå Arquivo √≥rf√£o deletado
from synapse.core.config_new import settings  # ‚ùå Arquivo tempor√°rio deletado
from synapse.core.config.constants import FILE_CATEGORIES  # ‚ùå Diret√≥rio deletado
from synapse.core.unified_config import settings  # ‚ùå Erro tempor√°rio deletado
from synapse.settings import settings  # ‚ùå Nome n√£o padronizado
```

### **Regra de Ouro: UMA CONFIGURA√á√ÉO, UMA FONTE**

**üéØ PRINC√çPIO FUNDAMENTAL:**
> "Um reposit√≥rio, uma configura√ß√£o, uma fonte de verdade."
> 
> Se voc√™ est√° pensando em criar outro arquivo de configura√ß√£o,
> **PARE** e adicione ao `src/synapse/core/config.py` existente.

**‚úÖ SEMPRE:**
- Adicionar novas configura√ß√µes no arquivo principal
- Usar valida√ß√£o Pydantic para novos campos
- Documentar configura√ß√µes com comments
- Testar configura√ß√µes em desenvolvimento

**‚ùå NUNCA:**
- Criar arquivos de configura√ß√£o duplicados
- Hardcoding de valores que deveriam ser configur√°veis
- Ignorar valida√ß√£o de configura√ß√µes cr√≠ticas
- Deixar configura√ß√µes sens√≠veis sem vari√°veis de ambiente

## ‚ö° CACHE REDIS & BACKGROUND TASKS

### **Cache Redis**
```python
# ‚úÖ Cache service com TTL configur√°vel
from synapse.core.cache import cache_service
import json
from typing import Any, Optional

class CacheService:
    def __init__(self):
        self.redis = redis.Redis.from_url(settings.REDIS_URL)
        self.default_ttl = settings.CACHE_DEFAULT_TTL
    
    async def get_or_set(
        self,
        key: str,
        func: callable,
        ttl: int = None,
        *args,
        **kwargs
    ) -> Any:
        """Cache pattern: busca ou calcula e armazena."""
        cached = await self.get(key)
        if cached is not None:
            return cached
        
        value = await func(*args, **kwargs) if asyncio.iscoroutinefunction(func) else func(*args, **kwargs)
        await self.set(key, value, ttl)
        return value

# ‚úÖ Decorator para cache autom√°tico
def cached(ttl: int = 300, key_prefix: str = ""):
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            cache_key = f"{key_prefix}:{func.__name__}:{hash(str(args) + str(kwargs))}"
            return await cache_service.get_or_set(cache_key, func, ttl, *args, **kwargs)
        return wrapper
    return decorator
```

### **Background Tasks com Celery**
```python
# ‚úÖ Tasks em background
from celery import Celery
from synapse.core.celery_app import celery_app

@celery_app.task(bind=True, max_retries=3)
def process_file_upload(self, file_id: str, user_id: int):
    """Processa upload de arquivo em background."""
    try:
        update_file_status(file_id, "processing")
        file_info = process_file_content(file_id)
        notify_user_file_ready(user_id, file_id, file_info)
        update_file_status(file_id, "ready")
    except Exception as e:
        logger.error(f"File processing failed: {file_id}, error: {e}")
        self.retry(countdown=2 ** self.request.retries)

# ‚úÖ Trigger em endpoints
@router.post("/files/upload")
async def upload_file_endpoint():
    # ... upload logic ...
    process_file_upload.delay(file.id, current_user.id)
    return {"message": "File uploaded, processing in background"}
```

## üß™ TESTES AVAN√áADOS

### **Estrutura Completa de Testes**
```python
# ‚úÖ Configura√ß√£o com fixtures
import pytest
from httpx import AsyncClient
from sqlalchemy.ext.asyncio import AsyncSession

@pytest.fixture
async def client():
    """Cliente HTTP para testes."""
    async with AsyncClient(app=app, base_url="http://test") as ac:
        yield ac

@pytest.fixture
async def test_user(db_session):
    """Usu√°rio de teste."""
    user = User(email="test@example.com", full_name="Test User", is_active=True)
    db_session.add(user)
    await db_session.commit()
    return user

@pytest.fixture
async def auth_headers(test_user):
    """Headers de autentica√ß√£o."""
    token = create_access_token(data={"sub": str(test_user.id)})
    return {"Authorization": f"Bearer {token}"}

# ‚úÖ Testes de integra√ß√£o completos
@pytest.mark.asyncio
async def test_create_workflow_complete_flow(
    client: AsyncClient,
    auth_headers: dict,
    test_user: User
):
    """Testa fluxo completo de cria√ß√£o de workflow."""
    
    # Criar workspace
    workspace_response = await client.post(
        "/api/v1/workspaces/",
        json={"name": "Test Workspace"},
        headers=auth_headers
    )
    assert workspace_response.status_code == 201
    workspace = workspace_response.json()
    
    # Criar workflow
    workflow_response = await client.post(
        "/api/v1/workflows/",
        json={
            "name": "Test Workflow",
            "workspace_id": workspace["id"],
            "steps": [{"name": "Step 1", "type": "llm_generation"}]
        },
        headers=auth_headers
    )
    
    assert workflow_response.status_code == 201
    assert workflow_response.json()["name"] == "Test Workflow"

# ‚úÖ Testes de performance
@pytest.mark.performance
async def test_endpoint_performance(client: AsyncClient, auth_headers: dict):
    """Testa performance de endpoints cr√≠ticos."""
    start_time = time.time()
    response = await client.get("/api/v1/analytics/dashboard", headers=auth_headers)
    duration = time.time() - start_time
    
    assert response.status_code == 200
    assert duration < 2.0  # Deve responder em menos de 2 segundos

# ‚úÖ Factories para dados de teste
class UserFactory(SQLAlchemyModelFactory):
    class Meta:
        model = User
        sqlalchemy_session_persistence = "commit"
    
    email = factory.Sequence(lambda n: f"user{n}@example.com")
    full_name = factory.Faker("name")
    is_active = True
```

### **Cobertura & Qualidade**
- Manter cobertura > 85%
- Testes unit√°rios, integra√ß√£o e e2e
- Mock servi√ßos externos (LLM, email, etc.)
- Testes de casos extremos e erros

## üöÄ PERFORMANCE

### **Otimiza√ß√µes**
- Use conex√µes ass√≠ncronas (asyncpg para PostgreSQL)
- Implemente cache Redis para dados frequentemente acessados
- Pagine resultados grandes (limit/offset)
- Use background tasks para opera√ß√µes demoradas

### **WebSockets**
- Use ConnectionManager para gerenciar conex√µes
- Implemente heartbeat para detectar conex√µes mortas
- Limite n√∫mero de conex√µes por usu√°rio

## üì± VERSIONAMENTO & DEPLOY

### **API Versioning**
- Mantenha compatibilidade com vers√µes anteriores
- Use deprecation warnings antes de remover features
- Documente breaking changes no CHANGELOG.md

### **Deploy**
- SEMPRE testar em ambiente de staging
- Usar vari√°veis de ambiente para configura√ß√£o
- Configurar health checks apropriados

## üîÑ WORKFLOWS & EXECU√á√ÉO

### **Engine de Execu√ß√£o**
- Use sistema de filas para execu√ß√µes longas
- Implemente retry com backoff exponencial
- Monitor timeouts e recursos utilizados

### **Nodes**
- Valide entradas antes da execu√ß√£o
- Mantenha estado consistente
- Log detalhado para debugging

## üìÅ ARQUIVOS & STORAGE

### **Upload**
- Valide tipo e tamanho de arquivo
- Use storage configur√°vel (local/S3/GCS)
- Implementar scan de malware em produ√ß√£o

### **Processamento**
- Use workers ass√≠ncronos para processamento
- Mantenha refer√™ncias consistentes no banco
- Cleanup de arquivos tempor√°rios

## üõ†Ô∏è DEBUGGING & TROUBLESHOOTING

### **Logs Estruturados**
- Include user_id, request_id nos logs
- Use structured logging (JSON) em produ√ß√£o
- Correlacione logs com m√©tricas

### **Health Checks**
- `/health` para status b√°sico
- `/health/detailed` para diagn√≥stico completo
- Monitor depend√™ncias externas

## üé® C√ìDIGO LIMPO

### **Naming Conventions**
- Fun√ß√µes: `verbo_objeto()` (ex: `create_user()`)
- Classes: `PascalCase` (ex: `UserService`)
- Constantes: `UPPER_CASE` (ex: `MAX_FILE_SIZE`)
- Vari√°veis: `snake_case` (ex: `user_data`)

### **Documenta√ß√£o**
- README.md atualizado
- Docstrings em ingl√™s
- Coment√°rios explicativos para l√≥gica complexa
- OpenAPI documentation completa

## ‚ö° COMANDOS √öTEIS & PRODU√á√ÉO

### **Desenvolvimento Local**
```bash
# Servidor de desenvolvimento
./dev.sh                            # Iniciar com hot-reload
./prod.sh                           # Simular ambiente de produ√ß√£o

# Banco de dados
python -m alembic upgrade head      # Aplicar migra√ß√µes
python -m alembic revision --autogenerate -m "descri√ß√£o"  # Nova migra√ß√£o
python -m alembic downgrade -1      # Rollback √∫ltima migra√ß√£o

# Testes & Qualidade
python -m pytest                   # Todos os testes
python -m pytest --cov=src --cov-report=html  # Com cobertura HTML
python scripts/run_tests.py        # Teste completo de endpoints
black src/ && isort src/           # Formata√ß√£o + imports
mypy src/                          # Verifica√ß√£o de tipos
```

### **Produ√ß√£o & Deploy**
```bash
# Health checks
curl https://api.synapscale.com/health
curl https://api.synapscale.com/health/detailed

# Monitoring
docker logs synapscale-backend --tail=100 -f
docker stats synapscale-backend

# Database backup (Render PostgreSQL)
pg_dump $DATABASE_URL > backup_$(date +%Y%m%d_%H%M%S).sql

# Deploy checklist
git tag v1.0.1                     # Tag da vers√£o
git push origin v1.0.1            # Push da tag
# Verificar CI/CD pipeline no Render
# Confirmar deploy atrav√©s dos logs
# Executar smoke tests b√°sicos
```

### **Debug & Troubleshooting**
```bash
# Logs estruturados
tail -f logs/app.log | grep ERROR
tail -f logs/app.log | grep "user_id=123"

# Database queries em desenvolvimento  
export SQLALCHEMY_ECHO=True        # Mostrar queries SQL

# Performance profiling
python -m cProfile -o profile.stats main.py
python -c "import pstats; pstats.Stats('profile.stats').sort_stats('cumulative').print_stats(20)"

# Memory debugging
python -m memory_profiler src/synapse/main.py

# Redis debugging
redis-cli ping                     # Verificar conex√£o
redis-cli info memory            # Status de mem√≥ria
redis-cli --scan --pattern "synapscale:*" | head -10  # Listar chaves
```

## üö® NUNCA FA√áA

**‚ùå CR√çTICO - ARQUIVO PRINCIPAL:**
‚ùå Criar `src/synapse/app.py` (arquivo removido, usar apenas main.py)
‚ùå Duplicar inst√¢ncia FastAPI fora de `src/synapse/main.py`
‚ùå Referenciar qualquer arquivo que n√£o seja `src.synapse.main:app` em deploy/scripts

**‚ùå OUTRAS REGRAS CR√çTICAS:**
‚ùå Hardcoding de valores sens√≠veis
‚ùå Commit de credenciais ou .env
‚ùå Queries SQL diretas sem valida√ß√£o
‚ùå Opera√ß√µes s√≠ncronas em endpoints ass√≠ncronos
‚ùå Ignorar tratamento de erros
‚ùå Deixar TODO comments no c√≥digo de produ√ß√£o
‚ùå Usar `import *`
‚ùå Modificar banco diretamente sem migra√ß√£o

## ‚úÖ SEMPRE FA√áA

**‚úÖ CR√çTICO - ARQUIVO PRINCIPAL:**
‚úÖ Usar APENAS `src/synapse/main.py` para inicializa√ß√£o FastAPI
‚úÖ Referenciar `src.synapse.main:app` em scripts, deploy e documenta√ß√£o
‚úÖ Modificar aplica√ß√£o principal SOMENTE em `main.py`

**‚úÖ OUTRAS REGRAS ESSENCIAIS:**
‚úÖ Validar inputs do usu√°rio
‚úÖ Usar transa√ß√µes para opera√ß√µes cr√≠ticas
‚úÖ Logs informativos mas n√£o verbosos demais
‚úÖ Testes para novas funcionalidades
‚úÖ Documentar APIs com exemplos
‚úÖ Revisar c√≥digo antes do commit
‚úÖ Considerar impacto de performance
‚úÖ Pensar em seguran√ßa primeiro

## üéØ RESUMO DAS MELHORES PR√ÅTICAS

### **üöÄ Prioridades de Desenvolvimento**
1. **Seguran√ßa Primeiro**: Valida√ß√£o, autentica√ß√£o, rate limiting
2. **Performance**: Cache Redis, queries otimizadas, background tasks
3. **Monitoramento**: Logs estruturados, m√©tricas, health checks
4. **Qualidade**: Testes > 85%, code review, type hints
5. **Documenta√ß√£o**: Taskmaster logs, API docs, commits claros

### **‚ö° Quick Wins para Performance**
- Use `@cached()` decorator para dados repetitivos
- Implemente `joinedload`/`selectinload` para relacionamentos
- Background tasks para opera√ß√µes > 2 segundos
- Rate limiting em endpoints sens√≠veis
- Pagination para listas grandes

### **üîí Checklist de Seguran√ßa**
- [ ] Input validation com Pydantic
- [ ] Sanitiza√ß√£o de HTML com bleach
- [ ] Rate limiting configurado
- [ ] Logs de auditoria para a√ß√µes cr√≠ticas
- [ ] API keys criptografadas no banco
- [ ] HTTPS em produ√ß√£o
- [ ] Backup autom√°tico do banco

### **üìä Monitoring Essencial**
- Logs estruturados com user_id/request_id
- M√©tricas de performance por endpoint
- Usage tracking para LLM (tokens/cost)
- Health checks para depend√™ncias
- Alertas para erros cr√≠ticos

### **üîÑ Workflow Di√°rio Recomendado**
```bash
# 1. Come√ßar sess√£o
task-master list                    # Ver tasks pendentes
task-master next                    # Pr√≥xima task a trabalhar

# 2. Durante desenvolvimento
task-master research "pergunta t√©cnica" --files=src/relevante/
# Implementar feature
task-master update-subtask --id=X.Y --prompt="Progresso detalhado..."

# 3. Finalizar
task-master set-status --id=X.Y --status=done
git commit -m "feat: descri√ß√£o\n\nTask: X.Y"
```

### **üß† Mindset SynapScale**
- **Think Production First**: Todo c√≥digo deve estar pronto para produ√ß√£o
- **Document Everything**: Use Taskmaster para manter hist√≥rico de decis√µes
- **Performance Matters**: Sistema serve milhares de usu√°rios
- **Security is Critical**: Dados sens√≠veis de empresas
- **Quality Over Speed**: Melhor fazer certo na primeira vez

---

**üéØ OBJETIVO: Construir a melhor plataforma de automa√ß√£o com IA do mercado - cada linha de c√≥digo conta!**

*√öltima atualiza√ß√£o: Janeiro 2024 | SynapScale Backend v2.0* 