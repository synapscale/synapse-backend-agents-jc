# ü¶ô Provedor LLaMA (Meta) - SynapScale

Esta documenta√ß√£o descreve como usar modelos LLaMA da Meta atrav√©s da **API LLM unificada** do SynapScale Backend.

> **üí° Nota:** Use sempre a API unificada `/api/v1/llm/*` para melhor compatibilidade e recursos avan√ßados.

## üöÄ Usando Modelos LLaMA

### **Endpoint Unificado**
```http
POST /api/v1/llm/generate
POST /api/v1/llm/chat
```

### **Modelos Dispon√≠veis**
- `llama-3.1-405b-instruct` - Modelo mais avan√ßado
- `llama-3.1-70b-instruct` - Balanceado
- `llama-3.1-8b-instruct` - R√°pido
- `llama-3-70b-instruct` - Vers√£o est√°vel
- `llama-3-8b-instruct` - Eficiente

## üìù Exemplos de Uso

### **Gera√ß√£o de Texto**

```bash
curl -X POST "http://localhost:8000/api/v1/llm/generate" \
  -H "Authorization: Bearer <access_token>" \
  -H "Content-Type: application/json" \
  -d '{
    "provider": "llama",
    "model": "llama-3.1-70b-instruct",
    "prompt": "Explique o conceito de machine learning",
    "max_tokens": 500,
    "temperature": 0.7
  }'
```

### **Chat Conversacional**

```bash
curl -X POST "http://localhost:8000/api/v1/llm/chat" \
  -H "Authorization: Bearer <access_token>" \
  -H "Content-Type: application/json" \
  -d '{
    "provider": "llama",
    "model": "llama-3.1-405b-instruct",
    "messages": [
      {"role": "system", "content": "Voc√™ √© um assistente √∫til."},
      {"role": "user", "content": "Como posso melhorar meu c√≥digo Python?"}
    ],
    "max_tokens": 1000,
    "temperature": 0.8
  }'
```

## ‚öôÔ∏è Par√¢metros Espec√≠ficos do LLaMA

| Par√¢metro | Tipo | Descri√ß√£o |
|-----------|------|-----------|
| `provider` | string | **Obrigat√≥rio**: `"llama"` |
| `model` | string | Modelo espec√≠fico (ver lista acima) |
| `temperature` | float | 0.0-2.0, controla criatividade |
| `top_p` | float | 0.0-1.0, amostragem nucleus |
| `top_k` | integer | 1-100, limita vocabul√°rio |
| `repetition_penalty` | float | 1.0-1.3, evita repeti√ß√£o |
| `system_prompt` | string | Prompt de sistema (chat) |

## üîÑ Resposta T√≠pica

```json
{
  "text": "Machine learning √© uma sub√°rea da intelig√™ncia artificial...",
  "model": "llama-3.1-70b-instruct",
  "provider": "llama",
  "tokens_used": 245,
  "cost": 0.0012,
  "cached": false,
  "processing_time": 2.1,
  "finish_reason": "stop"
}
```

## üéØ Casos de Uso Recomendados

### **LLaMA 3.1 405B**
- An√°lises complexas
- Racioc√≠nio avan√ßado
- Tarefas criativas sofisticadas

### **LLaMA 3.1 70B**
- Uso geral balanceado
- Conversas naturais
- An√°lise de c√≥digo

### **LLaMA 3.1 8B**
- Respostas r√°pidas
- Tarefas simples
- Alto volume de requisi√ß√µes

## üîß Configura√ß√£o de API Key

### **Usando API Key Global (Sistema)**
O sistema usar√° automaticamente a chave configurada nos settings do servidor.

### **Usando API Key Espec√≠fica do Usu√°rio**

```bash
# Configurar sua pr√≥pria chave LLaMA
curl -X POST "http://localhost:8000/api/v1/user-variables/api-keys/llama" \
  -H "Authorization: Bearer <access_token>" \
  -H "Content-Type: application/json" \
  -d '{
    "value": "sua-chave-llama-aqui",
    "description": "Minha chave LLaMA pessoal"
  }'
```

## üìä Limites e Cotas

- **Rate Limit**: 100 requisi√ß√µes/minuto (padr√£o)
- **Tokens por requisi√ß√£o**: M√°ximo 4.096 tokens
- **Concorr√™ncia**: 10 requisi√ß√µes simult√¢neas
- **Cache**: 1 hora para respostas id√™nticas

## üîó Links Relacionados

- **[API LLM Unificada](../integration_guide.md)** - Guia principal
- **[User API Keys](../../user_variables_guide.md)** - Configurar chaves pessoais
- **[Migration Guide](../migration_guide.md)** - Migrar de APIs antigas
- **[LLM Endpoints](../endpoints.md)** - Documenta√ß√£o completa

## ‚ö†Ô∏è Notas Importantes

- **Sempre use `provider: "llama"`** nas requisi√ß√µes
- **Modelos 405B t√™m custo maior** - use com modera√ß√£o
- **Cache autom√°tico** reduz custos para prompts repetidos
- **Fallback autom√°tico** para chave global se sua chave falhar
