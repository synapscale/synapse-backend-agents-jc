# ü§ñ Guia Completo de Integra√ß√£o LLM - SynapScale

## üéØ Vis√£o Geral

O SynapScale oferece integra√ß√£o unificada com **m√∫ltiplos provedores de LLM** atrav√©s de uma √∫nica API, permitindo flexibilidade, resili√™ncia e facilidade de uso. Este guia abrange configura√ß√£o, uso e melhores pr√°ticas.

### ‚ú® **Funcionalidades Principais**

- **üîë API Keys Personalizadas**: Usu√°rios podem configurar suas pr√≥prias chaves para cada provedor
- **üîÑ Fallback Autom√°tico**: Sistema usa chaves globais se usu√°rio n√£o configurou chaves espec√≠ficas  
- **üõ°Ô∏è Criptografia Segura**: Todas as chaves s√£o criptografadas com Fernet
- **‚ö° Interface Unificada**: Mesma API para todos os provedores
- **üîç Transpar√™ncia Total**: Endpoints funcionam automaticamente com chaves espec√≠ficas

---

## üöÄ **Provedores Suportados**

| Provedor | Chave API | Modelos Principais | Casos de Uso |
|----------|-----------|-------------------|--------------|
| **OpenAI** | `OPENAI_API_KEY` | GPT-4o, GPT-4, GPT-3.5-turbo | Gera√ß√£o geral, an√°lise, classifica√ß√£o |
| **Anthropic** | `ANTHROPIC_API_KEY` | Claude 3 Opus, Sonnet, Haiku | Racioc√≠nio √©tico, seguimento de instru√ß√µes |
| **Google** | `GOOGLE_API_KEY` | Gemini 1.5 Pro, Gemini Pro | Multimodal, an√°lise de dados, c√≥digo |
| **Grok** | `GROK_API_KEY` | Grok-1, Grok-2 | Conversa√ß√£o, criatividade |
| **DeepSeek** | `DEEPSEEK_API_KEY` | DeepSeek Coder, Chat | Gera√ß√£o de c√≥digo, programa√ß√£o |
| **Llama** | `LLAMA_API_KEY` | Llama 3 70B, 8B, Llama 2 | Open source, fine-tuning |

---

## ‚öôÔ∏è **Configura√ß√£o**

### **1. Configura√ß√£o Global (Sistema)**

No arquivo `.env` da raiz do projeto:

```env
# Provedores LLM (configura√ß√£o global)
LLM_DEFAULT_PROVIDER=openai
OPENAI_API_KEY=sk-proj-1234567890abcdef...
ANTHROPIC_API_KEY=sk-ant-api03-1234567890abcdef...
GOOGLE_API_KEY=AIzaSy1234567890abcdef...
GROK_API_KEY=xai-1234567890abcdef...
DEEPSEEK_API_KEY=sk-1234567890abcdef...
LLAMA_API_KEY=la_1234567890abcdef...
```

### **2. Configura√ß√£o por Usu√°rio (API Keys Espec√≠ficas)**

Usu√°rios podem configurar suas pr√≥prias chaves:

```http
POST /api/v1/user-variables/api-keys/openai
Authorization: Bearer <access_token>
Content-Type: application/json

{
  "value": "sk-proj-SUA_CHAVE_PESSOAL_AQUI",
  "description": "Minha chave OpenAI pessoal"
}
```

**Provedores suportados**: `openai`, `anthropic`, `google`, `grok`, `deepseek`, `llama`

---

## üìö **Uso da API**

### **Gera√ß√£o de Texto (Endpoint Unificado)**

```http
POST /api/v1/llm/generate
Authorization: Bearer <access_token>
Content-Type: application/json

{
  "prompt": "Explique machine learning em termos simples",
  "provider": "openai",
  "model": "gpt-4o",
  "max_tokens": 500,
  "temperature": 0.7
}
```

> üîë **Chaves Autom√°ticas**: Este endpoint usa automaticamente a chave espec√≠fica do usu√°rio, ou fallback para chaves globais.

### **Endpoints Espec√≠ficos por Provedor**

```http
POST /api/v1/llm/openai/generate
POST /api/v1/llm/anthropic/generate  
POST /api/v1/llm/google/generate
POST /api/v1/llm/grok/generate
POST /api/v1/llm/deepseek/generate
POST /api/v1/llm/llama/generate
```

### **Contagem de Tokens**

```http
POST /api/v1/llm/count-tokens
Content-Type: application/json

{
  "text": "Texto para contar tokens",
  "provider": "openai"
}
```

### **Listar Modelos e Provedores**

```http
GET /api/v1/llm/models          # Todos os modelos
GET /api/v1/llm/providers       # Todos os provedores
GET /api/v1/llm/openai/models   # Modelos OpenAI espec√≠ficos
```

---

## üîß **Par√¢metros Avan√ßados**

### **Par√¢metros Comuns (Todos os Provedores)**

| Par√¢metro | Tipo | Padr√£o | Descri√ß√£o |
|-----------|------|--------|-----------|
| `prompt` | string | - | **Obrigat√≥rio**: Texto de entrada |
| `provider` | string | `openai` | Provedor a usar |
| `model` | string | *varia* | Modelo espec√≠fico |
| `max_tokens` | integer | `1000` | Limite de tokens na resposta |
| `temperature` | float | `0.7` | Aleatoriedade (0.0-1.0) |

### **Par√¢metros Espec√≠ficos por Provedor**

#### **OpenAI**
```json
{
  "top_p": 1.0,
  "frequency_penalty": 0.0,
  "presence_penalty": 0.0,
  "stop": []
}
```

#### **Anthropic**
```json
{
  "top_k": 40,
  "top_p": 0.7,
  "stop_sequences": []
}
```

#### **Google**
```json
{
  "top_k": 40,
  "top_p": 0.95,
  "candidate_count": 1
}
```

---

## üíª **Exemplos de C√≥digo**

### **Python - Uso B√°sico**

```python
import requests

def generate_text(prompt, provider="openai"):
    url = "http://localhost:8000/api/v1/llm/generate"
    headers = {"Authorization": "Bearer YOUR_TOKEN"}
    
    response = requests.post(url, json={
        "prompt": prompt,
        "provider": provider,
        "max_tokens": 500,
        "temperature": 0.7
    }, headers=headers)
    
    return response.json()["text"]

# Uso
resultado = generate_text("Explique IA em termos simples", "openai")
print(resultado)
```

### **Python - Sistema de Fallback**

```python
import requests
import time

def generate_with_fallback(prompt, providers=["openai", "anthropic", "google"]):
    for provider in providers:
        try:
            response = requests.post(
                "http://localhost:8000/api/v1/llm/generate",
                json={"prompt": prompt, "provider": provider},
                headers={"Authorization": "Bearer YOUR_TOKEN"},
                timeout=30
            )
            
            if response.status_code == 200:
                return response.json()["text"]
                
        except Exception as e:
            print(f"Erro com {provider}: {e}")
            time.sleep(1)
            
    raise Exception("Todos os provedores falharam")
```

### **JavaScript/Node.js**

```javascript
async function generateText(prompt, provider = "openai") {
    const response = await fetch("http://localhost:8000/api/v1/llm/generate", {
        method: "POST",
        headers: {
            "Authorization": "Bearer YOUR_TOKEN",
            "Content-Type": "application/json"
        },
        body: JSON.stringify({
            prompt: prompt,
            provider: provider,
            max_tokens: 500,
            temperature: 0.7
        })
    });
    
    const data = await response.json();
    return data.text;
}

// Uso
const resultado = await generateText("Explique IA em termos simples");
console.log(resultado);
```

---

## üîë **Gerenciamento de API Keys por Usu√°rio**

### **Configurar Chave**

```python
import requests

def configure_api_key(provider, api_key, token):
    url = f"http://localhost:8000/api/v1/user-variables/api-keys/{provider}"
    headers = {"Authorization": f"Bearer {token}"}
    
    response = requests.post(url, json={
        "value": api_key,
        "description": f"Chave {provider} pessoal"
    }, headers=headers)
    
    return response.json()

# Configurar chave OpenAI
configure_api_key("openai", "sk-proj-SUA_CHAVE", "SEU_TOKEN")
```

### **Listar Chaves (Mascaradas)**

```python
def list_api_keys(token):
    url = "http://localhost:8000/api/v1/user-variables/api-keys"
    headers = {"Authorization": f"Bearer {token}"}
    
    response = requests.get(url, headers=headers)
    return response.json()

# Resultado mostra: "****cdef" para seguran√ßa
```

### **Remover Chave**

```python
def remove_api_key(provider, token):
    url = f"http://localhost:8000/api/v1/user-variables/api-keys/{provider}"
    headers = {"Authorization": f"Bearer {token}"}
    
    response = requests.delete(url, headers=headers)
    return response.status_code == 200
```

---

## üõ°Ô∏è **Seguran√ßa e Melhores Pr√°ticas**

### **Seguran√ßa**
- ‚úÖ **Criptografia**: Todas as chaves s√£o criptografadas com Fernet
- ‚úÖ **Mascaramento**: Chaves exibidas como `****1234` na listagem
- ‚úÖ **Isolamento**: Cada usu√°rio s√≥ acessa suas pr√≥prias chaves
- ‚úÖ **Fallback Seguro**: Usa chaves globais quando usu√°rio n√£o tem configurada

### **Melhores Pr√°ticas**
1. **Configure chaves espec√≠ficas** para controle de custos
2. **Use cache** (`use_cache: true`) para economizar tokens
3. **Defina limites** (`max_tokens`) apropriados
4. **Implemente fallback** entre provedores
5. **Monitore custos** regularmente

### **Rate Limiting**
- **Requests**: 100/min por usu√°rio
- **Tokens**: Limitado pelas APIs dos provedores
- **Cache TTL**: 300 segundos (5 minutos)

---

## üîç **Troubleshooting**

### **Problemas Comuns**

| Erro | Causa | Solu√ß√£o |
|------|-------|---------|
| `401 Unauthorized` | Chave API inv√°lida | Verificar chave no provedor |
| `429 Too Many Requests` | Rate limit | Aguardar ou usar outro provedor |
| `400 Bad Request` | Par√¢metros inv√°lidos | Verificar documenta√ß√£o do modelo |
| `Provider not available` | Provedor offline | Usar fallback autom√°tico |

### **Debug**

```python
# Verificar status dos provedores
response = requests.get("http://localhost:8000/api/v1/llm/providers")
providers = response.json()

for provider in providers["providers"]:
    print(f"{provider['name']}: {provider['available']}")
```

---

## üìà **Performance e Otimiza√ß√£o**

### **Cache Inteligente**
- **Cache autom√°tico** para prompts id√™nticos
- **TTL configur√°vel** (padr√£o: 5 minutos)
- **Economia significativa** de tokens e custos

### **Sele√ß√£o Autom√°tica de Modelos**

```python
def select_optimal_model(prompt_complexity):
    if prompt_complexity == "simple":
        return {"provider": "openai", "model": "gpt-3.5-turbo"}
    elif prompt_complexity == "complex":
        return {"provider": "anthropic", "model": "claude-3-opus"}
    else:
        return {"provider": "google", "model": "gemini-1.5-pro"}
```

---

## üîó **Recursos Adicionais**

- **[Endpoints LLM Detalhados](./endpoints.md)** - Documenta√ß√£o completa de endpoints
- **[OpenAI Espec√≠fico](./openai_endpoints.md)** - Configura√ß√µes espec√≠ficas OpenAI
- **[Llama Espec√≠fico](./llama_endpoints.md)** - Configura√ß√µes espec√≠ficas Llama
- **[Guia de API Keys](../user_variables_api_keys_guide.md)** - Gerenciamento detalhado de chaves

---

**√öltima atualiza√ß√£o**: Dezembro 2024  
**Vers√£o**: 2.0.0  
**Status**: ‚úÖ Documenta√ß√£o consolidada e atualizada
