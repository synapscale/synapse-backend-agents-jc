"""
Servi√ßo de Execu√ß√£o de Workflows
Criado por Jos√© - O melhor Full Stack do mundo
Engine completa de execu√ß√£o em tempo real com todas as funcionalidades
"""

import asyncio
import json
import logging
import uuid
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional, Tuple, Union
from sqlalchemy.orm import Session
from sqlalchemy import and_, or_, desc, asc, func
from contextlib import asynccontextmanager
import time
import traceback
from concurrent.futures import ThreadPoolExecutor
import threading

from src.synapse.models.workflow_execution import (
    WorkflowExecution, NodeExecution, ExecutionQueue, ExecutionMetrics,
    ExecutionStatus, NodeExecutionStatus
)
from src.synapse.models.workflow import Workflow
from src.synapse.models.node import Node
from src.synapse.models.user import User
from src.synapse.schemas.workflow_execution import (
    ExecutionCreate, ExecutionUpdate, ExecutionResponse,
    NodeExecutionCreate, NodeExecutionUpdate, NodeExecutionResponse,
    QueueItemCreate, MetricCreate, ExecutionStats, ExecutionFilter
)
from src.synapse.database import get_db
from src.synapse.core.websockets.manager import ConnectionManager
from src.synapse.services.variable_service import VariableService


# Configura√ß√£o de logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ExecutionEngine:
    """
    Engine principal de execu√ß√£o de workflows
    Gerencia todo o ciclo de vida de execu√ß√£o
    """
    
    def __init__(self, websocket_manager: ConnectionManager = None):
        self.websocket_manager = websocket_manager
        self.variable_service = VariableService()
        self.executor = ThreadPoolExecutor(max_workers=10)
        self.running_executions: Dict[str, asyncio.Task] = {}
        self.execution_lock = threading.Lock()
        self.is_running = False
        self.queue_processor_task = None
        
    async def start(self):
        """Inicia a engine de execu√ß√£o"""
        if self.is_running:
            return
            
        self.is_running = True
        self.queue_processor_task = asyncio.create_task(self._process_queue())
        logger.info("üöÄ Engine de Execu√ß√£o iniciada com sucesso!")
        
    async def stop(self):
        """Para a engine de execu√ß√£o"""
        self.is_running = False
        
        # Cancela o processador de fila
        if self.queue_processor_task:
            self.queue_processor_task.cancel()
            
        # Cancela execu√ß√µes em andamento
        for execution_id, task in self.running_executions.items():
            task.cancel()
            logger.info(f"Cancelando execu√ß√£o {execution_id}")
            
        # Aguarda finaliza√ß√£o
        await asyncio.gather(*self.running_executions.values(), return_exceptions=True)
        self.running_executions.clear()
        
        logger.info("üõë Engine de Execu√ß√£o parada com sucesso!")
        
    async def create_execution(
        self, 
        db: Session, 
        execution_data: ExecutionCreate, 
        user_id: int
    ) -> ExecutionResponse:
        """
        Cria uma nova execu√ß√£o de workflow
        """
        try:
            # Valida o workflow
            workflow = db.query(Workflow).filter(Workflow.id == execution_data.workflow_id).first()
            if not workflow:
                raise ValueError(f"Workflow {execution_data.workflow_id} n√£o encontrado")
                
            # Valida permiss√µes do usu√°rio
            if workflow.user_id != user_id:
                raise ValueError("Usu√°rio n√£o tem permiss√£o para executar este workflow")
                
            # Carrega vari√°veis do usu√°rio se necess√°rio
            user_variables = {}
            if execution_data.variables is None:
                user_variables = await self.variable_service.get_user_variables_dict(db, user_id)
            else:
                user_variables = execution_data.variables
                
            # Valida o workflow antes da execu√ß√£o
            validation = await self._validate_workflow(db, workflow, user_variables)
            if not validation["is_valid"]:
                raise ValueError(f"Workflow inv√°lido: {', '.join(validation['errors'])}")
                
            # Cria a execu√ß√£o
            execution = WorkflowExecution(
                execution_id=str(uuid.uuid4()),
                workflow_id=execution_data.workflow_id,
                user_id=user_id,
                status=ExecutionStatus.PENDING,
                input_data=execution_data.input_data,
                context_data=execution_data.context_data,
                variables=user_variables,
                priority=execution_data.priority,
                total_nodes=validation["total_nodes"],
                timeout_at=datetime.utcnow() + timedelta(seconds=execution_data.timeout_seconds) if execution_data.timeout_seconds else None,
                estimated_duration=validation.get("estimated_duration_seconds"),
                max_retries=execution_data.max_retries,
                auto_retry=execution_data.auto_retry,
                notify_on_completion=execution_data.notify_on_completion,
                notify_on_failure=execution_data.notify_on_failure,
                tags=execution_data.tags,
                metadata=execution_data.metadata
            )
            
            db.add(execution)
            db.commit()
            db.refresh(execution)
            
            # Cria execu√ß√µes de n√≥s
            await self._create_node_executions(db, execution, workflow)
            
            # Adiciona √† fila de execu√ß√£o
            await self._add_to_queue(db, execution)
            
            # Notifica via WebSocket
            if self.websocket_manager:
                await self.websocket_manager.send_to_user(
                    user_id,
                    {
                        "type": "execution_created",
                        "execution_id": execution.execution_id,
                        "workflow_id": execution.workflow_id,
                        "status": execution.status.value
                    }
                )
                
            logger.info(f"‚úÖ Execu√ß√£o {execution.execution_id} criada com sucesso")
            return ExecutionResponse.from_orm(execution)
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao criar execu√ß√£o: {str(e)}")
            db.rollback()
            raise
            
    async def start_execution(self, db: Session, execution_id: str, user_id: int) -> bool:
        """
        Inicia uma execu√ß√£o espec√≠fica
        """
        try:
            execution = db.query(WorkflowExecution).filter(
                WorkflowExecution.execution_id == execution_id,
                WorkflowExecution.user_id == user_id
            ).first()
            
            if not execution:
                raise ValueError(f"Execu√ß√£o {execution_id} n√£o encontrada")
                
            if execution.status != ExecutionStatus.PENDING:
                raise ValueError(f"Execu√ß√£o {execution_id} n√£o est√° pendente")
                
            # Inicia a execu√ß√£o
            task = asyncio.create_task(self._execute_workflow(db, execution))
            
            with self.execution_lock:
                self.running_executions[execution_id] = task
                
            logger.info(f"üöÄ Execu√ß√£o {execution_id} iniciada")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao iniciar execu√ß√£o {execution_id}: {str(e)}")
            return False
            
    async def cancel_execution(self, db: Session, execution_id: str, user_id: int, reason: str = None) -> bool:
        """
        Cancela uma execu√ß√£o
        """
        try:
            execution = db.query(WorkflowExecution).filter(
                WorkflowExecution.execution_id == execution_id,
                WorkflowExecution.user_id == user_id
            ).first()
            
            if not execution:
                raise ValueError(f"Execu√ß√£o {execution_id} n√£o encontrada")
                
            # Cancela a task se estiver rodando
            with self.execution_lock:
                if execution_id in self.running_executions:
                    self.running_executions[execution_id].cancel()
                    del self.running_executions[execution_id]
                    
            # Atualiza status no banco
            execution.status = ExecutionStatus.CANCELLED
            execution.completed_at = datetime.utcnow()
            execution.error_message = reason or "Execu√ß√£o cancelada pelo usu√°rio"
            
            # Cancela n√≥s em execu√ß√£o
            db.query(NodeExecution).filter(
                NodeExecution.workflow_execution_id == execution.id,
                NodeExecution.status.in_([NodeExecutionStatus.PENDING, NodeExecutionStatus.RUNNING])
            ).update({
                "status": NodeExecutionStatus.SKIPPED,
                "completed_at": datetime.utcnow(),
                "error_message": "Cancelado junto com o workflow"
            })
            
            db.commit()
            
            # Notifica via WebSocket
            if self.websocket_manager:
                await self.websocket_manager.send_to_user(
                    user_id,
                    {
                        "type": "execution_cancelled",
                        "execution_id": execution_id,
                        "reason": reason
                    }
                )
                
            logger.info(f"üõë Execu√ß√£o {execution_id} cancelada")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao cancelar execu√ß√£o {execution_id}: {str(e)}")
            return False
            
    async def retry_execution(self, db: Session, execution_id: str, user_id: int) -> bool:
        """
        Reinicia uma execu√ß√£o que falhou
        """
        try:
            execution = db.query(WorkflowExecution).filter(
                WorkflowExecution.execution_id == execution_id,
                WorkflowExecution.user_id == user_id
            ).first()
            
            if not execution:
                raise ValueError(f"Execu√ß√£o {execution_id} n√£o encontrada")
                
            if execution.status not in [ExecutionStatus.FAILED, ExecutionStatus.CANCELLED, ExecutionStatus.TIMEOUT]:
                raise ValueError(f"Execu√ß√£o {execution_id} n√£o pode ser reiniciada")
                
            if execution.retry_count >= execution.max_retries:
                raise ValueError(f"Execu√ß√£o {execution_id} excedeu o n√∫mero m√°ximo de tentativas")
                
            # Reset da execu√ß√£o
            execution.status = ExecutionStatus.PENDING
            execution.retry_count += 1
            execution.started_at = None
            execution.completed_at = None
            execution.error_message = None
            execution.error_details = None
            execution.completed_nodes = 0
            execution.failed_nodes = 0
            execution.progress_percentage = 0
            
            # Reset dos n√≥s
            db.query(NodeExecution).filter(
                NodeExecution.workflow_execution_id == execution.id
            ).update({
                "status": NodeExecutionStatus.PENDING,
                "started_at": None,
                "completed_at": None,
                "error_message": None,
                "error_details": None,
                "output_data": None,
                "retry_count": 0
            })
            
            db.commit()
            
            # Adiciona novamente √† fila
            await self._add_to_queue(db, execution)
            
            logger.info(f"üîÑ Execu√ß√£o {execution_id} reiniciada (tentativa {execution.retry_count})")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao reiniciar execu√ß√£o {execution_id}: {str(e)}")
            return False
            
    async def get_execution_status(self, db: Session, execution_id: str, user_id: int) -> Optional[ExecutionResponse]:
        """
        Obt√©m o status atual de uma execu√ß√£o
        """
        execution = db.query(WorkflowExecution).filter(
            WorkflowExecution.execution_id == execution_id,
            WorkflowExecution.user_id == user_id
        ).first()
        
        if not execution:
            return None
            
        return ExecutionResponse.from_orm(execution)
        
    async def get_node_executions(self, db: Session, execution_id: str, user_id: int) -> List[NodeExecutionResponse]:
        """
        Obt√©m as execu√ß√µes de n√≥s de um workflow
        """
        execution = db.query(WorkflowExecution).filter(
            WorkflowExecution.execution_id == execution_id,
            WorkflowExecution.user_id == user_id
        ).first()
        
        if not execution:
            return []
            
        node_executions = db.query(NodeExecution).filter(
            NodeExecution.workflow_execution_id == execution.id
        ).order_by(NodeExecution.execution_order).all()
        
        return [NodeExecutionResponse.from_orm(ne) for ne in node_executions]
        
    async def get_execution_logs(self, db: Session, execution_id: str, user_id: int) -> Dict[str, Any]:
        """
        Obt√©m os logs detalhados de uma execu√ß√£o
        """
        execution = db.query(WorkflowExecution).filter(
            WorkflowExecution.execution_id == execution_id,
            WorkflowExecution.user_id == user_id
        ).first()
        
        if not execution:
            return {}
            
        node_executions = db.query(NodeExecution).filter(
            NodeExecution.workflow_execution_id == execution.id
        ).order_by(NodeExecution.execution_order).all()
        
        return {
            "execution": {
                "id": execution.execution_id,
                "status": execution.status.value,
                "log": execution.execution_log,
                "error": execution.error_message,
                "debug": execution.debug_info
            },
            "nodes": [
                {
                    "key": ne.node_key,
                    "status": ne.status.value,
                    "log": ne.execution_log,
                    "error": ne.error_message,
                    "debug": ne.debug_info,
                    "duration_ms": ne.duration_ms
                }
                for ne in node_executions
            ]
        }
        
    async def get_execution_metrics(self, db: Session, execution_id: str, user_id: int) -> List[Dict[str, Any]]:
        """
        Obt√©m m√©tricas de uma execu√ß√£o
        """
        execution = db.query(WorkflowExecution).filter(
            WorkflowExecution.execution_id == execution_id,
            WorkflowExecution.user_id == user_id
        ).first()
        
        if not execution:
            return []
            
        metrics = db.query(ExecutionMetrics).filter(
            ExecutionMetrics.workflow_execution_id == execution.id
        ).all()
        
        return [
            {
                "type": m.metric_type,
                "name": m.metric_name,
                "value": m.value_numeric or m.value_float or m.value_text or m.value_json,
                "context": m.context,
                "measured_at": m.measured_at
            }
            for m in metrics
        ]
        
    async def _process_queue(self):
        """
        Processador principal da fila de execu√ß√£o
        """
        while self.is_running:
            try:
                async with asynccontextmanager(get_db)() as db:
                    # Busca pr√≥xima execu√ß√£o na fila
                    queue_item = db.query(ExecutionQueue).filter(
                        ExecutionQueue.status == "queued",
                        or_(
                            ExecutionQueue.scheduled_at.is_(None),
                            ExecutionQueue.scheduled_at <= datetime.utcnow()
                        )
                    ).order_by(
                        desc(ExecutionQueue.priority),
                        asc(ExecutionQueue.created_at)
                    ).first()
                    
                    if queue_item:
                        # Marca como processando
                        queue_item.status = "processing"
                        queue_item.started_at = datetime.utcnow()
                        queue_item.worker_id = f"worker-{uuid.uuid4().hex[:8]}"
                        db.commit()
                        
                        # Inicia execu√ß√£o
                        execution = queue_item.workflow_execution
                        await self.start_execution(db, execution.execution_id, execution.user_id)
                        
                await asyncio.sleep(1)  # Verifica a fila a cada segundo
                
            except Exception as e:
                logger.error(f"‚ùå Erro no processador de fila: {str(e)}")
                await asyncio.sleep(5)  # Espera mais tempo em caso de erro
                
    async def _execute_workflow(self, db: Session, execution: WorkflowExecution):
        """
        Executa um workflow completo
        """
        try:
            # Atualiza status para executando
            execution.status = ExecutionStatus.RUNNING
            execution.started_at = datetime.utcnow()
            db.commit()
            
            # Notifica in√≠cio
            if self.websocket_manager:
                await self.websocket_manager.send_to_user(
                    execution.user_id,
                    {
                        "type": "execution_started",
                        "execution_id": execution.execution_id,
                        "started_at": execution.started_at.isoformat()
                    }
                )
                
            # Carrega n√≥s para execu√ß√£o
            node_executions = db.query(NodeExecution).filter(
                NodeExecution.workflow_execution_id == execution.id
            ).order_by(NodeExecution.execution_order).all()
            
            # Executa n√≥s em ordem
            for node_execution in node_executions:
                if execution.status == ExecutionStatus.CANCELLED:
                    break
                    
                # Verifica timeout
                if execution.timeout_at and datetime.utcnow() > execution.timeout_at:
                    execution.status = ExecutionStatus.TIMEOUT
                    execution.error_message = "Execu√ß√£o excedeu o tempo limite"
                    break
                    
                # Executa o n√≥
                success = await self._execute_node(db, execution, node_execution)
                
                if success:
                    execution.completed_nodes += 1
                else:
                    execution.failed_nodes += 1
                    
                    # Se falhou e n√£o deve continuar, para a execu√ß√£o
                    if not node_execution.node.continue_on_error:
                        execution.status = ExecutionStatus.FAILED
                        execution.error_message = f"N√≥ {node_execution.node_key} falhou e interrompeu a execu√ß√£o"
                        break
                        
                # Atualiza progresso
                execution.update_progress()
                db.commit()
                
                # Notifica progresso
                if self.websocket_manager:
                    await self.websocket_manager.send_to_user(
                        execution.user_id,
                        {
                            "type": "execution_progress",
                            "execution_id": execution.execution_id,
                            "progress": execution.progress_percentage,
                            "completed_nodes": execution.completed_nodes,
                            "total_nodes": execution.total_nodes
                        }
                    )
                    
            # Finaliza execu√ß√£o
            if execution.status == ExecutionStatus.RUNNING:
                execution.status = ExecutionStatus.COMPLETED
                
            execution.completed_at = datetime.utcnow()
            execution.actual_duration = execution.duration_seconds
            db.commit()
            
            # Remove da lista de execu√ß√µes ativas
            with self.execution_lock:
                if execution.execution_id in self.running_executions:
                    del self.running_executions[execution.execution_id]
                    
            # Notifica conclus√£o
            if self.websocket_manager:
                await self.websocket_manager.send_to_user(
                    execution.user_id,
                    {
                        "type": "execution_completed",
                        "execution_id": execution.execution_id,
                        "status": execution.status.value,
                        "completed_at": execution.completed_at.isoformat(),
                        "duration_seconds": execution.actual_duration
                    }
                )
                
            logger.info(f"‚úÖ Execu√ß√£o {execution.execution_id} finalizada com status {execution.status.value}")
            
        except Exception as e:
            logger.error(f"‚ùå Erro na execu√ß√£o {execution.execution_id}: {str(e)}")
            
            # Marca como falha
            execution.status = ExecutionStatus.FAILED
            execution.completed_at = datetime.utcnow()
            execution.error_message = str(e)
            execution.error_details = {"traceback": traceback.format_exc()}
            db.commit()
            
            # Remove da lista de execu√ß√µes ativas
            with self.execution_lock:
                if execution.execution_id in self.running_executions:
                    del self.running_executions[execution.execution_id]
                    
    async def _execute_node(self, db: Session, execution: WorkflowExecution, node_execution: NodeExecution) -> bool:
        """
        Executa um n√≥ espec√≠fico
        """
        try:
            # Atualiza status para executando
            node_execution.status = NodeExecutionStatus.RUNNING
            node_execution.started_at = datetime.utcnow()
            db.commit()
            
            start_time = time.time()
            
            # Simula execu√ß√£o do n√≥ (aqui voc√™ implementaria a l√≥gica real)
            # Por enquanto, vamos simular diferentes tipos de n√≥s
            await self._simulate_node_execution(node_execution)
            
            # Calcula dura√ß√£o
            duration_ms = int((time.time() - start_time) * 1000)
            
            # Marca como conclu√≠do
            node_execution.status = NodeExecutionStatus.COMPLETED
            node_execution.completed_at = datetime.utcnow()
            node_execution.duration_ms = duration_ms
            node_execution.output_data = {"result": "success", "processed_at": datetime.utcnow().isoformat()}
            
            db.commit()
            
            # Registra m√©trica
            await self._record_metric(
                db, execution.id, node_execution.id,
                "execution_time", "node_duration_ms", duration_ms
            )
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Erro na execu√ß√£o do n√≥ {node_execution.node_key}: {str(e)}")
            
            # Marca como falha
            node_execution.status = NodeExecutionStatus.FAILED
            node_execution.completed_at = datetime.utcnow()
            node_execution.error_message = str(e)
            node_execution.error_details = {"traceback": traceback.format_exc()}
            
            db.commit()
            return False
            
    async def _simulate_node_execution(self, node_execution: NodeExecution):
        """
        Simula execu√ß√£o de diferentes tipos de n√≥s
        """
        node_type = node_execution.node_type.lower()
        
        if node_type == "api_call":
            # Simula chamada de API
            await asyncio.sleep(0.5)
        elif node_type == "data_processing":
            # Simula processamento de dados
            await asyncio.sleep(1.0)
        elif node_type == "ai_model":
            # Simula execu√ß√£o de modelo de IA
            await asyncio.sleep(2.0)
        elif node_type == "webhook":
            # Simula webhook
            await asyncio.sleep(0.3)
        else:
            # Execu√ß√£o padr√£o
            await asyncio.sleep(0.5)
            
    async def _validate_workflow(self, db: Session, workflow: Workflow, variables: Dict[str, Any]) -> Dict[str, Any]:
        """
        Valida um workflow antes da execu√ß√£o
        """
        errors = []
        warnings = []
        
        # Carrega n√≥s do workflow
        nodes = db.query(Node).filter(Node.workflow_id == workflow.id).all()
        
        if not nodes:
            errors.append("Workflow n√£o possui n√≥s")
            
        # Valida vari√°veis necess√°rias
        required_vars = []
        for node in nodes:
            if node.config and isinstance(node.config, dict):
                # Extrai vari√°veis do config (formato ${VAR_NAME})
                import re
                config_str = json.dumps(node.config)
                vars_in_config = re.findall(r'\$\{([^}]+)\}', config_str)
                required_vars.extend(vars_in_config)
                
        # Remove duplicatas
        required_vars = list(set(required_vars))
        
        # Verifica se todas as vari√°veis necess√°rias est√£o dispon√≠veis
        missing_vars = [var for var in required_vars if var not in variables]
        if missing_vars:
            errors.append(f"Vari√°veis necess√°rias n√£o encontradas: {', '.join(missing_vars)}")
            
        # Estima dura√ß√£o (baseado no tipo e quantidade de n√≥s)
        estimated_duration = len(nodes) * 2  # 2 segundos por n√≥ em m√©dia
        
        return {
            "is_valid": len(errors) == 0,
            "errors": errors,
            "warnings": warnings,
            "total_nodes": len(nodes),
            "estimated_duration_seconds": estimated_duration,
            "required_variables": required_vars
        }
        
    async def _create_node_executions(self, db: Session, execution: WorkflowExecution, workflow: Workflow):
        """
        Cria execu√ß√µes para todos os n√≥s do workflow
        """
        nodes = db.query(Node).filter(Node.workflow_id == workflow.id).order_by(Node.position).all()
        
        for i, node in enumerate(nodes):
            node_execution = NodeExecution(
                execution_id=str(uuid.uuid4()),
                workflow_execution_id=execution.id,
                node_id=node.id,
                node_key=node.key or f"node_{i}",
                node_type=node.type,
                node_name=node.name,
                execution_order=i,
                config_data=node.config,
                max_retries=3,
                retry_delay_ms=1000
            )
            db.add(node_execution)
            
        db.commit()
        
    async def _add_to_queue(self, db: Session, execution: WorkflowExecution):
        """
        Adiciona execu√ß√£o √† fila
        """
        queue_item = ExecutionQueue(
            queue_id=str(uuid.uuid4()),
            workflow_execution_id=execution.id,
            user_id=execution.user_id,
            priority=execution.priority,
            max_execution_time=3600,  # 1 hora por padr√£o
            max_retries=execution.max_retries
        )
        db.add(queue_item)
        db.commit()
        
    async def _record_metric(
        self, 
        db: Session, 
        workflow_execution_id: int, 
        node_execution_id: Optional[int],
        metric_type: str, 
        metric_name: str, 
        value: Union[int, float, str, Dict[str, Any]]
    ):
        """
        Registra uma m√©trica de execu√ß√£o
        """
        metric = ExecutionMetrics(
            workflow_execution_id=workflow_execution_id,
            node_execution_id=node_execution_id,
            metric_type=metric_type,
            metric_name=metric_name
        )
        
        if isinstance(value, int):
            metric.value_numeric = value
        elif isinstance(value, float):
            metric.value_float = str(value)
        elif isinstance(value, str):
            metric.value_text = value
        else:
            metric.value_json = value
            
        db.add(metric)
        db.commit()


class ExecutionService:
    """
    Servi√ßo principal para gerenciamento de execu√ß√µes
    Interface de alto n√≠vel para a ExecutionEngine
    """
    
    def __init__(self, websocket_manager: ConnectionManager = None):
        self.engine = ExecutionEngine(websocket_manager)
        
    async def start_engine(self):
        """Inicia a engine de execu√ß√£o"""
        await self.engine.start()
        
    async def stop_engine(self):
        """Para a engine de execu√ß√£o"""
        await self.engine.stop()
        
    async def create_and_start_execution(
        self, 
        db: Session, 
        execution_data: ExecutionCreate, 
        user_id: int,
        start_immediately: bool = True
    ) -> ExecutionResponse:
        """
        Cria e opcionalmente inicia uma execu√ß√£o
        """
        execution = await self.engine.create_execution(db, execution_data, user_id)
        
        if start_immediately:
            await self.engine.start_execution(db, execution.execution_id, user_id)
            
        return execution
        
    async def get_user_executions(
        self, 
        db: Session, 
        user_id: int, 
        filters: ExecutionFilter = None
    ) -> List[ExecutionResponse]:
        """
        Obt√©m execu√ß√µes de um usu√°rio com filtros
        """
        query = db.query(WorkflowExecution).filter(WorkflowExecution.user_id == user_id)
        
        if filters:
            if filters.status:
                query = query.filter(WorkflowExecution.status.in_(filters.status))
            if filters.workflow_ids:
                query = query.filter(WorkflowExecution.workflow_id.in_(filters.workflow_ids))
            if filters.created_after:
                query = query.filter(WorkflowExecution.created_at >= filters.created_after)
            if filters.created_before:
                query = query.filter(WorkflowExecution.created_at <= filters.created_before)
            if filters.tags:
                # Filtro por tags (JSON contains)
                for tag in filters.tags:
                    query = query.filter(WorkflowExecution.tags.contains([tag]))
                    
            # Ordena√ß√£o
            if filters.order_by == "created_at":
                order_col = WorkflowExecution.created_at
            elif filters.order_by == "updated_at":
                order_col = WorkflowExecution.updated_at
            elif filters.order_by == "started_at":
                order_col = WorkflowExecution.started_at
            elif filters.order_by == "completed_at":
                order_col = WorkflowExecution.completed_at
            elif filters.order_by == "priority":
                order_col = WorkflowExecution.priority
            else:
                order_col = WorkflowExecution.created_at
                
            if filters.order_direction == "desc":
                query = query.order_by(desc(order_col))
            else:
                query = query.order_by(asc(order_col))
                
            # Pagina√ß√£o
            query = query.offset(filters.offset).limit(filters.limit)
        else:
            query = query.order_by(desc(WorkflowExecution.created_at)).limit(50)
            
        executions = query.all()
        return [ExecutionResponse.from_orm(e) for e in executions]
        
    async def get_execution_statistics(self, db: Session, user_id: int) -> ExecutionStats:
        """
        Obt√©m estat√≠sticas de execu√ß√£o de um usu√°rio
        """
        # Contadores b√°sicos
        total = db.query(WorkflowExecution).filter(WorkflowExecution.user_id == user_id).count()
        running = db.query(WorkflowExecution).filter(
            WorkflowExecution.user_id == user_id,
            WorkflowExecution.status.in_([ExecutionStatus.PENDING, ExecutionStatus.RUNNING])
        ).count()
        completed = db.query(WorkflowExecution).filter(
            WorkflowExecution.user_id == user_id,
            WorkflowExecution.status == ExecutionStatus.COMPLETED
        ).count()
        failed = db.query(WorkflowExecution).filter(
            WorkflowExecution.user_id == user_id,
            WorkflowExecution.status == ExecutionStatus.FAILED
        ).count()
        cancelled = db.query(WorkflowExecution).filter(
            WorkflowExecution.user_id == user_id,
            WorkflowExecution.status == ExecutionStatus.CANCELLED
        ).count()
        
        # Taxa de sucesso
        success_rate = (completed / total * 100) if total > 0 else 0
        
        # Dura√ß√£o m√©dia
        avg_duration = db.query(func.avg(WorkflowExecution.actual_duration)).filter(
            WorkflowExecution.user_id == user_id,
            WorkflowExecution.actual_duration.isnot(None)
        ).scalar() or 0
        
        # Total de n√≥s executados
        total_nodes = db.query(func.sum(WorkflowExecution.completed_nodes)).filter(
            WorkflowExecution.user_id == user_id
        ).scalar() or 0
        
        # M√©dia de n√≥s por execu√ß√£o
        avg_nodes = (total_nodes / total) if total > 0 else 0
        
        # Workflows mais usados
        most_used = db.query(
            WorkflowExecution.workflow_id,
            func.count(WorkflowExecution.id).label('count')
        ).filter(
            WorkflowExecution.user_id == user_id
        ).group_by(WorkflowExecution.workflow_id).order_by(desc('count')).limit(5).all()
        
        return ExecutionStats(
            total_executions=total,
            running_executions=running,
            completed_executions=completed,
            failed_executions=failed,
            cancelled_executions=cancelled,
            average_duration_seconds=float(avg_duration) if avg_duration else None,
            success_rate_percentage=success_rate,
            total_nodes_executed=total_nodes,
            average_nodes_per_execution=avg_nodes,
            most_used_workflows=[{"workflow_id": w[0], "count": w[1]} for w in most_used],
            execution_trends={}  # Implementar trends se necess√°rio
        )
        
    # M√©todos de conveni√™ncia que delegam para a engine
    async def cancel_execution(self, db: Session, execution_id: str, user_id: int, reason: str = None) -> bool:
        return await self.engine.cancel_execution(db, execution_id, user_id, reason)
        
    async def retry_execution(self, db: Session, execution_id: str, user_id: int) -> bool:
        return await self.engine.retry_execution(db, execution_id, user_id)
        
    async def get_execution_status(self, db: Session, execution_id: str, user_id: int) -> Optional[ExecutionResponse]:
        return await self.engine.get_execution_status(db, execution_id, user_id)
        
    async def get_node_executions(self, db: Session, execution_id: str, user_id: int) -> List[NodeExecutionResponse]:
        return await self.engine.get_node_executions(db, execution_id, user_id)
        
    async def get_execution_logs(self, db: Session, execution_id: str, user_id: int) -> Dict[str, Any]:
        return await self.engine.get_execution_logs(db, execution_id, user_id)
        
    async def get_execution_metrics(self, db: Session, execution_id: str, user_id: int) -> List[Dict[str, Any]]:
        return await self.engine.get_execution_metrics(db, execution_id, user_id)


