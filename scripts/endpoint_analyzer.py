#!/usr/bin/env python3
"""
Script de An√°lise Avan√ßada de Endpoints - SynapScale Backend

Este script analisa todos os endpoints do backend, calculando m√©tricas
de complexidade, performance e sugerindo otimiza√ß√µes espec√≠ficas.

Autor: Sistema de Otimiza√ß√£o SynapScale
Vers√£o: 1.0.0
Data: Dezembro 2024
"""

import os
import ast
import time
import json
import logging
import statistics
from typing import Dict, List, Tuple, Any, Optional
from pathlib import Path
from dataclasses import dataclass, asdict
from collections import defaultdict

# Configura√ß√£o de logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


@dataclass
class EndpointMetrics:
    """M√©tricas de um endpoint espec√≠fico."""
    name: str
    file_path: str
    line_count: int
    complexity_score: int
    parameters_count: int
    dependencies_count: int
    docstring_quality: float
    error_handling_score: float
    async_operations: int
    database_operations: int
    cache_usage: bool
    logging_usage: bool
    validation_score: float
    security_score: float


@dataclass
class FileAnalysis:
    """An√°lise completa de um arquivo de endpoint."""
    file_path: str
    file_size_kb: float
    total_lines: int
    total_endpoints: int
    complexity_average: float
    maintainability_index: float
    endpoints: List[EndpointMetrics]
    recommendations: List[str]


class EndpointAnalyzer:
    """Analisador avan√ßado de endpoints."""
    
    def __init__(self, base_path: str):
        self.base_path = Path(base_path)
        self.endpoints_path = self.base_path / "src" / "synapse" / "api" / "v1" / "endpoints"
        self.analysis_results = []
        
    def analyze_all_endpoints(self) -> Dict[str, Any]:
        """
        Analisa todos os arquivos de endpoints.
        
        Returns:
            Dict com resultados completos da an√°lise
        """
        logger.info("üîç Iniciando an√°lise completa dos endpoints...")
        start_time = time.time()
        
        # Arquivos para an√°lise
        endpoint_files = list(self.endpoints_path.glob("*.py"))
        endpoint_files = [f for f in endpoint_files if f.name != "__init__.py"]
        
        logger.info(f"üìÅ Encontrados {len(endpoint_files)} arquivos para an√°lise")
        
        analysis_summary = {
            "total_files": len(endpoint_files),
            "analysis_date": time.strftime("%Y-%m-%d %H:%M:%S"),
            "files": [],
            "overall_metrics": {},
            "recommendations": [],
            "priority_issues": []
        }
        
        # Analisar cada arquivo
        for file_path in endpoint_files:
            try:
                logger.info(f"üìä Analisando {file_path.name}...")
                file_analysis = self._analyze_file(file_path)
                analysis_summary["files"].append(asdict(file_analysis))
                self.analysis_results.append(file_analysis)
            except Exception as e:
                logger.error(f"‚ùå Erro ao analisar {file_path.name}: {str(e)}")
        
        # Calcular m√©tricas gerais
        analysis_summary["overall_metrics"] = self._calculate_overall_metrics()
        analysis_summary["recommendations"] = self._generate_global_recommendations()
        analysis_summary["priority_issues"] = self._identify_priority_issues()
        
        analysis_time = time.time() - start_time
        logger.info(f"‚úÖ An√°lise conclu√≠da em {analysis_time:.2f} segundos")
        
        return analysis_summary
    
    def _analyze_file(self, file_path: Path) -> FileAnalysis:
        """
        Analisa um arquivo espec√≠fico de endpoint.
        
        Args:
            file_path: Caminho para o arquivo
            
        Returns:
            FileAnalysis com m√©tricas do arquivo
        """
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
            lines = content.split('\n')
        
        # M√©tricas b√°sicas do arquivo
        file_size_kb = file_path.stat().st_size / 1024
        total_lines = len(lines)
        
        # Parse AST para an√°lise estrutural
        try:
            tree = ast.parse(content)
        except SyntaxError as e:
            logger.warning(f"‚ö†Ô∏è Erro de sintaxe em {file_path.name}: {str(e)}")
            return self._create_empty_analysis(file_path)
        
        # Analisar endpoints
        endpoints = self._extract_endpoints(tree, file_path, content)
        
        # Calcular m√©tricas do arquivo
        complexity_scores = [ep.complexity_score for ep in endpoints if ep.complexity_score > 0]
        complexity_average = statistics.mean(complexity_scores) if complexity_scores else 0
        
        maintainability_index = self._calculate_maintainability_index(
            total_lines, complexity_average, len(endpoints)
        )
        
        # Gerar recomenda√ß√µes espec√≠ficas
        recommendations = self._generate_file_recommendations(
            file_path, file_size_kb, total_lines, len(endpoints), complexity_average
        )
        
        return FileAnalysis(
            file_path=str(file_path),
            file_size_kb=file_size_kb,
            total_lines=total_lines,
            total_endpoints=len(endpoints),
            complexity_average=complexity_average,
            maintainability_index=maintainability_index,
            endpoints=endpoints,
            recommendations=recommendations
        )
    
    def _extract_endpoints(self, tree: ast.AST, file_path: Path, content: str) -> List[EndpointMetrics]:
        """
        Extrai e analisa endpoints do AST.
        
        Args:
            tree: AST do arquivo
            file_path: Caminho do arquivo
            content: Conte√∫do do arquivo
            
        Returns:
            Lista de m√©tricas dos endpoints
        """
        endpoints = []
        
        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef):
                # Verifica se √© um endpoint (tem decorador de rota)
                if self._is_endpoint_function(node):
                    metrics = self._analyze_endpoint_function(node, file_path, content)
                    endpoints.append(metrics)
        
        return endpoints
    
    def _is_endpoint_function(self, func_node: ast.FunctionDef) -> bool:
        """
        Verifica se uma fun√ß√£o √© um endpoint da API.
        
        Args:
            func_node: N√≥ AST da fun√ß√£o
            
        Returns:
            True se for um endpoint
        """
        route_decorators = ['router.get', 'router.post', 'router.put', 'router.delete', 'router.patch']
        
        for decorator in func_node.decorator_list:
            if isinstance(decorator, ast.Call) and isinstance(decorator.func, ast.Attribute):
                decorator_name = f"{decorator.func.value.id}.{decorator.func.attr}"
                if decorator_name in route_decorators:
                    return True
        
        return False
    
    def _analyze_endpoint_function(self, func_node: ast.FunctionDef, file_path: Path, content: str) -> EndpointMetrics:
        """
        Analisa uma fun√ß√£o de endpoint espec√≠fica.
        
        Args:
            func_node: N√≥ AST da fun√ß√£o
            file_path: Caminho do arquivo
            content: Conte√∫do do arquivo
            
        Returns:
            EndpointMetrics com m√©tricas da fun√ß√£o
        """
        # Contar linhas da fun√ß√£o
        func_lines = func_node.end_lineno - func_node.lineno + 1
        
        # Calcular complexidade ciclom√°tica
        complexity = self._calculate_cyclomatic_complexity(func_node)
        
        # Contar par√¢metros
        params_count = len(func_node.args.args)
        
        # Analisar depend√™ncias
        dependencies = self._count_dependencies(func_node)
        
        # Qualidade da docstring
        docstring_quality = self._analyze_docstring_quality(func_node)
        
        # Tratamento de erros
        error_handling = self._analyze_error_handling(func_node)
        
        # Opera√ß√µes ass√≠ncronas
        async_ops = self._count_async_operations(func_node)
        
        # Opera√ß√µes de banco de dados
        db_ops = self._count_database_operations(func_node)
        
        # Uso de cache
        cache_usage = self._has_cache_usage(func_node)
        
        # Uso de logging
        logging_usage = self._has_logging_usage(func_node)
        
        # Score de valida√ß√£o
        validation_score = self._analyze_validation(func_node)
        
        # Score de seguran√ßa
        security_score = self._analyze_security(func_node)
        
        return EndpointMetrics(
            name=func_node.name,
            file_path=str(file_path),
            line_count=func_lines,
            complexity_score=complexity,
            parameters_count=params_count,
            dependencies_count=dependencies,
            docstring_quality=docstring_quality,
            error_handling_score=error_handling,
            async_operations=async_ops,
            database_operations=db_ops,
            cache_usage=cache_usage,
            logging_usage=logging_usage,
            validation_score=validation_score,
            security_score=security_score
        )
    
    def _calculate_cyclomatic_complexity(self, node: ast.AST) -> int:
        """
        Calcula a complexidade ciclom√°tica de uma fun√ß√£o.
        
        Args:
            node: N√≥ AST da fun√ß√£o
            
        Returns:
            Score de complexidade ciclom√°tica
        """
        complexity = 1  # Base complexity
        
        for child in ast.walk(node):
            if isinstance(child, (ast.If, ast.While, ast.For, ast.Try, ast.With)):
                complexity += 1
            elif isinstance(child, ast.BoolOp):
                complexity += len(child.values) - 1
            elif isinstance(child, ast.ExceptHandler):
                complexity += 1
        
        return complexity
    
    def _count_dependencies(self, func_node: ast.FunctionDef) -> int:
        """
        Conta o n√∫mero de depend√™ncias de uma fun√ß√£o.
        
        Args:
            func_node: N√≥ AST da fun√ß√£o
            
        Returns:
            N√∫mero de depend√™ncias
        """
        dependencies = set()
        
        # Depend√™ncias nos par√¢metros (Depends)
        for arg in func_node.args.args:
            if hasattr(arg, 'annotation') and arg.annotation:
                if isinstance(arg.annotation, ast.Call):
                    if hasattr(arg.annotation.func, 'id') and arg.annotation.func.id == 'Depends':
                        dependencies.add('depends')
        
        # Imports e calls dentro da fun√ß√£o
        for node in ast.walk(func_node):
            if isinstance(node, ast.Call):
                if hasattr(node.func, 'id'):
                    dependencies.add(node.func.id)
                elif hasattr(node.func, 'attr'):
                    dependencies.add(node.func.attr)
        
        return len(dependencies)
    
    def _analyze_docstring_quality(self, func_node: ast.FunctionDef) -> float:
        """
        Analisa a qualidade da docstring de uma fun√ß√£o.
        
        Args:
            func_node: N√≥ AST da fun√ß√£o
            
        Returns:
            Score de qualidade (0.0 a 1.0)
        """
        docstring = ast.get_docstring(func_node)
        if not docstring:
            return 0.0
        
        quality_score = 0.0
        
        # Presen√ßa de docstring
        quality_score += 0.2
        
        # Tamanho adequado
        if len(docstring) > 50:
            quality_score += 0.2
        
        # Se√ß√µes estruturadas
        sections = ['Args:', 'Returns:', 'Raises:']
        for section in sections:
            if section in docstring:
                quality_score += 0.2
        
        return min(quality_score, 1.0)
    
    def _analyze_error_handling(self, func_node: ast.FunctionDef) -> float:
        """
        Analisa a qualidade do tratamento de erros.
        
        Args:
            func_node: N√≥ AST da fun√ß√£o
            
        Returns:
            Score de tratamento de erros (0.0 a 1.0)
        """
        error_score = 0.0
        
        # Presen√ßa de try/except
        has_try_except = False
        exception_types = set()
        
        for node in ast.walk(func_node):
            if isinstance(node, ast.Try):
                has_try_except = True
                error_score += 0.3
                
                # Tipos espec√≠ficos de exce√ß√£o
                for handler in node.handlers:
                    if handler.type:
                        exception_types.add('specific')
                    else:
                        exception_types.add('generic')
        
        # HTTPException usage
        for node in ast.walk(func_node):
            if isinstance(node, ast.Raise):
                if isinstance(node.exc, ast.Call) and hasattr(node.exc.func, 'id'):
                    if node.exc.func.id == 'HTTPException':
                        error_score += 0.4
        
        # Logging de erros
        for node in ast.walk(func_node):
            if isinstance(node, ast.Call):
                if hasattr(node.func, 'attr') and 'error' in node.func.attr:
                    error_score += 0.3
        
        return min(error_score, 1.0)
    
    def _count_async_operations(self, func_node: ast.FunctionDef) -> int:
        """
        Conta opera√ß√µes ass√≠ncronas na fun√ß√£o.
        
        Args:
            func_node: N√≥ AST da fun√ß√£o
            
        Returns:
            N√∫mero de opera√ß√µes ass√≠ncronas
        """
        async_count = 0
        
        for node in ast.walk(func_node):
            if isinstance(node, ast.Await):
                async_count += 1
        
        return async_count
    
    def _count_database_operations(self, func_node: ast.FunctionDef) -> int:
        """
        Conta opera√ß√µes de banco de dados.
        
        Args:
            func_node: N√≥ AST da fun√ß√£o
            
        Returns:
            N√∫mero de opera√ß√µes de BD
        """
        db_ops = 0
        db_keywords = ['query', 'filter', 'add', 'commit', 'rollback', 'execute']
        
        for node in ast.walk(func_node):
            if isinstance(node, ast.Call):
                if hasattr(node.func, 'attr') and node.func.attr in db_keywords:
                    db_ops += 1
        
        return db_ops
    
    def _has_cache_usage(self, func_node: ast.FunctionDef) -> bool:
        """
        Verifica se a fun√ß√£o usa cache.
        
        Args:
            func_node: N√≥ AST da fun√ß√£o
            
        Returns:
            True se usa cache
        """
        cache_keywords = ['cache', 'cached', 'redis']
        
        for node in ast.walk(func_node):
            if isinstance(node, ast.Call):
                if hasattr(node.func, 'id') and any(kw in node.func.id.lower() for kw in cache_keywords):
                    return True
                if hasattr(node.func, 'attr') and any(kw in node.func.attr.lower() for kw in cache_keywords):
                    return True
        
        return False
    
    def _has_logging_usage(self, func_node: ast.FunctionDef) -> bool:
        """
        Verifica se a fun√ß√£o usa logging.
        
        Args:
            func_node: N√≥ AST da fun√ß√£o
            
        Returns:
            True se usa logging
        """
        for node in ast.walk(func_node):
            if isinstance(node, ast.Call):
                if hasattr(node.func, 'value') and hasattr(node.func.value, 'id'):
                    if node.func.value.id == 'logger':
                        return True
        
        return False
    
    def _analyze_validation(self, func_node: ast.FunctionDef) -> float:
        """
        Analisa a qualidade da valida√ß√£o de entrada.
        
        Args:
            func_node: N√≥ AST da fun√ß√£o
            
        Returns:
            Score de valida√ß√£o (0.0 a 1.0)
        """
        validation_score = 0.0
        
        # Pydantic models nos par√¢metros
        for arg in func_node.args.args:
            if hasattr(arg, 'annotation') and arg.annotation:
                validation_score += 0.2
        
        # Query parameter validation
        for node in ast.walk(func_node):
            if isinstance(node, ast.Call) and hasattr(node.func, 'id'):
                if node.func.id == 'Query':
                    validation_score += 0.3
        
        # Manual validation checks
        for node in ast.walk(func_node):
            if isinstance(node, ast.If):
                validation_score += 0.1
        
        return min(validation_score, 1.0)
    
    def _analyze_security(self, func_node: ast.FunctionDef) -> float:
        """
        Analisa aspectos de seguran√ßa da fun√ß√£o.
        
        Args:
            func_node: N√≥ AST da fun√ß√£o
            
        Returns:
            Score de seguran√ßa (0.0 a 1.0)
        """
        security_score = 0.0
        
        # Autentica√ß√£o obrigat√≥ria
        for arg in func_node.args.args:
            if 'current_user' in arg.arg:
                security_score += 0.4
        
        # Rate limiting
        for decorator in func_node.decorator_list:
            if isinstance(decorator, ast.Call):
                if hasattr(decorator.func, 'attr') and 'limit' in decorator.func.attr.lower():
                    security_score += 0.3
        
        # Input sanitization
        sanitization_funcs = ['escape', 'sanitize', 'validate']
        for node in ast.walk(func_node):
            if isinstance(node, ast.Call) and hasattr(node.func, 'id'):
                if any(sf in node.func.id.lower() for sf in sanitization_funcs):
                    security_score += 0.3
        
        return min(security_score, 1.0)
    
    def _calculate_maintainability_index(self, lines: int, complexity: float, endpoints: int) -> float:
        """
        Calcula o √≠ndice de manutenibilidade de um arquivo.
        
        Args:
            lines: N√∫mero de linhas
            complexity: Complexidade m√©dia
            endpoints: N√∫mero de endpoints
            
        Returns:
            √çndice de manutenibilidade (0-100)
        """
        # F√≥rmula adaptada do Maintainability Index
        if lines == 0 or complexity == 0:
            return 100.0
        
        volume = lines * (endpoints + 1)
        mi = max(0, (171 - 5.2 * (volume ** 0.23) - 0.23 * complexity - 16.2 * (lines ** 0.5)) * 100 / 171)
        return round(mi, 2)
    
    def _generate_file_recommendations(self, file_path: Path, size_kb: float, lines: int, 
                                     endpoints: int, complexity: float) -> List[str]:
        """
        Gera recomenda√ß√µes espec√≠ficas para um arquivo.
        
        Args:
            file_path: Caminho do arquivo
            size_kb: Tamanho em KB
            lines: N√∫mero de linhas
            endpoints: N√∫mero de endpoints
            complexity: Complexidade m√©dia
            
        Returns:
            Lista de recomenda√ß√µes
        """
        recommendations = []
        
        # Tamanho do arquivo
        if size_kb > 50:
            recommendations.append(f"üìè Arquivo muito grande ({size_kb:.1f}KB). Considere modularizar.")
        
        # N√∫mero de linhas
        if lines > 1000:
            recommendations.append(f"üìÑ Muitas linhas ({lines}). Divida em m√≥dulos menores.")
        
        # N√∫mero de endpoints
        if endpoints > 15:
            recommendations.append(f"üîó Muitos endpoints ({endpoints}). Considere separar por dom√≠nio.")
        
        # Complexidade
        if complexity > 10:
            recommendations.append(f"üß† Complexidade alta ({complexity:.1f}). Simplifique a l√≥gica.")
        elif complexity > 7:
            recommendations.append(f"‚ö†Ô∏è Complexidade moderada ({complexity:.1f}). Monitor para melhorias.")
        
        # Recomenda√ß√µes espec√≠ficas por arquivo
        file_name = file_path.name
        if file_name == "analytics.py" and size_kb > 70:
            recommendations.append("üìä Analytics: Separar em m√≥dulos (events, metrics, reports, dashboards)")
        
        if file_name == "templates.py" and endpoints > 12:
            recommendations.append("üìã Templates: Separar marketplace, reviews e collections")
        
        if file_name == "executions.py":
            recommendations.append("‚ö° Executions: Implementar cache Redis para performance")
        
        return recommendations
    
    def _calculate_overall_metrics(self) -> Dict[str, Any]:
        """
        Calcula m√©tricas gerais do projeto.
        
        Returns:
            Dict com m√©tricas agregadas
        """
        if not self.analysis_results:
            return {}
        
        total_files = len(self.analysis_results)
        total_endpoints = sum(f.total_endpoints for f in self.analysis_results)
        total_lines = sum(f.total_lines for f in self.analysis_results)
        total_size_kb = sum(f.file_size_kb for f in self.analysis_results)
        
        complexities = [f.complexity_average for f in self.analysis_results if f.complexity_average > 0]
        avg_complexity = statistics.mean(complexities) if complexities else 0
        
        maintainability_scores = [f.maintainability_index for f in self.analysis_results]
        avg_maintainability = statistics.mean(maintainability_scores) if maintainability_scores else 0
        
        # Estat√≠sticas de endpoints
        all_endpoints = []
        for file_analysis in self.analysis_results:
            all_endpoints.extend(file_analysis.endpoints)
        
        endpoint_complexities = [ep.complexity_score for ep in all_endpoints if ep.complexity_score > 0]
        endpoint_lines = [ep.line_count for ep in all_endpoints if ep.line_count > 0]
        
        return {
            "total_files": total_files,
            "total_endpoints": total_endpoints,
            "total_lines": total_lines,
            "total_size_kb": round(total_size_kb, 2),
            "average_complexity": round(avg_complexity, 2),
            "average_maintainability": round(avg_maintainability, 2),
            "endpoint_stats": {
                "average_complexity": round(statistics.mean(endpoint_complexities), 2) if endpoint_complexities else 0,
                "max_complexity": max(endpoint_complexities) if endpoint_complexities else 0,
                "average_lines": round(statistics.mean(endpoint_lines), 2) if endpoint_lines else 0,
                "max_lines": max(endpoint_lines) if endpoint_lines else 0
            },
            "files_needing_attention": [
                f.file_path for f in self.analysis_results 
                if f.file_size_kb > 50 or f.complexity_average > 8 or f.maintainability_index < 60
            ]
        }
    
    def _generate_global_recommendations(self) -> List[str]:
        """
        Gera recomenda√ß√µes globais para o projeto.
        
        Returns:
            Lista de recomenda√ß√µes priorit√°rias
        """
        recommendations = []
        
        # Analisar arquivos grandes
        large_files = [f for f in self.analysis_results if f.file_size_kb > 50]
        if large_files:
            recommendations.append(
                f"üóÇÔ∏è Modularizar {len(large_files)} arquivo(s) grande(s): " +
                ", ".join([Path(f.file_path).name for f in large_files])
            )
        
        # Analisar complexidade alta
        complex_files = [f for f in self.analysis_results if f.complexity_average > 8]
        if complex_files:
            recommendations.append(
                f"üß† Reduzir complexidade em {len(complex_files)} arquivo(s): " +
                ", ".join([Path(f.file_path).name for f in complex_files])
            )
        
        # Cache implementation
        recommendations.append("üöÄ Implementar cache Redis para endpoints de analytics e m√©tricas")
        
        # Database optimization
        recommendations.append("üóÑÔ∏è Adicionar √≠ndices espec√≠ficos para queries frequentes")
        
        # Monitoring
        recommendations.append("üìä Implementar Prometheus metrics para monitoramento")
        
        return recommendations
    
    def _identify_priority_issues(self) -> List[Dict[str, Any]]:
        """
        Identifica issues de alta prioridade.
        
        Returns:
            Lista de issues cr√≠ticos
        """
        issues = []
        
        for file_analysis in self.analysis_results:
            file_name = Path(file_analysis.file_path).name
            
            # Issue de tamanho
            if file_analysis.file_size_kb > 70:
                issues.append({
                    "type": "size",
                    "severity": "high",
                    "file": file_name,
                    "description": f"Arquivo muito grande ({file_analysis.file_size_kb:.1f}KB)",
                    "recommendation": "Modularizar por dom√≠nio de neg√≥cio"
                })
            
            # Issue de complexidade
            if file_analysis.complexity_average > 10:
                issues.append({
                    "type": "complexity",
                    "severity": "high", 
                    "file": file_name,
                    "description": f"Complexidade muito alta ({file_analysis.complexity_average:.1f})",
                    "recommendation": "Refatorar fun√ß√µes complexas"
                })
            
            # Issue de manutenibilidade
            if file_analysis.maintainability_index < 50:
                issues.append({
                    "type": "maintainability",
                    "severity": "medium",
                    "file": file_name,
                    "description": f"Baixa manutenibilidade ({file_analysis.maintainability_index:.1f})",
                    "recommendation": "Melhorar estrutura e documenta√ß√£o"
                })
        
        return sorted(issues, key=lambda x: x["severity"], reverse=True)
    
    def _create_empty_analysis(self, file_path: Path) -> FileAnalysis:
        """
        Cria uma an√°lise vazia para arquivos com erro.
        
        Args:
            file_path: Caminho do arquivo
            
        Returns:
            FileAnalysis vazia
        """
        return FileAnalysis(
            file_path=str(file_path),
            file_size_kb=0.0,
            total_lines=0,
            total_endpoints=0,
            complexity_average=0.0,
            maintainability_index=0.0,
            endpoints=[],
            recommendations=["‚ùå Erro na an√°lise do arquivo"]
        )


def save_analysis_report(analysis_data: Dict[str, Any], output_file: str = None) -> None:
    """
    Salva o relat√≥rio de an√°lise em arquivo.
    
    Args:
        analysis_data: Dados da an√°lise
        output_file: Arquivo de sa√≠da (opcional)
    """
    if not output_file:
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        output_file = f"endpoint_analysis_{timestamp}.json"
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(analysis_data, f, indent=2, ensure_ascii=False)
    
    logger.info(f"üìä Relat√≥rio salvo em: {output_file}")


def print_summary_report(analysis_data: Dict[str, Any]) -> None:
    """
    Exibe um resumo da an√°lise no console.
    
    Args:
        analysis_data: Dados da an√°lise
    """
    print("\n" + "="*80)
    print("üîç RELAT√ìRIO DE AN√ÅLISE DE ENDPOINTS - SYNAPSCALE BACKEND")
    print("="*80)
    
    metrics = analysis_data.get("overall_metrics", {})
    
    print(f"\nüìä M√âTRICAS GERAIS:")
    print(f"   ‚Ä¢ Total de arquivos analisados: {metrics.get('total_files', 0)}")
    print(f"   ‚Ä¢ Total de endpoints: {metrics.get('total_endpoints', 0)}")
    print(f"   ‚Ä¢ Total de linhas: {metrics.get('total_lines', 0):,}")
    print(f"   ‚Ä¢ Tamanho total: {metrics.get('total_size_kb', 0):.1f} KB")
    print(f"   ‚Ä¢ Complexidade m√©dia: {metrics.get('average_complexity', 0):.2f}")
    print(f"   ‚Ä¢ Manutenibilidade m√©dia: {metrics.get('average_maintainability', 0):.1f}/100")
    
    endpoint_stats = metrics.get('endpoint_stats', {})
    print(f"\nüéØ ESTAT√çSTICAS DE ENDPOINTS:")
    print(f"   ‚Ä¢ Complexidade m√©dia: {endpoint_stats.get('average_complexity', 0):.2f}")
    print(f"   ‚Ä¢ Complexidade m√°xima: {endpoint_stats.get('max_complexity', 0)}")
    print(f"   ‚Ä¢ Linhas m√©dias por endpoint: {endpoint_stats.get('average_lines', 0):.1f}")
    print(f"   ‚Ä¢ Endpoint mais longo: {endpoint_stats.get('max_lines', 0)} linhas")
    
    recommendations = analysis_data.get("recommendations", [])
    if recommendations:
        print(f"\nüí° RECOMENDA√á√ïES PRINCIPAIS:")
        for i, rec in enumerate(recommendations[:5], 1):
            print(f"   {i}. {rec}")
    
    priority_issues = analysis_data.get("priority_issues", [])
    if priority_issues:
        print(f"\nüö® ISSUES PRIORIT√ÅRIOS:")
        for issue in priority_issues[:3]:
            print(f"   ‚Ä¢ {issue['file']}: {issue['description']}")
            print(f"     ‚Üí {issue['recommendation']}")
    
    files_attention = metrics.get('files_needing_attention', [])
    if files_attention:
        print(f"\n‚ö†Ô∏è ARQUIVOS QUE PRECISAM DE ATEN√á√ÉO:")
        for file_path in files_attention:
            print(f"   ‚Ä¢ {Path(file_path).name}")
    
    print("\n" + "="*80)
    print("üìã An√°lise conclu√≠da! Verifique o arquivo JSON para detalhes completos.")
    print("="*80)


def main():
    """Fun√ß√£o principal do script."""
    print("üöÄ Iniciando an√°lise avan√ßada de endpoints...")
    
    # Diret√≥rio base do projeto
    base_dir = Path(__file__).parent.parent
    
    # Criar analisador
    analyzer = EndpointAnalyzer(str(base_dir))
    
    # Executar an√°lise
    analysis_results = analyzer.analyze_all_endpoints()
    
    # Salvar relat√≥rio
    save_analysis_report(analysis_results)
    
    # Exibir resumo
    print_summary_report(analysis_results)
    
    print("\n‚úÖ An√°lise conclu√≠da com sucesso!")


if __name__ == "__main__":
    main() 